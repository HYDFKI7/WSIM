{fenge}
84931096571	Real-time object tracking via optimal feature subspace	In this paper, we present a real-time tracking approach based on the Optimal Feature Subspace (OFS). OFS is an optimal subspace of a random feature space, which can best represent the target and making it most distinguished in the whole scene. Initially, we randomly crop patches inside the bounding box to generate an efficient feature template set. Then a greedy algorithm fusing the cues of both target and background is proposed to seek the OFS at every frame. In the forthcoming frame, considering the correlation of different dimensions, we compute the Mahalanobis distance of candidate patches to the appearance model in the obtained subspace to locate the target. The experimental results on several challenging video clips demonstrate that our approach outperforms the state-of-the-art methods, in terms of both speed and robustness.
{fenge}
33744744439	Skeleton-growing algorithm based on boundary curve evolution	Traditional skeletonization algorithm based on the distance transform can not be used for skeleton recognition directly, and the connectivity property of the skeleton is not guaranteed. A novel skeletonization algorithm is presented, in which the whole skeleton is obtained by growing from the original skeleton seed one by one. In the growing process, the redundant skeleton branches are eliminated by the discrete curve evolution model, the visual branches remain completely, and the hierarchical control can be achieved easily. Examples have showed that the complexity of this algorithm is low, the connectivity of the skeleton is guaranteed, and the skeleton can represent the visual parts to satisfy human vision. The algorithm can be used in graphic recognition and shape measurement.
{fenge}
33847396877	Skeleton pruning by contour partitioning with discrete curve evolution	In this paper, we introduce a new skeleton pruning method based on contour partitioning. Any contour partition can be used, but the partitions obtained by Discrete Curve Evolution (DCE) yield excellent results. The theoretical properties and the experiments presented demonstrate that obtained skeletons are in accord with human visual perception and stable, even in the presence of significant noise and shape variations, and have the same topology as the original skeletons. In particular, we have proven that the proposed approach never produces spurious branches, which are common when using the known skeleton pruning methods. Moreover, the proposed pruning method does not displace the skeleton points. Consequently, all skeleton points are centers of maximal disks. Again, many existing methods displace skeleton points in order to produces pruned skeletons. © 2007 IEEE.
{fenge}
34948890026	Visual curvature	In this paper, we propose a new definition of curvature, called visual curvature. It is based on statistics of the extreme points of the height functions computed over all directions. By gradually ignoring relatively small heights, a single parameter multi-scale curvature is obtained. It does not modify the original contour and the scale parameter has an obvious geometric meaning. The theoretical properties and the experiments presented demonstrate that multi-scale visual curvature is stable, even in the presence of significant noise. In particular, it can deal with contours with significant gaps. We also show a relation between multi-scale visual curvature and convexity of simple closed curves. To our best knowledge, the proposed definition of visual curvature is the first ever that applies to regular curves as defined in differential geometry as well as to turn angles of polygonal curves. Moreover, it yields stable curvature estimates of curves in digital images even under sever distortions. ©2007 IEEE.
{fenge}
38349005012	Discrete skeleton evolution	Skeleton can be viewed as a compact shape representation in that the shape can be completely reconstructed form the skeleton. We present a novel method for skeleton pruning that is based on this fundamental skeleton property. We iteratively remove skeleton end braches with smallest relevance for shape reconstruction. The relevance of branches is measured as their contribution to shape reconstruction. The proposed pruning method allows us to overcome the instability of skeleton representation: a small boundary deformation leads to large changes in skeleton topology. Consequently, we are able to obtain very stable skeleton representation of planar shapes. © Springer-Verlag Berlin Heidelberg 2007.
{fenge}
38349036515	Shape classification based on skeleton path similarity	Most of the traditional methods for shape classification are based on contour. They often encounter difficulties when dealing with classes that have large nonlinear variability, especially when the variability is structural or due to articulation. It is well-known that shape representation based on skeletons is superior to contour based representation in such situations. However, approaches to shape similarity based on skeletons suffer from the instability of skeletons and matching of skeleton graphs is still an open problem. Using a skeleton pruning method, we are able to obtain stable pruned skeletons even in the presence of significant contour distortions. In contrast to most existing methods, it does not require converting of skeleton graphs to trees and it does not require any graph editing. We represent each shape as set of shortest paths in the skeleton between pairs of skeleton endpoints. Shape classification is done with Bayesian classifier. We present excellent classification results for complete shape. © Springer-Verlag Berlin Heidelberg 2007.
{fenge}
33644883051	Edge detection algorithm based on the Optimal Discrete Filter	Edge detection has been an important domain in comprehension of image and computer vision, Its quality determines the result of subsequent analysis. Thus it's an important goal for people to find a kind of method that is insensitive to noise, precisely locates true edges and excludes false edges. In theory Canny operator is optimal in continuous domain for edge detection, but not optimal in discrete case, which may lead to inaccurate result. In order to solve the problem above, a good edge detection algorithm in discrete field is proposed. The smooth operator is inferred based on the Optimal Discrete Filter's theory, which can reduce the influence of noise, false edges and image error. The experimental result shows that the proposed method can achieve multi -scale edge detection and high SNR, satisfy the requirement for computer recognition. So it is a good edge detection method.
{fenge}
41549128483	Detection and recognition of contour parts based on shape similarity	Due to distortion, noise, segmentation errors, overlap, and occlusion of objects in digital images, it is usually impossible to extract complete object contours or to segment the whole objects. However, in many cases parts of contours can be correctly reconstructed either by performing edge grouping or as parts of boundaries of segmented regions. Therefore, recognition of objects based on their contour parts seems to be a promising as well as a necessary research direction. The main contribution of this paper is a system for detection and recognition of contour parts in digital images. Both detection and recognition are based on shape similarity of contour parts. For each contour part produced by contour grouping, we use shape similarity to retrieve the most similar contour parts in a database of known contour segments. A shape-based classification of the retrieved contour parts performs then a simultaneous detection and recognition. An important step in our approach is the construction of the database of known contour segments. First complete contours of known objects are decomposed into parts using discrete curve evolution. Then, their representation is constructed that is invariant to scaling, rotation, and translation. © 2008 Elsevier Ltd. All rights reserved.
{fenge}
77954567918	Skeletonization with particle filters	We present a novel method to obtain high quality skeletons of binary shapes. The obtained skeletons are connected and one pixel thick. They do not require any pruning or any other post-processing. The computation is composed of two major parts. First, a small set of salient contour points is computed. We use Discrete Curve Evolution, but any other robust method could be used. Second, particle filters are used to obtain the skeleton. The main idea is that the particles walk along the skeletal paths between pairs of the salient points. We provide experimental results that clearly demonstrate that the proposed method significantly outperforms other well-known methods for skeleton computation. Moreover, we propose an extension of our method to computing skeletons of gray level images and provide promising experimental results. © 2010 World Scientific Publishing Company.
{fenge}
80052874546	Feature context for image classification and object detection	In this paper, we presents a new method to encode the spatial information of local image features, which is a natural extension of Shape Context (SC), so we call it Feature Context (FC). Given a position in a image, SC computes histogram of other points belonging to the target binary shape based on their distances and angles to the position. The value of each histogram bin of SC is the number of the shape points in the region assigned to the bin. Thus, SC requires knowing the location of the points of the target shape. In other words, an image point can have only two labels, it belongs to the shape or not. In contrast, FC can be applied to the whole image without knowing the location of the target shape in the image. Each image point can have multiple labels depending on its local features. The value of each histogram bin of FC is a histogram of various features assigned to points in the bin region. We also introduce an efficient coding method to encode the local image features, call Radial Basis Coding (RBC). Combining RBC and FC together, and using a linear SVM classifier, our method is suitable for both image classification and object detection. © 2011 IEEE.
{fenge}
84866640582	Detecting texts of arbitrary orientations in natural images	With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes. © 2012 IEEE.
{fenge}
84867399645	Shape clustering: Common structure discovery	This paper aims to address the problem of shape clustering by discovering the common structure which captures the intrinsic structural information of shapes belonging to the same cluster. It is based on a skeleton graph, named common structure skeleton graph (CSSG), which expresses possible correspondences between nodes of the individual skeletons of the cluster. To construct the CSSG, we derive the correspondences by the optimal subsequence bijection (OSB). To cluster the shape data, we apply an agglomerative clustering scheme, in each iteration, the CSSGs are formed from each cluster and the two closest clusters are merged into one. The proposed agglomerative clustering algorithm has been evaluated on several shape data sets, including three articulated shape data sets, Torsellos data set, and a gesture data set. In all experiments, our method demonstrates effective performance compared to other algorithms. © 2012 Elsevier Ltd.
{fenge}
84866681606	Fan shape model for object detection	We propose a novel shape model for object detection called Fan Shape Model (FSM). We model contour sample points as rays of final length emanating for a reference point. As in folding fan, its slats, which we call rays, are very flexible. This flexibility allows FSM to tolerate large shape variance. However, the order and the adjacency relation of the slats stay invariant during fan deformation, since the slats are connected with a thin fabric. In analogy, we enforce the order and adjacency relation of the rays to stay invariant during the deformation. Therefore, FSM preserves discriminative power while allowing for a substantial shape deformation. FSM allows also for precise scale estimation during object detection. Thus, there is not need to scale the shape model or image in order to perform object detection. Another advantage of FSM is the fact that it can be applied directly to edge images, since it does not require any linking of edge pixels to edge fragments (contours). © 2012 IEEE.
{fenge}
84877754554	Fusion with Diffusion for robust visual tracking	A weighted graph is used as an underlying structure of many algorithms like semisupervised learning and spectral clustering. If the edge weights are determined by a single similarity measure, then it hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure. In particular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations. In this paper, a novel approach to integrate multiple similarity measures is proposed. First pairs of similarity measures are combined with a diffusion process on their tensor product graph (TPG). Hence the diffused similarity of each pair of objects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the TPG. We call this process Fusion with Diffusion (FD). However, a higher order graph like the TPG usually means significant increase in time complexity. This is not the case in the proposed approach. A key feature of our approach is that the time complexity of the diffusion on the TPG is the same as the diffusion process on each of the original graphs. Moreover, it is not necessary to explicitly construct the TPG in our framework. Finally all diffused pairs of similarity measures are combined as a weighted sum. We demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t - 1 and target object candidates in frame t are integrated. The obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods.
{fenge}
84897567746	Online multiple targets detection and tracking from mobile robot in cluttered indoor environments with depth camera	Indoor environment is a common scene in our everyday life, and detecting and tracking multiple targets in this environment is a key component for many applications. However, this task still remains challenging due to limited space, intrinsic target appearance variation, e.g. full or partial occlusion, large pose deformation, and scale change. In the proposed approach, we give a novel framework for detection and tracking in indoor environments, and extend it to robot navigation. One of the key components of our approach is a virtual top view created from an RGB-D camera, which is named ground plane projection (GPP). The key advantage of using GPP is the fact that the intrinsic target appearance variation and extrinsic noise is far less likely to appear in GPP than in a regular side-view image. Moreover, it is a very simple task to determine free space in GPP without any appearance learning even from a moving camera. Hence GPP is very different from the top-view image obtained from a ceiling mounted camera. We perform both object detection and tracking in GPP. Two kinds of GPP images are utilized: gray GPP, which represents the maximal height of 3D points projecting to each pixel, and binary GPP, which is obtained by thresholding the gray GPP. For detection, a simple connected component labeling is used to detect footprints of targets in binary GPP. For tracking, a novel Pixel Level Association (PLA) strategy is proposed to link the same target in consecutive frames in gray GPP. It utilizes optical flow in gray GPP, which to our best knowledge has never been done before. Then we back project the detected and tracked objects in GPP to original, side-view (RGB) images. Hence we are able to detect and track objects in the side-view (RGB) images. Our system is able to robustly detect and track multiple moving targets in real time. The detection process does not rely on any target model, which means we do not need any training process. Moreover, tracking does not require any manual initialization, since all entering objects are robustly detected. We also extend the novel framework to robot navigation by tracking. As our experimental results demonstrate, our approach can achieve near prefect detection and tracking results. The performance gain in comparison to state-of-the-art trackers is most significant in the presence of occlusion and background clutter. © 2014 World Scientific Publishing Company.
{fenge}
45349083242	Path similarity skeleton graph matching	This paper presents a novel framework to shape recognition based on object silhouettes. The main idea is to match skeleton graphs by comparing the shortest paths between skeleton endpoints. In contrast to typical tree or graph matching methods, we completely ignore the topological graph structure. Our approach is motivated by the fact that visually similar skeleton graphs may have completely different topological structures. The proposed comparison of shortest paths between endpoints of skeleton graphs yields correct matching results in such cases. The skeletons are pruned by contour partitioning with Discrete Curve Evolution, which implies that the endpoints of skeleton branches correspond to visual parts of the objects. The experimental results demonstrate that our method is able to produce correct results in the presence of articulations, stretching, and occlusion. © 2008 IEEE.
{fenge}
47749095403	Skeleton-based shape classification using path similarity	Most of the traditional methods for shape classification are based on contour. They often encounter difficulties when dealing with classes that have large nonlinear variability, especially when the variability is structural or due to articulation. It is well-known that shape representation based on skeletons is superior to contour based representation in such situations. However, approaches to shape similarity based on skeletons suffer from the instability of skeletons, and matching of skeleton graphs is still an open problem. Using a new skeleton pruning method, we are able to obtain stable pruned skeletons even in the presence of significant contour distortions. We also propose a new method for matching of skeleton graphs. In contrast to most existing methods, it does not require converting of skeleton graphs to trees and it does not require any graph editing. Shape classification is done with Bayesian classifier. We present excellent classification results for complete shapes. © 2008 World Scientific Publishing Company.
{fenge}
48149096733	Skeletonization using SSM of the distance transform	This paper proposes a new approach for skeletonization based on the skeleton strength map (SSM) caculated by Euclidean distance transform of a binary image. After the distance transform and gradient are computed, isotropic diffusion is performed on the gradient vector field and the skeleton strength map is computed from the diffused vector field. A critical point set is then selected from local maxima of the SSM. The critical points are located on significant visual parts of the object. The skeleton is obtained by connecting the critical points with geodesic paths. This approach overcomes intrinsic drawbacks of distance transform based skeletons, since it yields stable and connected skeletons without losing significant visual parts. © 2007 IEEE.
{fenge}
50649085197	Contour grouping based on local symmetry	The paper deals with grouping of edges to contours of shapes using only local symmetry and continuity. Shape skeletons are used to generate the search space for a version of the Markov Chain Monte Carlo approach utilizing particle filters to find the most likely skeleton. Intuitively this means that grouping of edge segments is performed by walking along the skeleton. The particle search, which is an adapted version of a successful algorithm in robot mapping, is assisted by a reference model of a shape, which is expressed as the sequence of sample points and radii of maximal skeleton disks. This model is sufficiently flexible to represent non-rigid deformations, but restrictive enough to perform well on real, noisy image data. The order of skeleton points (and their corresponding segments) found by the particles defines the grouping. ©2007 IEEE.
{fenge}
52149095860	An efficient quick thinning algorithm	A new thinning algorithm is addressed in this paper. The most important feature in this paper is that it thins symbols to their central lines with high speed. This means that the method is rotation invariant. Compared to the method of Ahmed and Ward, it can improve the speed and solve their falls on two-pixel wide lines. The method in this paper contains two steps. The first one uses the concept of weight-value to separate the rules into 6 groups in order to improve the speed of thinning; The second one uses 2 rides to make sure that the skeleton is single pixel and can preserve the connectivity. The results show that this method has good effect on preserving the topology of symbols and letters. © 2008 IEEE.
{fenge}
56749153656	Improving shape retrieval by learning graph transduction	Shape retrieval/matching is a very important topic in computer vision. The recent progress in this domain has been mostly driven by designing smart features for providing better similarity measure between pairs of shapes. In this paper, we provide a new perspective to this problem by considering the existing shapes as a group, and study their similarity measures to the query shape in a graph structure. Our method is general and can be built on top of any existing shape matching algorithms. It learns a better metric through graph transduction by propagating the model through existing shapes, in a way similar to computing geodesics in shape manifold. However, the proposed method does not require learning the shape manifold explicitly and it does not require knowing any class labels of existing shapes. The presented experimental results demonstrate that the proposed approach yields significant improvements over the state-of-art shape matching algorithms. We obtained a retrieval rate of 91% on the MPEG-7 data set, which is the highest ever reported in the literature. © 2008 Springer Berlin Heidelberg.
{fenge}
58349085213	Computing stable skeletons with particle filters	We present a novel method to obtain high quality skeletons of binary shapes. The obtained skeletons are connected and one pixel thick. They do not require any pruning or any other post-processing. The computation is composed of two major parts. First, a small set of salient contour points is computed. We use Discrete Curve Evolution, but any other robust method could be used. Second, particle filters are used to obtain the skeleton. The main idea is that the particles walk along the skeletal paths between pairs of the salient points. We provide experimental results that clearly demonstrate that the proposed method significantly outperforms other well-known methods for skeleton computation. © 2008 Springer Berlin Heidelberg.
{fenge}
60149083243	Contour grouping with partial shape similarity	In this paper, a novel algorithm is introduced to group contours from clutter images by integrating high-level information (prior of part segments) and low-level information (paths of segmentations of clutter images). The partial shape similarity between these two levels of information is embedded into the particle filter framework, an effective recursively estimating model. The particles in the framework are modeled as the paths on the edges of segmentation results (Normalized Cuts in this paper). At prediction step, the paths extend along the edges of Normalized Cuts; while, at the update step, the weights of particles update according to their partial shape similarity with priors of the trained contour segments. Successful results are achieved against the noise of the testing image, the inaccuracy of the segmentation result as well as the inexactness of the similarity between the contour segment and edges segmentation. The experimental results also demonstrate robust contour grouping performance in the presence of occlusion and large texture variation within the segmented objects. © 2009 Springer Berlin Heidelberg.
{fenge}
69949178421	Skeletonization of gray-scale image from incomplete boundaries	Skeletonization of gray-scale images is a challenging problem in computer vision due to the difficulty of segmenting gray-scale images to get the complete contour. Compared with previous skeletonization algorithms which use computational methods to avoid segmentation, this paper reveals that it is applicable to skeletonize gray-scale images from boundaries directly. We start from boundaries of gray-scale images and perform Euclidean Distance Transform on boundaries. Then we compute the gradient magnitude of the distance transform and perform isotropic vector diffusion. After diffusion, the Skeleton Strength Map (SSM) is computed and skeleton can be extracted from SSM. The experiments show that this method can obtain good performance from boundaries so long as major boundary segments are preserved. © 2008 IEEE.
{fenge}
70149084561	Symmetry of shapes via self-similarity	We describe a simple and novel approach to identify main similarity axes by maximizing self-similarity of object contour parts divided by the axes. For a symmetric or approximately symmetric shape, the main self-similarity axis coincides with the main axis of symmetry. However, the concept of the main self-similarity axis is more general, and significantly easier to compute. By identifying critical points on the contour self-similarity computation can be expressed as a discrete problem of finding two subsets of the critical points such that the two contour parts determined by the subsets are maximally similar. In other words, for each shape, we compute its division into two parts so that the parts are maximally similar. Our experimental results yield correctly placed maximal symmetry axes for articulated and highly distorted shapes. © 2008 Springer Berlin Heidelberg.
{fenge}
70349667683	CASE: Connectivity-based skeleton extraction in wireless sensor networks	Many sensor network applications are tightly coupled with the geometric environment where the sensor nodes are deployed. The topological skeleton extraction has shown great impact on the performance of such services as location, routing, and path planning in sensor networks. Nonetheless, current studies focus on using skeleton extraction for various applications in sensor networks. How to achieve a better skeleton extraction has not been thoroughly investigated. There are studies on skeleton extraction from the computer vision community; their centralized algorithms for continuous space, however, is not immediately applicable for the discrete and distributed sensor networks. In this paper we present CASE: a novel Connectivity-bAsed Skeleton Extraction algorithm to compute skeleton graph that is robust to noise, and accurate in preservation of the original topology. In addition, no centralized operation is required. The skeleton graph is extracted by partitioning the boundary of the sensor network to identify the skeleton points, then generating the skeleton arcs, connecting these arcs, and finally refining the coarse skeleton graph. Our evaluation shows that CASE is able to extract a well-connected skeleton graph in the presence of significant noise and shape variations, and outperforms state-of-the-art algorithms. © 2009 IEEE.
{fenge}
70450192878	Shape band: A deformable object detection approach	In this paper, we focus on the problem of detecting/ matching a query object in a given image. We propose a new algorithm, shape band, which models an object within a bandwidth of its sketch/contour. The features associated with each point on the sketch are the gradients within the bandwidth. In the detection stage, the algorithm simply scans an input image at various locations and scales for good candidates. We then perform fine scale shape matching to locate the precise object boundaries, also by taking advantage of the information from the shape band. The overall algorithm is very easy to implement, and our experimental results show that it can outperform stat-of-the-art contour based object detection algorithms. ©2009 IEEE.
{fenge}
73849117553	An efficient quick algorithm for computing stable skeletons	A new method to obtain high quality skeletons of binary shapes is proposed in this paper. First, a small set of salient contour points is computed by Discrete Curve Evolution (DCE). These salient points are the stable endpoints of the skeleton. Second, the skeleton is grown between pairs of the endpoints. Examining every eight-connected point of the current skeleton points, Selecte the point, that have equal distance to the contour parts which are partitioned by the two endpoints, as the new skeleton point. The skeleton path continues growing in this way until it reaches the other endpoint or another skeleton branch. The main idea is that the skeleton points are always the center of the maximal disks, and the endpoints of the skeleton are those contour points with high global curvature which is stable to noise and shape variations. The obtained skeletons are in accord with human visual perception and stable, also connected and one pixel thick. They do not require any pruning or any other post-processing. The experimental results clearly demonstrate that the proposed method significantly outperforms other well-known methods for skeleton computation. ©2009 IEEE.
{fenge}
77949875725	Learning context-sensitive shape similarity by graph transduction	Shape similarity and shape retrieval are very important topics in computer vision. The recent progress in this domain has been mostly driven by designing smart shape descriptors for providing better similarity measure between pairs of shapes. In this paper, we provide a new perspective to this problem by considering the existing shapes as a group, and study their similarity measures to the query shape in a graph structure. Our method is general and can be built on top of any existing shape similarity measure. For a given similarity measure, a new similarity is learned through graph transduction. The new similarity is learned iteratively so that the neighbors of a given shape influence its final similarity to the query. The basic idea here is related to PageRank ranking, which forms a foundation of Google Web search. The presented experimental results demonstrate that the proposed approach yields significant improvements over the state-of-art shape matching algorithms. We obtained a retrieval rate of 91.61 percent on the MPEG-7 data set, which is the highest ever reported in the literature. Moreover, the learned similarity by the proposed method also achieves promising improvements on both shape classification and shape clustering. © 2006 IEEE.
{fenge}
77950628125	Connectivity-based skeleton extraction in wireless sensor networks	Many sensor network applications are tightly coupled with the geometric environment where the sensor nodes are deployed. The topological skeleton extraction for the topology has shown great impact on the performance of such services as location, routing, and path planning in wireless sensor networks. Nonetheless, current studies focus on using skeleton extraction for various applications in wireless sensor networks. How to achieve a better skeleton extraction has not been thoroughly investigated. There are studies on skeleton extraction from the computer vision community; their centralized algorithms for continuous space, however, are not immediately applicable for the discrete and distributed wireless sensor networks. In this paper, we present a novel Connectivity-bAsed Skeleton Extraction (CASE) algorithm to compute skeleton graph that is robust to noise, and accurate in preservation of the original topology. In addition, CASE is distributed as no centralized operation is required, and is scalable as both its time complexity and its message complexity are linearly proportional to the network size. The skeleton graph is extracted by partitioning the boundary of the sensor network to identify the skeleton points, then generating the skeleton arcs, connecting these arcs, and finally refining the coarse skeleton graph. We believe that CASE has broad applications and present a skeleton-assisted segmentation algorithm as an example. Our evaluation shows that CASE is able to extract a well-connected skeleton graph in the presence of significant noise and shape variations, and outperforms the state-of-the-art algorithms. © 2010 IEEE.
{fenge}
77953186257	Active skeleton for non-rigid object detection	We present a shape-based algorithm for detecting and recognizing non-rigid objects from natural images. The existing literature in this domain often cannot model the objects very well. In this paper, we use the skeleton (medial axis) information to capture the main structure of an object, which has the particular advantage in modeling articulation and non-rigid deformation. Given a set of training samples, a tree-union structure is learned on the extracted skeletons to model the variation in configuration. Each branch on the skeleton is associated with a few part-based templates, modeling the object boundary information. We then apply sum-and-max algorithm to perform rapid object detection by matching the skeleton-based active template to the edge map extracted from a test image. The algorithm reports the detection result by a composition of the local maximum responses. Compared with the alternatives on this topic, our algorithm requires less training samples. It is simple, yet efficient and effective. We show encouraging results on two widely used benchmark image sets: the Weizmann horse dataset [7] and the ETHZ dataset [16]. ©2009 IEEE.
{fenge}
77953220881	Integrating contour and skeleton for shape classification	Shape analysis has been a long standing problem in the literature. In this paper, we address the shape classification problem and make the following contributions: (1) We combine both contour and skeleton (also local and global) information for shape analysis, and we derive an effective classifier. (2) We collect a challenging shape database in which there are 20 categories of animals, with each having 100 shapes. All these shapes are obtained from real images with a large variation in pose, viewing angle, articulation, and self-occlusion. (3) We emphasize the importance of having good representation for shape classification to address the unique characteristics of shape. A thorough experimental study is conducted showing significant improvement by the proposed algorithm over many of the state-of-the-art shape matching and classification algorithms, on both our dataset and the well-known MPEG-7 dataset [19]. In addition, we applied our algorithm for recognizing and classifying objects from natural images and obtained very encouraging results. ©2009 IEEE.
{fenge}
77956051102	Auto-context and its application to high-level vision tasks and 3D brain image segmentation	The notion of using context information for solving high-level vision and medical image segmentation problems has been increasingly realized in the field. However, how to learn an effective and efficient context model, together with an image appearance model, remains mostly unknown. The current literature using Markov Random Fields (MRFs) and Conditional Random Fields (CRFs) often involves specific algorithm design in which the modeling and computing stages are studied in isolation. In this paper, we propose a learning algorithm, auto-context. Given a set of training images and their corresponding label maps, we first learn a classifier on local image patches. The discriminative probability (or classification confidence) maps created by the learned classifier are then used as context information, in addition to the original image patches, to train a new classifier. The algorithm then iterates until convergence. Auto-context integrates low-level and context information by fusing a large number of low-level appearance features with context and implicit shape information. The resulting discriminative algorithm is general and easy to implement. Under nearly the same parameter settings in training, we apply the algorithm to three challenging vision applications: foreground/background segregation, human body configuration estimation, and scene region labeling. Moreover, context also plays a very important role in medical/brain images where the anatomical structures are mostly constrained to relatively fixed positions. With only some slight changes resulting from using 3D instead of 2D features, the auto-context algorithm applied to brain MRI image segmentation is shown to outperform state-of-the-art algorithms specifically designed for this domain. Furthermore, the scope of the proposed algorithm goes beyond image analysis and it has the potential to be used for a wide variety of problems for structured prediction problems. © 2006 IEEE.
{fenge}
77957998505	Skeleton growing and pruning with bending potential ratio	We propose a novel significance measure for skeleton pruning, called bending potential ratio (BPR), in which the decision regarding whether a skeletal branch should be pruned or not is based on the context of the boundary segment that corresponds to the branch. By considering this contextual information, we can better evaluate the contribution of the boundary segment to the overall shape, which generally depends on its particular location within the whole contour (i.e., a segment may be considered to be insignificant in one place while it may be considered as a feature elsewhere). The BPR is a measure of the significance of contour segments in such context, and depicts the bending potential of a contour segment. Unlike other significance measures that only contain local shape information, the BPR evaluates both local and global shape information. Thus, it is insensitive to local boundary deformation. In addition, we also present a scheme for skeleton growing, which integrates pruning based on the BPR measurement. Our experiments demonstrate that the skeletons obtained by the proposed algorithm are medially placed and connected. We also demonstrate that shapes reconstructed from these skeletons are very close to the original shapes. Moreover, the BPR measure yields a natural multi-scale skeletal representation. © 2010 Elsevier Ltd.
{fenge}
78149290686	Co-transduction for shape retrieval	In this paper, we propose a new shape/object retrieval algorithm, co-transduction. The performance of a retrieval system is critically decided by the accuracy of adopted similarity measures (distances or metrics). Different types of measures may focus on different aspects of the objects: e.g. measures computed based on contours and skeletons are often complementary to each other. Our goal is to develop an algorithm to fuse different similarity measures for robust shape retrieval through a semi-supervised learning framework. We name our method co-transduction which is inspired by the co-training algorithm [1]. Given two similarity measures and a query shape, the algorithm iteratively retrieves the most similar shapes using one measure and assigns them to a pool for the other measure to do a re-ranking, and vice-versa. Using co-transduction, we achieved a significantly improved result of 97.72% on the MPEG-7 dataset [2] over the state-of-the-art performances (91% in [3], 93.4% in [4]). Our algorithm is general and it works directly on any given similarity measures/metrics; it is not limited to object shape retrieval and can be applied to other tasks for ranking/retrieval. © 2010 Springer-Verlag.
{fenge}
78149396094	Object recognition using junctions	In this paper, we propose an object detection/recognition algorithm based on a new set of shape-driven features and morphological operators. Each object class is modeled by the corner points (junctions) on its contour. We design two types of shape-context like features between the corner points, which are efficient to compute and effective in capturing the underlying shape deformation. In the testing stage, we use a recently proposed junction detection algorithm [1] to detect corner points/junctions on natural images. The detection and recognition of an object are then done by matching learned shape features to those in the input image with an efficient search strategy. The proposed system is robust to a certain degree of scale change and we obtained encouraging results on the ETHZ dataset. Our algorithm also has advantages of recognizing object parts and dealing with occlusions. © 2010 Springer-Verlag.
{fenge}
78650476342	Skeleton graph matching based on critical points using path similarity	This paper proposes a novel graph matching algorithm based on skeletons and applies it to shape recognition based on object silhouettes. The main idea is to match the critical points (junction points and end points) on skeleton graphs by comparing the geodesic paths between end points and junction points of the skeleton. Our method is motivated by the fact that junction points can carry information about the global structure of an object while paths between junction points and end points can represent speci.c geometric information of local parts. Our method yields the promising accuracy rates on two shape datasets in the presence of articulations, stretching, boundary deformations, part occlusion and rotation. © Springer-Verlag 2010.
{fenge}
79958809114	Learning context-sensitive similarity by shortest path propagation	In this paper, we introduce a novel shape/object retrieval algorithm shortest path propagation (SSP). Given a query object q and a target database object p, we explicitly find the shortest path between them in the distance manifold of the database objects. Then a new distance measure between q and p is learned based on the database objects on the shortest path to replace the original distance measure. The promising results on both MEPG-7 shape dataset and a protein dataset demonstrate that our method can significantly improve the ranking of the object retrieval. © 2011 Elsevier Ltd. All rights reserved.
{fenge}
80053029663	Shape matching using points co-occurrence pattern	Shape matching is a very critical problem in computer vision, and many smart features have been designed in recent literature for improving the similarity measure between pairs of shapes, and most of them consider either distribution of the sample contour points, or convexity/concavity property of the contour. In this paper, we design a novel shape feature to capture the Co-Occurrence Pattern (COP) of the points sampled from any given shape contour, and each pattern is described by Self-Similarity which investigates the spatial cooccurrence relation among all the sample points. We test our feature on three famous shape databases: MPEG-7 CE-Shape- 1 part B, Tari1000, and Kimia99 data set for shape matching and retrieval. The experimental results show that the proposed descriptor achieves higher computational efficiency with no significant performance loss. © 2011 IEEE.
{fenge}
80855139432	Shape matching and classification using height functions	We propose a novel shape descriptor for matching and recognizing 2D object silhouettes. The contour of each object is represented by a fixed number of sample points. For each sample point, a height function is defined based on the distances of the other sample points to its tangent line. One compact and robust shape descriptor is obtained by smoothing the height functions. The proposed descriptor is not only invariant to geometric transformations such as translation, rotation and scaling but also insensitive to nonlinear deformations due to noise and occlusion. In the matching stage, the Dynamic Programming (DP) algorithm is employed to find out the optimal correspondence between sample points of every two shapes. The height function provides an excellent discriminative power, which is demonstrated by excellent retrieval performances on several popular shape benchmarks, including MPEG-7 data set, Kimia's data set and ETH-80 data set. © 2011 Elsevier B.V. All rights reserved.
{fenge}
82155197242	Shape matching and recognition using group-wised points	Shape matching/recognition is a very critical problem in the field of computer vision, and a lot of descriptors and methods have been studied in the literature. However, based on predefined descriptors, most of current matching stages are accomplished by finding the optimal correspondence between every two contour points, i.e., in a pair-wised manner. In this paper, we provide a novel matching method which is to find the correspondence between groups of contour points. The points in the same group are adjacent to each other, resulting in a strong relationship among them. Two groups are considered to be matched when the two point sequences formed by the two groups lead to a perfect one-to-one mapping. The proposed group-wised matching method is able to obtain a more robust matching result, since the co-occurrence (order) information of the grouped points is used in the matching stage. We test our method on three famous benchmarks: MPEG-7 data set, Kimia's data set and Tari1000 data set. The retrieval results show that the new group-wised matching method is able to get encouraging improvements compared to some traditional pair-wised matching approaches. © 2011 Springer-Verlag.
{fenge}
84858773329	Multiscale random fields with application to contour grouping	We introduce a new interpretation of multiscale random fields (MSRFs) that admits efficient optimization in the framework of regular (single level) random fields (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. It provides a natural representation of multiscale contour models, which is needed in order to cope with unstable contour decompositions. The append operator allows us to find optimal image segment labels using the classical framework of relaxation labeling. Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used.
{fenge}
84860159759	Co-transduction for shape retrieval	In this paper, we propose a new shape/object retrieval algorithm, namely, co-transduction. The performance of a retrieval system is critically decided by the accuracy of adopted similarity measures (distances or metrics). In shape/object retrieval, ideally, intraclass objects should have smaller distances than interclass objects. However, it is a difficult task to design an ideal metric to account for the large intraclass variation. Different types of measures may focus on different aspects of the objects: for example, measures computed based on contours and skeletons are often complementary to each other. Our goal is to develop an algorithm to fuse different similarity measures for robust shape retrieval through a semisupervised learning framework. We name our method co-transduction, which is inspired by the co-training algorithm. Given two similarity measures and a query shape, the algorithm iteratively retrieves the most similar shapes using one measure and assigns them to a pool for the other measure to do a re-ranking, and vice versa. Using co-transduction, we achieved an improved result of 97.72% (bull's-eye measure) on the MPEG-7 data set over the state-of-the-art performance. We also present an algorithm called tri-transduction to fuse multiple-input similarities, and it achieved 99.06% on the MPEG-7 data set. Our algorithm is general, and it can be directly applied on input similarity measures/metrics; it is not limited to object shape retrieval and can be applied to other tasks for ranking/retrieval. © 1992-2012 IEEE.
{fenge}
84860612541	Class-specific object contour detection by iteratively combining context information	In this paper, we propose an approach to class-specific object contour detection. Unlike traditional edge/boundary detection algorithms which are designed to detect characteristic changes in brightness, color, and texture, class-specific object contour detection involves the concept of object. It aims to capture the main structure (outline) of the object of interest and suppress the strong edge/boundary responses in the background and interior of the object at the same time. Towards this end, we formulate class-specific object contour detection as a supervised learning problem and adopt auto-context to mine the context information (structure similarity among object instances) and combine it with the appearance information in an iterative manner. Experiments on the Weizmann Horse dataset demonstrate that the proposed method is effective and efficient. © 2011 IEEE.
{fenge}
84860639333	Maximal cliques that satisfy hard constraints with application to deformable object model learning	We propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints. The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution. The proposed inference is based on a novel particle filter algorithm with state permeations. We apply the inference framework to a challenging problem of learning part-based, deformable object models. Two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints. Our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.
{fenge}
84863068312	Image labeling by multiple segmentation	In this paper, we provide a method for image labeling by combining the local features and contextual cues in a multiple segmentation framework. Our main insight is to weight the classification results of each image region in different levels, which are obtained by a series of learned discriminative models based on bag of features. The contextual cues are implicitly embedded as feature selection in learning process. Multiple segmentation framework provides robust representation, allowing a wide variety of cues to contribute to the confidence in each semantic label. Our algorithm has been applied on the lotus hill institute(LHI) 15-class dataset and outperforms other state-of-the-art methods. © 2011 IEEE.
{fenge}
84864316772	Research and perspective on shape matching	Shape matching and classification are important issues in computer vision. In recent years, contour-based shape matching approaches (e.g., shape context) and skeleton-based shape matching methods (e.g., shock graph) both have a lot of developments. In this paper, we introduce the basic concept of shape matching, give the difficulties of this topic, and provide a detailed review on the most recent approaches about shape representation and matching for both contour-based and skeleton-based methods. We also give a brief introduction about metric learning based shape retrieval. Moreover, we introduce some widely used benchmarks for shape matching in details, along with some hot topics including partial shape matching, shape classification, etc. Finally, this paper concludes with the whole framework of shape matching and the application perspective of this topic.
{fenge}
84866720203	Exemplar-based human action pose correction and tagging	The launch of Xbox Kinect has built a very successful computer vision product and made a big impact to the gaming industry; this sheds lights onto a wide variety of potential applications related to action recognition. The accurate estimation of human poses from the depth image is universally a critical step. However, existing pose estimation systems exhibit failures when faced severe occlusion. In this paper, we propose an exemplar-based method to learn to correct the initially estimated poses. We learn an inhomogeneous systematic bias by leveraging the exemplar information within specific human action domain. Our algorithm is illustrated on both joint-based skeleton correction and tag prediction. In the experiments, significant improvement is observed over the contemporary approaches, including what is delivered by the current Kinect system. © 2012 IEEE.
{fenge}
84866905357	Skeleton extraction from incomplete boundaries in sensor networks based on distance transform	We study the problem of skeleton extraction for large-scale sensor networks using only connectivity information. Existing solutions for this problem heavily depend on an algorithm that can accurately detect network boundaries. This dependence may seriously affect the effectiveness of skeleton extraction. For example, in low density networks, boundary detection algorithms normally do not work well, potentially leading to an incorrect skeleton being generated. This paper proposes a novel approach, named DIST, to skeleton extraction from incomplete boundaries using the idea of distance transform, a concept in the computer graphics area. The main contribution is a distributed and low-cost algorithm that produces accurate network skeletons without requiring that the boundaries be complete or tight. The algorithm first establishes the network's distance transform - the hop distance of each node to the network's boundaries. Based on this, some critical skeleton nodes are identified. Next, a set of skeleton arcs are generated by controlled flooding; connecting these skeleton arcs then gives us a coarse skeleton. The algorithm finally refines the coarse skeleton by building shortest path trees, followed by a prune phase. The obtained skeletons are robust to boundary noise and shape variations. © 2012 IEEE.
{fenge}
84867870218	Face identification using reference-based features with message passing model	In this paper, we propose a system for face identification. Given two query face images, our task is to tell whether or not they are of the same person. The main contribution of this paper comes from two aspects: (1) We adopt the one-shot similarity kernel [35] for learning the similarity of two face images. The learned similarity measures are then used to map a face image to reference images. (2) We propose a graph-based method for selecting an optimal set of reference images. Instead of directly working on the image features, we use the learned similarity to the reference images as the new features and compute the corresponding matching score of the two query images. Our approach is effective and easy to implement. We show encouraging and favorable results on the "Labeled Faces in the Wild" - a challenging data set of faces. © 2012 Elsevier B.V.
{fenge}
84871793385	Skeleton pruning as trade-off between skeleton simplicity and reconstruction error	Skeletons can be viewed as a compact shape representation in that each shape can be completely reconstructed from its skeleton. However, the usefulness of a skeletal representation is strongly limited by its instability. Skeletons suffer from contour noise in that small contour deformation may lead to large structural changes in the skeleton. A large number of skeleton computation and skeleton pruning approaches has been proposed to address this issue. Our approach differs fundamentally in the fact that we cast skeleton pruning as a trade-off between skeleton simplicity and shape reconstruction error. An ideal skeleton of a given shape should be the skeleton with a simplest possible structure that provides a best possible reconstruction of a given shape. To quantify this trade-off, we propose that the skeleton simplicity corresponds to model simplicity in the Bayesian framework, and the shape reconstruction accuracy is expressed as goodness of fit to the data. We also provide a simple algorithm to approximate the maximum of the Bayesian posterior probability which defines an order for iteratively removing the end branches to obtain the pruned skeleton. Presented experimental results obtained without any parameter tuning clearly demonstrate that the resulting skeletons are stable to boundary deformations and intra class shape variability. © 2013 Science China Press and Springer-Verlag Berlin Heidelberg.
{fenge}
84874555523	Online Random Ferns for robust visual tracking	Recently many appearance based visual tracking algorithms have been investigated, aimed at building robust appearance models against challenges brought by the varying appearance of the target as well as the unconstrained environment. More often adaptive appearance models were used to capture these variances over time, but this may sometimes result in losing the target (drifting) due to inappropriate update of the model. In this paper an online form of Random Ferns classifier is proposed to accomplish the task of robust appearance modeling with a constrained updating strategy against the potential incorrect update induced by runtime noise. Experiments on challenging benchmark video sequences have been conducted and improvement is observed when compared with recent state-of-the-art algorithms. © 2012 ICPR Org Committee.
{fenge}
84874571392	Adjacent coding for image classification	The locality and sparsity constrained encoding methods have shown the good image classification performance in recent papers. Among these methods, the common strategy is encoding one descriptor into one code by a learned codebook and then applying SPM and Pooling strategy to get the final image representation. However, the ignorance of local spatial context has been a barrier to improve their discriminative power. To address this problem, we propose the so called Adjacent Coding (AC), which employs the adjacency of one descriptor to express the local spatial context. Different from traditional coding methods, Adjacent Coding encodes one descriptor and its adjacent neighbors together. In this paper, we further show that AC also keeps the properties of locality and sparsity. Finally, our experiments on the standard benchmarks (Scene 15 and Caltech 101) show our method can outperform the state-of-the-art feature coding methods. © 2012 ICPR Org Committee.
{fenge}
84874937758	Densifying distance spaces for shape and image retrieval	Sparse data sets are an ever-present problem in many fields of computer science. In the shape retrieval community, several researchers use graph transduction algorithms to reveal the underlying structure of the shape manifold. Without an infinite number of shapes, the data sets can only imprecisely describe the shape manifold. For this problem, adding synthetic data points can be very effective. However existing methods add synthetic points only in feature space. In distance spaces, which are often non-metric and are widely used in bioinformatics, time series classification, shape similarity, and other domains, it is impossible to use these standard, feature-based methods, such as SMOTE, to insert synthetic points. Instead, we present an innovative approach that adds synthetic points directly to distance spaces. We call these synthetic points ghost points since they are not represented by vectors of features, and consequently, cannot be directly visualized. However, we can define the distances of ghost points to all other data points. Our experimental results on standard data sets show that ghost points not only significantly improve the accuracy of shape retrieval, but also the accuracy of image retrieval. We also discuss the conditions that allow the ghost points to improve retrieval results. © 2012 Springer Science+Business Media, LLC.
{fenge}
84875908344	One-class multiple instance learning via robust PCA for common object discovery	Principal component analysis (PCA), as a key component in statistical learning, has been adopted in a wide variety of applications in computer vision and machine learning. From a different angle, weakly supervised learning, more specifically multiple instance learning (MIL), allows fine-grained information to be exploited from coarsely-grained label information. In this paper, we propose an algorithm using the robust PCA (RPCA) [1] in a iterative way to perform simultaneous common object discovery and model learning under a one-class multiple instance learning setting. We show the advantage of our method on common object discovery and model learning, which needs no fine/coarse alignment in the input data; in addition, it achieves comparable results with standard two-class MIL learning algorithms but our method is learning from one-class data only. © 2013 Springer-Verlag.
{fenge}
84881169489	Rotation-Invariant Features for Multi-Oriented Text Detection in Natural Images	Texts in natural scenes carry rich semantic information, which can be used to assist a wide range of applications, such as object recognition, image/video retrieval, mapping/navigation, and human computer interaction. However, most existing systems are designed to detect and recognize horizontal (or near-horizontal) texts. Due to the increasing popularity of mobile-computing devices and applications, detecting texts of varying orientations from natural images under less controlled conditions has become an important but challenging task. In this paper, we propose a new algorithm to detect texts of varying orientations. Our algorithm is based on a two-level classification scheme and two sets of features specially designed for capturing the intrinsic characteristics of texts. To better evaluate the proposed method and compare it with the competing algorithms, we generate a comprehensive dataset with various types of texts in diverse real-world scenes. We also propose a new evaluation protocol, which is more suitable for benchmarking algorithms for detecting texts in varying orientations. Experiments on benchmark datasets demonstrate that our system compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on variant texts in complex natural scenes. © 2013 Yao et al.
{fenge}
84881061691	Distance transform-based skeleton extraction and its applications in sensor networks	We study the problem of skeleton extraction for large-scale sensor networks with reliance purely on connectivity information. Existing efforts in this line highly depend on the boundary detection algorithms, which are used to extract accurate boundary nodes. One challenge is that in practical this could limit the applicability of the boundary detection algorithms. For instance, in low node density networks where boundary detection algorithms do not work well, the extracted boundary nodes are often incomplete. This paper brings a new view to skeleton extraction from a distance transform perspective, bridging the distance transform of the network and the incomplete boundaries. As such, we propose a distributed and scalable algorithm for skeleton extraction, called DIST, based on DIStance(T)ransform, while incurring low communication overhead. The proposed algorithm does not require that the boundaries are complete or accurate, which makes the proposed algorithm more practical in applications. First, we compute the distance transform of the network. Specifically, the distance (hop count) of each node to the boundaries of a sensor network is estimated. The node map consisting of the distance values is considered as the distance transform (the distance map). The distance map is then used to identify skeleton nodes. Next, skeleton arcs are generated by controlled flooding within the identified skeleton nodes, thereby connecting these skeleton arcs, to extract a coarse skeleton. Finally, we refine the coarse skeleton by building shortest path trees followed by a prune phase. The obtained skeleton is robust to boundary noise or shape variations. Besides, we present two specific applications that benefit from the extracted skeleton: identifying complete boundaries and shape segmentation. First, with the extracted skeleton using DIST, we propose to identify more boundary nodes to form a meaningful boundary curve. Second, the utilization of the derived skeleton to segment the network into approximately convex pieces has been shown to be effective. © 1990-2012 IEEE.
{fenge}
84893502251	Robust subspace discovery via relaxed rank minimization	This letter examines the problem of robust subspace discovery from input data samples (instances) in the presence of overwhelming outliers and corruptions. A typical example is the case where we are given a set of images; each image contains, for example, a face at an unknown location of an unknown size; our goal is to identify or detect the face in the image and simultaneously learn its model. We employ a simple generative subspace model and propose a new formulation to simultaneously infer the label information and learn the model using low-rank optimization. Solving this problem enables us to simultaneously identify the ownership of instances to the subspace and learn the corresponding subspace model.We give an efficient and effective algorithm based on the alternating direction method of multipliers and provide extensive simulations and experiments to verify the effectiveness of our method. The proposed scheme can also be used to tackle many related high-dimensional combinatorial selection problems. © 2014 Massachusetts Institute of Technology.
{fenge}
84894378983	Bag of contour fragments for robust shape classification	Shape representation is a fundamental problem in computer vision. Current approaches to shape representation mainly focus on designing low-level shape descriptors which are robust to rotation, scaling and deformation of shapes. In this paper, we focus on mid-level modeling of shape representation. We develop a new shape representation called Bag of Contour Fragments (BCF) inspired by classical Bag of Words (BoW) model. In BCF, a shape is decomposed into contour fragments each of which is then individually described using a shape descriptor, e.g., the Shape Context descriptor, and encoded into a shape code. Finally, a compact shape representation is built by pooling shape codes in the shape. Shape classification with BCF only requires an efficient linear SVM classifier. In our experiments, we fully study the characteristics of BCF, show that BCF achieves the state-of-the-art performance on several well-known shape benchmarks, and can be applied to real image classification problem. © 2014 Elsevier Ltd.
{fenge}
84897750207	Traffic sign classification using two-layer image representation	This paper makes use of locality-constrained linear coding (LLC) in a two-layer image representation framework for traffic sign recognition. As a multi-category classification problem with unbalanced frequencies and variations, many machine learning approaches have been adopted with some low level features for traffic sign recognition. To the best of our knowledge, this is the first method using coding features for traffic sign recognition. First, we extract features(dense SIFT features, HOG features and LBP features) and encode them with a k-means generated codebook and LLC. Second, each traffic sign image is represented by the features generated by spatial pyramid matching (SPM). Then, all the image representations from each kind of features are concatenated together as the final image representation. Finally, we show that a linear SVM classifier trained with this image representation can achieve the state-of-the-art recognition rate of 99.67% on the well-known German Traffic Sign Recognition Benchmark. © 2013 IEEE.
{fenge}
84898297707	Shape retrieval and classification based on geodesic paths in skeleton graphs	Skeleton- is well-known to be superior to contour-based representation when shapes have large nonlinear variability, especially articulation. However, approaches to shape similarity based on skeletons suffer from the instability of skeletons, and matching of skeleton graphs is still an open problem. To deal with this problem for shape retrieval, the authors first propose to match skeleton graphs by comparing the geodesic paths between skeleton endpoints. In contrast to typical tree or graph matching methods, they do not explicitly consider the topological graph structure. Their approach is motivated by the fact that visually similar skeleton graphs may have completely different topological structures, while the paths between their end nodes still remain similar. The proposed comparison of geodesic paths between endpoints of skeleton graphs yields correct matching results in such cases. The experimental results demonstrate that the method is able to produce correct results in the presence of articulations, stretching, and contour deformations. The authors also utilize the geodesic skeleton paths for shape classification. Similar to shape retrieval, direct graph matching algorithms like graph edit distance have great difficulties with the instability of the skeleton graph structure. In contrast, the representation based on skeleton paths remains stable. Therefore, a simple Bayesian classifier is able to obtain excellent shape classification results. © 2013, IGI Global.
{fenge}
84903199091	Exemplar-based human action pose correction	The launch of Xbox Kinect has built a very successful computer vision product and made a big impact on the gaming industry. This sheds lights onto a wide variety of potential applications related to action recognition. The accurate estimation of human poses from the depth image is universally a critical step. However, existing pose estimation systems exhibit failures when facing severe occlusion. In this paper, we propose an exemplar-based method to learn to correct the initially estimated poses. We learn an inhomogeneous systematic bias by leveraging the exemplar information within a specific human action domain. Furthermore, as an extension, we learn a conditional model by incorporation of pose tags to further increase the accuracy of pose correction. In the experiments, significant improvements on both joint-based skeleton correction and tag prediction are observed over the contemporary approaches, including what is delivered by the current Kinect system. Our experiments for the facial landmark correction also illustrate that our algorithm can improve the accuracy of other detection/estimation systems. © 2013 IEEE.
{fenge}
84904600960	Scale-space SIFT flow	The state-of-the-art SIFT flow has been widely adopted for the general image matching task, especially in dealing with image pairs from similar scenes but with different object configurations. However, the way in which the dense SIFT features are computed at a fixed scale in the SIFT flow method limits its capability of dealing with scenes of large scale changes. In this paper, we propose a simple, intuitive, and very effective approach, Scale-Space SIFT flow, to deal with the large scale differences in different image locations. We introduce a scale field to the SIFT flow function to automatically explore the scale deformations. Our approach achieves similar performance as the SIFT flow method on general natural scenes but obtains significant improvement on the images with large scale differences. Compared with a recent method that addresses the similar problem, our approach shows its clear advantage being more effective, and significantly less demanding in memory and time requirement. © 2014 IEEE.
{fenge}
84905247930	Shape vocabulary: A robust and efficient shape representation for shape matching	In this paper, a learning-based shape descriptor for shape matching is demonstrated. Formulated in a bag-of-words like framework, the proposed method summarizes the local features extracted from certain shape to generate a integrated representation. It contributes to the speed-up of shape matching, since the distance metric in the vector space analysis can be directly applied to compare the constructed global descriptors, eliminating the time consuming stage of local feature matching. Similar to the philosophy in spatial pyramid matching, a strategy for feature division is applied in the phase of encoded feature pooling and vocabulary learning, which helps to construct a more discriminative descriptor incorporating both global and local information. Also, a local contour-based feature extraction method is designed for 2D shapes, while significant properties of the local contours are inspected for the design of feature division rules. The designed local feature extraction method and the feature division rules manage to reduce the variances of shape representation due to the changes in rotation. In addition to 2D shape, we also present a simple and natural method to extend the proposed method to the scenario of 3D shape representation. The proposed shape descriptor is validated on several benchmark data sets for evaluating 2D and 3D shape matching algorithms, and it is observed that the investigated shape descriptor maintains superior discriminative power as well as high time efficiency. © 1992-2012 IEEE.
{fenge}
84906498729	Human detection using learned part alphabet and pose dictionary	As structured data, human body and text are similar in many aspects. In this paper, we make use of the analogy between human body and text to build a compositional model for human detection in natural scenes. Basic concepts and mature techniques in text recognition are introduced into this model. A discriminative alphabet, each grapheme of which is a mid-level element representing a body part, is automatically learned from bounding box labels. Based on this alphabet, the flexible structure of human body is expressed by means of symbolic sequences, which correspond to various human poses and allow for robust, efficient matching. A pose dictionary is constructed from training examples, which is used to verify hypotheses at runtime. Experiments on standard benchmarks demonstrate that the proposed algorithm achieves state-of-the-art or competitive performance. © 2014 Springer International Publishing.
{fenge}
84907546391	A unified framework for multioriented text detection and recognition	High level semantics embodied in scene texts are both rich and clear and thus can serve as important cues for a wide range of vision applications, for instance, image understanding, image indexing, video search, geolocation, and automatic navigation. In this paper, we present a unified framework for text detection and recognition in natural images. The contributions of this paper are threefold: 1) text detection and recognition are accomplished concurrently using exactly the same features and classification scheme; 2) in contrast to methods in the literature, which mainly focus on horizontal or near-horizontal texts, the proposed system is capable of localizing and reading texts of varying orientations; and 3) a new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters. As an additional contribution, a novel image database with texts of different scales, colors, fonts, and orientations in diverse real-world scenarios, is generated and released. Extensive experiments on standard benchmarks as well as the proposed database demonstrate that the proposed system achieves highly competitive performance, especially on multioriented texts.
{fenge}
84907568883	Strokelets: A learned multi-scale representation for scene text recognition	Driven by the wide range of applications, scene text detection and recognition have become active research topics in computer vision. Though extensively studied, localizing and reading text in uncontrolled environments remain extremely challenging, due to various interference factors. In this paper, we propose a novel multi-scale representation for scene text recognition. This representation consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at different granularities. Strokelets possess four distinctive advantages: (1) Usability: automatically learned from bounding box labels, (2) Robustness: insensitive to interference factors, (3) Generality: applicable to variant languages, and (4) Expressivity: effective at describing characters. Extensive experiments on standard benchmarks verify the advantages of strokelets and demonstrate the effectiveness of the proposed algorithm for text recognition.
{fenge}
84907663795	Vehicle color recognition on urban road by feature context	Vehicle information recognition is a key component of intelligent transportation systems. Color plays an important role in vehicle identification. As a vehicle has its inner structure, the main challenge of vehicle color recognition is to select the region of interest (ROI) for recognizing its dominant color. In this paper, we propose a method to implicitly select the ROI for color recognition. Preprocessing is performed to overcome the influence of image quality degradation. Then, the ROI in vehicle images is selected by assigning the subregions with different weights that are learned by a classifier trained on the vehicle images. We train the classifier by linear support vector machine for its efficiency and high precision. The experiments are extensively validated on both images and videos, which are collected on urban roads. The proposed method outperforms other competing color recognition methods.
{fenge}
84914818670	Shape recognition by combining contour and skeleton into a mid-level representation	Contour and skeleton are two main stream representations for shape recognition in the literature. It has been shown that such two representations convey complementary information, however combining them in a nature way is nontrivial, as they are generally abstracted by different structures (closed string vs graph), respectively. This paper aims at addressing the shape recognition problem by combining contour and skeleton into a mid-level of shape representation. To form a midlevel representation for shape contours, a recent work named Bag of Contour Fragments (BCF) is adopted; While for skeleton, a new midlevel representation named Bag of Skeleton Paths (BSP) is proposed, which is formed by pooling the skeleton codes by encoding the skeleton paths connecting pairs of end points in the skeleton. Finally, a compact shape feature vector is formed by concatenating BCF with BSP and fed into a linear SVM classifier to recognize the shape. Although such a concatenation is simple, the SVM classifier can automatically learn the weights of contour and skeleton features to offer discriminative power. The encouraging experimental results demonstrate that the proposed new shape representation is effective for shape classification and achieves the state-of-the-art performances on several standard shape benchmarks.
{fenge}
84920743358	Deep learning representation using autoencoder for 3D shape retrieval	We study the problem of how to build a deep learning representation for 3D shape. Deep learning has shown to be very effective in variety of visual applications, such as image classification and object detection. However, it has not been successfully applied to 3D shape recognition. This is because 3D shape has complex structure in 3D space and there are limited number of 3D shapes for feature learning. To address these problems, we project 3D shapes into 2D space and use autoencoder for feature learning on the 2D images. High accuracy 3D shape retrieval performance is obtained by aggregating the features learned on 2D images. In addition, we show the proposed deep learning feature is complementary to conventional local image descriptors. By combing the global deep learning representation and the local descriptor representation, our method can obtain the state-of-the-art performance on 3D shape retrieval benchmarks.
