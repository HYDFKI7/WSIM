{fenge}
84937389173	An open framework for smart and personalized distance learning	Web based learning enables more students to have access to the distance-learning environment and provides students and teachers with unprecedented flexibility and convenience. However, the early experience of using this new learning means in China exposes a few problems. Among others, teachers accustomed to traditional teaching methods often find it difficult to put their courses online and some students, especially the adult students, find themselves overloaded with too much information. In this paper, we present an open framework to solve these two problems. This framework allows students to interact with an automated question answering system to get their answers. It enables teachers to analyze students learning patterns and organize the webbased contents efficiently. The framework is intelligent due to the data mining and case-based reasoning features, and user-friendly because of its personalized services to both teachers and students.
{fenge}
84942927438	Patterns discovery based on time-series decomposition	Complete or partial periodicity search in time-series databases is an interesting data mining problem. Most previous studies on finding periodic or partial periodic patterns focused on data struc- tures and computing issues. Analysis of long-term or short-term trends over difierent time windows is a great interest. This paper presents a new approach to discovery of periodic patterns from time-series with trends based on time-series decomposition. First, we decompose time series into three components, seasonal, trend and noise. Second, with an existing partial periodicity search algorithm, we search either par- tial periodic patterns from trends without seasonal component or partial periodic patterns for seasonal components. Difierent patterns from any combination of the three decomposed time-series can be found using this approach. Examples show that our approach is more flexible and suitable to mine periodic patterns from time-series with trends than the previous reported methods.
{fenge}
84943160561	A cube model and cluster analysis for web access sessions	Identification of the navigational patterns of casual visitors is an important step in online recommendation to convert casual visitors to customers in e-commerce. Clustering and sequential analysis are two primary techniques for mining navigational patterns from Web and application server logs. The characteristics of the log data and mining tasks require new data representation methods and analysis algorithms to be tested in the e-commerce environment. In this paper we present a cube model to represent Web access sessions for data mining. The cube model organizes session data into three dimensions. The COMPONENT dimension represents a session as a set of ordered components {c
{fenge}
9744267260	Statistical models for time sequences data mining	In this paper, we present an adaptive modelling technique for studying past behaviors of objects and predicting the near future events. Our approach is to define a sliding window (of different window sizes) over a time sequence and build autoregression models from subsequences in different windows. The models are representations of past behaviors of the sequence objects. We can use the AR coefficients as features to index subsequences to facilitate the query of subsequences with similar behaviors. We can use a clustering algorithm to group time sequences on their similarity in the feature space. We can also use the AR models for prediction within different windows. Our experiments show that the adaptive model can give better prediction than non-adaptive models.
{fenge}
9944225638	Real-time transaction processing for autonomic Grid applications	The future Grid will be an autonomie environment that can not only assist users to share large-scale resources and accomplish collaborative tasks but also self-manage to reduce the users' interventions as much as possible. In such an autonomie Grid environment, the real-time transaction processing is a key and challenging technology to protect systems from various failures. This paper presents an autonomie real-time transaction service (ARTTS) that can (1) dynamically discover Grid services as participants to execute specified sub-transactions, (2) coordinate these participants to achieve the real-time and transactional requirements, and (3) assign priorities to schedule concurrent transactions. Petri nets are used to model and validate the. coordination algorithms of real-time Grid transactions and experiment result demonstrates the feasibility of the ARTTS and the performance of the algorithms in different workloads. By handling the potential failures and exceptions autonomically, the ARTTS can facilitate the implementation of real-time Grid transactions and simplify the system management work, which frees users from the complex interference in the autonomie Grid environment. © 2004 Elsevier Ltd. All rights reserved.
{fenge}
9444284442	A feature weighting approach to building classification models by interactive clustering	In using a classified data set to test clustering algorithms, the data points in a class are considered as one cluster (or more than one) in space. In this paper we adopt this principle to build classification models through interactively clustering a training data set to construct a tree of clusters. The leaf clusters of the tree are selected as decision clusters to classify new data based on a distance function. We consider the feature weights in calculating the distances between a new object and the center of a decision cluster. The new algorithm, W-k-means, is used to automatically calculate the feature weights from the training data. The Fastmap technique is used to handle outliers in selecting decision clusters. This step increases the stability of the classifier. Experimental results on public domain data sets have shown that the models built using this clustering approach outperformed some popular classification algorithms.
{fenge}
13944279215	A categorized-registry model for grid resource publication and discovery using software agents	This paper presents a categorized-registry model for Grid Resource Publication and Discovery(GRPD) using software agents. We use Common Resource Model(CRM) specification to describe manageable resources as Open Grid Services Architecture(OGSA) services. We also adopt two-level registry mechanism to register corresponding resources of a Virtual Organization(VO) in order to obtain high GRPD efficiency. The "Index" node of a VO hosts the general registry. Other specific categorized registries are distributed in the VO. A large-scale Grid system may contain many VOs. "Index" nodes from various VOs connect to each other in Peer-to-Peer(P2P) mode instead of hierarchical mode. Software agent is a powerful high-level tool for modeling a complex software system. It is adopted to implement GRPD efficiently.
{fenge}
18144363375	Web services: Problems and future directions	Recently, Web services have generated great interests in both vendors and researchers. Web services, based on existing Internet protocols and open standards, can provide a flexible solution to the problem of application integration. With the help of WSDL, SOAP, and UDDI, Web services are becoming popular in Web applications. However, the current Web services architectures are confronted with a few stubborn problems, for instance, security. In this paper, we shall give an overview of these problems. We believe that solving these problems will become crucial to success of Web services. In the end, we predict distinct advances in semantic Grid services. © 2004 Elsevier B.V. All rights reserved.
{fenge}
18144419389	Automated variable weighting in k-means type clustering	This paper proposes a k-means type clustering algorithm that can automatically calculate variable weights. A new step is introduced to the k-means clustering process to iteratively update variable weights based on the current partition of data and a formula for weight calculation is proposed. The convergency theorem of the new clustering process is given. The variable weights produced by the algorithm measure the importance of variables in clustering and can be used in variable selection in data mining applications where large and complex real data are often involved. Experimental results on both synthetic and real data have shown that the new algorithm outperformed the standard k-means type algorithms in recovering clusters in data. © 2005 IEEE.
{fenge}
24944557601	A frequent pattern discovery method for outlier detection	An outlier in a dataset is an observation or a point that is considerably dissimilar to or inconsistent with the remainder of the data. Detection of outliers is important for many applications and has recently attracted much attention in the data mining research community. In this paper, we present a new method to detect outliers by discovering frequent patterns (or frequent itemsets) from the data set. The outliers are defined as the data transactions which contain less frequent patterns in their itemsets. We define a measure called FPOF (Frequent Pattern Outlier Factor) to detect the outlier transactions and propose the Find FPOF algorithm to discover outliers. The experimental results show that our approach outperformed the existing methods on identifying interesting outliers. © Springer-Verlag 2004.
{fenge}
26944481948	Subspace clustering of text documents with feature weighting k-means algorithm	This paper presents a new method to solve the problem of clustering large and complex text data. The method is based on a new subspace clustering algorithm that automatically calculates the feature weights in the k-means clustering process. In clustering sparse text data the feature weights are used to discover clusters from subspaces of the document vector space and identify key words that represent the semantics of the clusters. We present a modification of the published algorithm to solve the sparsity problem that occurs in text clustering. Experimental results on real-world text data have shown that the new method outper-formed the Standard K Means and Bisection-KMeans algorithms, while still maintaining efficiency of the k-means clustering process. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
29144450066	ADDI: An agent-based extension to UDDI for supply chain management	This paper proposes a new mediate module ADDI to organize the UDDI nodes published by the different corporations on supply chain and establish a UDDI dynamic ranking model. We suggest a novel model for supply chain conduct classification, and define the supply chain entity conduct types as a standard for web service classification and UDDI ranlang. In this model, Web service-oriented technologies and protocols are deployed for modeling, managing and executing business-oriented functionalities and environments. We focus on the efficient integration of supply chain as key points to harmonize these technologies. And agentorientation concepts and technologies are applied for SCM construction and interaction patterns.
{fenge}
33646827964	An ontology-based model for grid resource publication and discovery	Resource management system is the core component of a Grid system. It has two important functions: resource publication and discovery. This paper presents an ontology-based model for Grid Resource Publication and Discovery(GRPD). We adopt multiple domainspecific registries to manage corresponding resources of a Virtual Organization(VO) in order to obtain high GRPD efficiency. Resource descriptions and resource requests are all based on domain-specific ontology. The ontology-based matchmaker of the domain-specific registry plays the important role in resources selection. The "Index" node of a VO hosts the general registry. Other domain-specific registries are distributed in the VO. This is a two-level registry mechanism. A large-scale Grid system may contain many VOs. "Index" nodes from various VOs connect to each other in the peer-to-peer mode instead of the hierarchical mode. © Springer-Verlag 2004.
{fenge}
33646509173	On the performance of feature weighting K-means for text subspace clustering	Text clustering is an effective way of not only organizing textual information, but discovering interesting patterns. Most existing methods, however, suffer from two main drawbacks; they cannot provide an understandable representation for text clusters, and cannot scale to very large text collections. Highly scalable text clustering algorithms are becoming increasingly relevant. In this paper, we present a performance study of a new subspace clustering algorithm for large sparse text data. This algorithm automatically calculates the feature weights in the k-means clustering process. The feature weights are used to discover clusters from subspaces of the text vector space and identify terms that represent the semantics of the clusters. A series of experiments have been conducted to test the performance of the algorithm, including resource consumption and clustering quality. The experimental results on real-world text data have shown that our algorithm quickly converges to a local optimal solution and is scalable to the number of documents, terms and the number of clusters. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
33745517930	Supplier categorization with K-means type subspace clustering	Many large enterprises work with thousands of suppliers to provide raw materials, product components and final products. Supplier relationship management (SRM) is a business strategy to reduce logistic costs and improve business performance and competitiveness. Effective categorization of suppliers is an important step in supplier relationship management. In this paper, we present a data-driven method to categorize suppliers from the suppliers' business behaviors that are derived from a large number of business transactions between suppliers and the buyer. A supplier business behavior is described as the set of product items it has provided in a given time period, a mount of each item in each order, the frequencies of orders, as well as other attributes such as product quality, product arrival time, etc. Categorization of suppliers based on business behaviors is a problem of clustering high dimensional data. We used the k-means type subspace clustering algorithm FW-KMeans to solve this high dimensional, sparse data clustering problem. We have applied this algorithm to a real data set from a food retail company to categorize over 1000 suppliers based on 11 months transaction data. Our results have produced better groupings of suppliers which can enhance the company's SRM. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33745556564	MFCRank: A Web ranking algorithm based on correlation of multiple features	This paper presents a new ranking algorithm MFCRank for topic-specific Web search systems. The basic idea is to correlate two types of similarity information into a unified link analysis model so that the rich content and link features in Web collections can be exploited efficiently to improve the ranking performance. First, a new surfer model JBC is proposed, under which the topic similarity information among neighborhood pages is used to weigh the jumping probability of the surfer and to direct the surfing activities. Secondly, as JBC surfer model is still query-independent, a correlation between the query and JBC is essential. This is implemented by the definition of MFCRank score, which is the linear combination of JBC score and the similarity value between the query and the matched pages. Through the two correlation steps, the features contained in the plain text, link structure, anchor text and user query can be smoothly correlated in one single ranking model. Ranking experiments have been carried out on a set of topic-specific Web page collections. Experimental results showed that our algorithm gained great improvement with regard to the ranking precision. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33745639140	Adaptive job scheduling for a service grid using a genetic algorithm	This paper presents a new approach to scheduling jobs on a service Grid using a genetic algorithm (GA). A fitness function is defined to minimize the average execution time of scheduling N jobs to M(≤ N) machines on the Grid. Two models are proposed to predict the execution time of a single job or multiple jobs on each machine with varied system load. The single service type model is used to schedule jobs of one single service to a machine while the multiple service types model schedules jobs of multiple services to a machine. The predicted execution times from these models are used as input to the genetic algorithm to schedule N jobs to M machines on the Grid. Experiments on a small Grid of four machines have shown a significant reduction of the average execution time by the new job scheduling approach. © Springer-Verlag 2004.
{fenge}
33745779262	A fast greedy algorithm for outlier mining	The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. Recently, the problem of outlier detection in categorical data is defined as an optimization problem and a local-search heuristic based algorithm (LSA) is presented. However, as is the case with most iterative type algorithms, the LSA algorithm is still very time-consuming on very large datasets. In this paper, we present a very fast greedy algorithm for mining outliers under the same optimization model. Experimental results on real datasets and large synthetic datasets show that: (1) Our new algorithm has comparable performance with respect to those state-of-the-art outlier detection algorithms on identifying true outliers and (2) Our algorithm can be an order of magnitude faster than LSA algorithm. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33745781290	Neighborhood density method for selecting initial cluster centers in K-means clustering	This paper presents a new method for effectively selecting initial cluster centers in k-means clustering. This method identifies the high density neighborhoods from the data first and then selects the central points of the neighborhoods as initial centers. The recently published Neighborhood-Based Clustering (NBC) algorithm is used to search for high density neighborhoods. The new clustering algorithm NK-means integrates NBC into the k-means clustering process to improve the performance of the k-means algorithm while preserving the k-means efficiency. NBC is enhanced with a new cell-based neighborhood search method to accelerate the search for initial cluster centers. A merging method is employed to filter out insignificant initial centers to avoid too many clusters being generated. Experimental results on synthetic data sets have shown significant improvements in clustering accuracy in comparison with the random k-means and the refinement k-means algorithms. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
33846867028	Automatic transaction compensation for reliable grid applications	As grid technology is expanding from scientific computing to business applications, service oriented grid computing is aimed at providing reliable services for users and hiding complexity of service processes from them. The grid services for coordinating long-lived transactions that occur in business applications play an important role in reliable grid applications. In this paper, the grid transaction service (GridTS) is proposed for dealing with long-lived business transactions. We present a compensation-based long-lived transaction coordination algorithm that enables users to select results from committed sub-transactions. Unlike other long-lived transaction models that require application programmers to develop corresponding compensating transactions, GridTS can automatically generate compensating transactions on execution of a long-lived grid transaction. The simulation result has demonstrated the feasibility of GridTS and effectiveness of the corresponding algorithm. © Springer Science + Business Media, Inc. 2006.
{fenge}
34250750210	SLF4SS: Facilitating flexible services selection	In this paper, we present SLF4SS, a self-learning framework for services selection. The main features of SLF4SS include (1) learning from previous match samples to help users discover more appropriate services, (2) using multi-dimensional properties to represent services for evaluation and selection, (3) optimizing the overall property of composite service appropriate to customer's constraints and preferences, and (4) addressing user's uncertain, vague requests. SLF4SS can simplify selection of suitable Web services in building high level services for various business applications, reduce implementation cost, and shorten the time of deploying enterprises applications based on SOA. © 2006 IEEE.
{fenge}
34347228671	An entropy weighting k-means algorithm for subspace clustering of high-dimensional sparse data	This paper presents a new k-means type algorithm for clustering high-dimensional objects in subspaces. In high-dimensional data, dusters of objects often exist in subspaces rather than in the entire space. For example, in text clustering, clusters of documents of different topics are categorized by different subsets of terms or keywords. The keywords for one cluster may not occur in the documents of other clusters. This is a data sparsity problem faced in clustering high-dimensional data. In the new algorithm, we extend the k-means clustering process to calculate a weight for each dimension in each cluster and use the weight values to identify the subsets of important dimensions that categorize different clusters. This is achieved by including the weight entropy in the objective function that is minimized in the k-means clustering process. An additional step is added to the k-means clustering process to automatically compute the weights of all dimensions in each cluster. The experiments on both synthetic and real data have shown that the new algorithm can generate better clustering results than other subspace clustering algorithms. The new algorithm is also scalable to large data sets. © 2007 IEEE.
{fenge}
35048883531	Mining frequent items in spatio-temporal databases	It is important to retrieve aggregate information in spatio-temporal applications. Recently, some applications, such as decision support systems, also require to mine frequent items based on a dataset within a query region during a query interval. Because of unbounded space requirement and slow response time, executing query based on operational databases becomes inapplicable. In this paper, we define the problem formally and give out a novel solution to overcome the above two disadvantages. Recently, some algorithms are proposed to mine frequent items from a summarization(sketch) of a mass dataset. In our solution, one of latest sketches is integrated with a spatio-temporal index to provide good performance. © Springer-Verlag 2004.
{fenge}
35048889547	Enhanced email classification based on feature space enriching	Email classification is challenging due to its sparse and noisy feature space. To address this problem, a novel feature space enriching (FSE) technique based on two semantic knowledge bases is proposed in this paper. The basic idea of FSE is to select the related semantic features that will increase the global information for learning algorithms from the semantic knowledge bases, and use them to enrich the original sparse feature space. The resulting feature space of FSE can provide semantic-richer features for classification algorithms to learn improved classifiers. Naive Bayes and support vector machine are selected as the classification algorithms. Experiments on a bilingual enterprise email dataset have shown that: (1) the FSE technique can improve the email classification accuracy, especially for the sparse classes, (2) the SVM classifier benefits more from FSE than the naive Bayes classifier, (3) with the support of domain knowledge, the FSE technique can be more effective. © Springer-Verlag 2004.
{fenge}
35048895304	iSurfer: A focused web crawler based on incremental learning from positive samples	This paper presents a focused Web crawling system iSurfer for information retrieval from the Web. Different from other focused crawlers, iSurfer uses an incremental method to learn a page classification model and a link prediction model. It employs an online sample detector to incrementally distill new samples from crawled Web pages for online updating of the model learned. Other focused crawling systems use classifiers that are built from initial positive and negative samples and can not learn incrementally. The performances of these classifiers depend on the topical coverage of the initial positive and negative samples. However, the initial samples, particularly the negative ones, with a good coverage of target topics are difficult to find. Therefore, the iSurfer's incremental learning strategy has an advantage. It starts from a few positive samples and gains more integrated knowledge about the target topics over time. Our experiments on various topics have demonstrated that the incremental learning method can improve the harvest rate with a few initial samples. © Springer-Verlag Berlin Heidelberg 2004.
{fenge}
35048899695	An efficient multidimensional data model for web usage mining	Web applications such as personalization and recommendation have raised the concerns of people because they are crucial to improve customer services, particularly for E-commerce Websites. Understanding customer preferences and requirements in time is a premise to optimize these Web services. In this paper, a new data model for Web data is introduced to analyze user behavior. The merit of the cube model is that it not only aggregates user access information but also takes the Web structure information into account. Based on the model, we propose some solutions to intelligently discover interesting user access patterns for Website optimization, Web personalization and recommendation. We used the Web usage data from a sports Website in China to evaluate the effectiveness of the model. The results show that this integrated data model is effective and efficient to apply into practical Web applications. © Springer-Verlag Berlin Heidelberg 2004.
{fenge}
35048836698	Mining class outliers: Concepts, algorithms and applications	Detection of outliers is important in many applications and has attracted much attention in the data mining research community recently. How-ever, most existing methods are designed for mining outliers from a single data-set without considering the class labels of data objects. In this paper, we consider the class outlier detection problem, i.e., "given a set of observations with class labels, find those that arouse suspicions, taking into account the class labels." By generalizing two pioneering contributions in this field, we propose the notion of class outliers and practical solutions by extending existing outlier detection algorithms to detect class outliers. Furthermore, its potential applications in CRM (customer relationship management) are discussed. The experiments on real datasets have shown that our method can find interesting outliers and can be used in practice. © Springer-Verlag 2004.
{fenge}
36448978849	Adaptive scheduling for shared window joins over data streams	Recently a few Continuous Query systems have been developed to cope with applications involving continuous data streams. At the same time, numerous algorithms are proposed for better performance. A recent work on this subject was to define scheduling strategies on shared window joins over data streams from multiple query expressions. In these strategies, a tuple with the highest priority is selected to process from multiple candidates. However, the performance of these static strategies is deeply influenced when data are bursting, because the priority is determined only by static information, such as the query windows, arriving order, etc. In this paper, we propose a novel adaptive strategy where the priority of a tuple is integrated with realtime information. A thorough experimental evaluation has demonstrated that this new strategy can outperform the existing strategies. © 2007 Higher Education Press and Springer-Verlag.
{fenge}
36849058016	Learning classifier system ensemble and compact rule set	This paper presents a learning classifier system ensemble for knowledge discovery from incremental data. The new ensemble was designed with a two-level architecture to improve the generalization ability. The new incoming cases are first bootstrapped to generate data as inputs to the first level classical learning classifier systems. The second level contains a plurality-vote module to determine the final classification by combining the classification results of the first level learning classifier systems. Each learning classifier system in the first level consists of two major modules, a genetic algorithm module for facilitating rule-discovery and a reinforcement learning module for adjusting the strength of the corresponding rules when rewards are received from the environment. We propose a revised Wilson's compact rule algorithm for generation of the compact rule set from the population set to improve the readability of the model. Two experiments were conducted. One was data mining of medical data and the other was steganalysis of images. The experimental results have shown that the new ensemble produced better performance on incremental data mining and better generalization than the single learning classifier system and other supervised learning methods. The results also showed that the compact rules were more interpretable.
{fenge}
37149055604	A case-based data mining platform	Data mining practice in industry heavily depends on experienced data mining professionals to provide solutions. Normal business users cannot easily use data mining tools to solve their business problems, because of the complexity of data mining process and data mining tools. In this paper, we propose a case-based data mining platform, which reuses the knowledge captured in past data mining cases to semi-automatically solve new similar problems. We first extend generic data mining model for knowledge reuse. Then we define data mining case. And then we introduce this platform in detail from its storage bases, functional modules, user interface, and application scenario. Theoretically, this platform can simplify data mining process, reduce the dependency on data mining professional, and shorten business decision time. © Springer-Verlag Berlin Heidelberg 2006.
{fenge}
38049095186	A rough set method for supplier performance evaluation in e-business	This paper proposes a new model for supplier performance evaluation in e-business. This model uses balanced scorecard to define the performance evaluation structure in an enterprise. The evaluation is focused on key performance indicators that are calculated from supplier business transactions. We use rough set theory to select key performance attributes at different levels in the evaluation structure and calculate the weights of the selected attributes. We present a performance evaluation process that can be easily implemented in an enterprise. We use a simplified example to show how the new process is used to evaluate suppliers. © 2007 IEEE.
{fenge}
38049153239	LCSE: Learning classifier system ensemble for incremental medical instances	This paper proposes LCSE, a learning classifier system ensemble, which is an extension to the classical learning classifier system(LCS). The classical LCS includes two major modules, a genetic algorithm module used to facilitate rule discovery, and a reinforcement learning module used to adjust the strength of the corresponding rules after the learning module receives the rewards from the environment. In LCSE we build a two-level ensemble architecture to enhance the generalization of LCS. In the first-level, new instances are first bootstrapped and sent to several LCSs for classification. Then, in the second-level, a simple plurality-vote method is used to combine the classification results of individual LCSs into a final decision. Experiments on some benchmark medical data sets from the UCI repository have shown that LCSE has better performance on incremental medical data learning and better generalization ability than the single LCS and other supervised learning methods. © Springer-Verlag Berlin Heidelberg 2007.
{fenge}
0033689141	Early supplier involvement in new product development on the Internet: Implementation perspectives	WeBid is a prototype web-based framework for supporting early supplier involvement in new product development on the Internet. It provides a suite of tools for establishing and managing the customer-supplier relationships in new product development process. Through WeBid, this paper discusses the issues related to the development and implementation of web applications in product design and manufacture. The typical 3-tier architecture is explained to show how the main WeBid components work together to achieve the intended functionality. A number of Java features are discussed for their suitability for implementing and deploying particular application components. Various options for the connection and deployment of the remote databases are compared in terms of their strengths and limitations. The paper provides some useful guidelines for those who are interested in developing and applying web applications in product design and manufacture in a supply chain or extended enterprise environment.
{fenge}
0033896035	Agent-based workflow management in collaborative product development on the Internet	Product development is collaborative, involving multi-disciplinary functions and heterogeneous tools. Teamwork is essential through seamless tool integration and better co-ordination of human activities. This paper proposes to use workflow management as a mechanism to facilitate the teamwork in a collaborative product development environment where remote Web-based Decision Support Systems (TeleDSS) are extensively used by team members who are geographically distributed. The workflow of a project is modelled as a network. Its nodes correspond to the work (packages), and its edges to flows of control and data. The concept of agents is introduced to define nodes and the concept of messages to define edges. As a sandwich layer, agents act as special-purpose application clients for the remote TeleDSS application servers. Agents are delegated to manipulate the corresponding TeleDSS on behalf of their human users. Details of the proceedings are recorded by agents as their properties for future references or shared uses by other team members. Through flow messages, agents are able to share input and output data and request for remote services. One of the major contributions is that agents, once defined, can be reused for different projects without any changes.
{fenge}
58049150898	A changing window approach to exploring gene expression patterns	This paper presents a changing window approach to exploring gene expression patterns in "snapshot windows". A snapshot window is a sub-matrix of co-expressed microarray data representing certain expression pattern. In this approach, we use a feature weighting k-means subspace clustering algorithm to generate a set of clusters and each cluster defines a set of "snapshot windows" which are characterized by different sets of ordered sample weights that were assigned by the clustering algorithm. We define an accumulated weighting threshold (AWT) as the sum of weights of samples in the "snapshot window". Given a cluster, different "snapshot windows" can be obtained by changing AWT to explore all possible local expression patterns in the cluster. Experiment results have shown our approach is effective and flexible in exploring various expression patterns and identifying novel ones. © 2008 IEEE.
{fenge}
84861442725	Hybrid random forests: Advantages of mixed trees in classifying text data	Random forests are a popular classification method based on an ensemble of a single type of decision tree. In the literature, there are many different types of decision tree algorithms, including C4.5, CART and CHAID. Each type of decision tree algorithms may capture different information and structures. In this paper, we propose a novel random forest algorithm, called a hybrid random forest. We ensemble multiple types of decision trees into a random forest, and exploit diversity of the trees to enhance the resulting model. We conducted a series of experiments on six text classification datasets to compare our method with traditional random forest methods and some other text categorization methods. The results show that our method consistently outperforms these compared methods. © 2012 Springer-Verlag.
{fenge}
84866622935	Classifying very high-dimensional data with random forests built from small subspaces	The selection of feature sub space s for growing decision trees is a key step in building random forest models. However, the common approach using randomly sampling a few features in the subspace is not suitable for high dimensional data consisting of thousands of features, because such data often contains many features which are uninformative to classification, and the random sampling often doesn't include informative feature s in the selected subspaces. Consequently, classification performance of the randomforestmodel is significantly affected. In this paper, the authors propose an improved random forest method which uses a novel feature weighting method for subspace selection and therefore enhances classification performance over high-dimensional data. A series of experiments on 9 real life high dimensional datasets demonstrated that using a subspace size of [log
{fenge}
84867071280	A microscopic study on group dynamics of the tencent-microblogs	Studying group dynamics or evolution is an appealing research topic in the area of social networks. In this paper, we have studied a novel problem of predicting social states of the groups in Tencent-microblogs at the level of nodes. A K-history model, which takes three historical factors of the past K time slices into consideration, is proposed to predict each user's future state. To evaluate the proposed model, we have crawled data about groups from Tencent-microblogs. Experimental results on four data sets have shown that the model is more effective in predicting accuracy by taking more historical data into consideration. © Springer Science+Business Media Dortdrecht 2012.
{fenge}
84867482724	Scalable ensemble information-theoretic co-clustering for massive data	Co-clustering is effective for simultaneously clustering rows and columns of a data matrix. Yet different co-clustering models usually produce very distinct results. In this paper, we propose a scalable algorithm to co-cluster massive, sparse and high dimensional data and combine individual clustering results to produce a better final result. Our algorithm is particularly suitable for distributed computing environment, which have been revealed in our experiments, and it is implemented on Hadoop platform with MapReduce programming framework in practice. Experimental results on several real and synthetic data sets demonstrated that the proposed algorithm achieved higher accuracy than other clustering algorithms and scale well.
{fenge}
43349093966	CNP-based implementation of service-oriented workflow mapping in SHGWMS	ShanghaiGrid has a complex service-oriented infrastructure. Workflow management is emerging as one of the most important part of it. Because the grid environment is very dynamic and the services are shared among many users, it is impossible to optimize the workflow from the point of view of execution ahead of time. In fact, one may want to make decisions about the execution locations and the access to a particular data set as late as possible. In this paper, we propose a method of using Contract Net Protocol(CNP) to implement service-oriented workflow mapping in ShanghaiGrid workflow management system(SHGWMS). Three types of workflow in SHGWMS are denoted as Abstract workflow(AW), Concrete workflow(CW) and Executable workflow(EW). Belief-Desire-Intention(BDI) agent technology in SHGWMS helps the system meet challenges from the grid context. CNP provides a very proper negotiation model for agents. The problem of workflow mapping has been transferred to the problem of multi-agent negotiation with the help of CNP model in SHGWMS. We also propose AW2CW mapping algorithm and CW2EW mapping algorithm to accomplish service-oriented workflow mapping. © 2007 Springer Science+Business Media, LLC.
{fenge}
4544371337	Service selection in dynamic demand-driven Web services	Recently, Web services have become a new technology trend for Enterprise Application Integration (EAI) and more and more applications based on Web services are emerging. One of the problems in using Web services in business applications such as logistics is services composition automatically and efficiently. In this paper, we present a Dynamic, Demand-Driven Web services Engine called D3D-Serv to implement composite service functionality that is used to dynamically build composite services from existing services according to different business logics and requirements. In this D3D_Serv framework, the most challenging function to implement is dynamic selection of service providers at run time. The highly dynamic and distributed nature of Web services often makes some service providers overloaded at certain times while others idle. To solve this problem, we propose an efficient services selection and execution strategy that is based on the queuing theory and can provide guarantees for the QOS (Quality of Service) under provider's limited resources. Preliminary experimental results have shown that this algorithm is effective.
{fenge}
4544379047	Mining class outliers: Concepts, algorithms and applications in CRM	Outliers, or commonly referred to as exceptional cases, exist in many real-world databases. Detection of such outliers is important for many applications and has attracted much attention from the data mining research community recently. However, most existing methods are designed for mining outliers from a single dataset without considering the class labels of data objects. In this paper, we consider the class outlier detection problem 'given a set of observations with class labels, find those that arouse suspicions, taking into account the class labels'. By generalizing two pioneer contributions [Proc WAIM02 (2002); Proc SSTD03] in this field, we develop the notion of class outlier and propose practical solutions by extending existing outlier detection algorithms to this case. Furthermore, its potential applications in CRM (customer relationship management) are also discussed. Finally, the experiments in real datasets show that our method can find interesting outliers and is of practical use. © 2004 Elsevier Ltd. All rights reserved.
{fenge}
51849161971	Fuzzy k-means with variable weighting in high dimensional data analysis	This paper presents a comparison study of the fuzzy k-means algorithm and a new variant with variable weighting in clustering high dimensional data. The fuzzy k-means algorithm is effective in discovering the clusters with overlapping boundaries. However, this effectiveness can be handicapped in high dimensional data. The recent development of the k-means algorithm with automated variable weighting offers a new technique for dealing with high dimensional data that occurs in many new applications such as text mining and bioinformatics. In this paper, the variable weighting mechanism is incorporated in the fuzzy k-means algorithm to cluster high dimensional data with overlapping clusters. Experiments on real data sets have shown that the variable weighting fuzzy k-means produced better clustering results than the fuzzy k-means without variable weighting. © 2008 IEEE.
{fenge}
52949101047	Agglomerative fuzzy K-Means clustering algorithm with selection of number of clusters	In this paper, we present an agglomerative fuzzy K-Means clustering algorithm for numerical data, an extension to the standard fuzzy K-Means algorithm by introducing a penalty term to the objective function to make the clustering process not sensitive to the Initial cluster centers. The new algorithm can produce more consistent clustering results from different sets of Initial clusters centers. Combined with cluster validation techniques, the new algorithm can determine the number of clusters In a data set, which is a well-known problem In K-Means clustering. Experimental results on synthetic data sets (2 to 5 dimensions, 500 to 5,000 objects and 3 to 7 clusters), the BIRCH two-dimensional data set of 20,000 objects and 100 cluster0and the WINE data set of 178 objects, 17 dimensions, and 3 clusters from UCI have demonstrated the effectiveness of the new algorithm in producing consistent clustering results and determining the correct number of clusters in different data sets, some with overlapping inherent clusters. © 2008 IEEE.
{fenge}
57749099418	An improved random forest approach for detection of hidden Web search interfaces	Search interface detection is an essential technique for extracting information from the hidden Web. The challenge for this task is search interface data that is represented in high dimensional and sparse features with many missing values. This paper presents a new multi-classifier ensemble approach to solving this problem. In this approach, we have extended the random forest algorithm with a weighted feature selection method to build individual classifiers. With this improved random forest algorithm (IRFA), each classifier can be learnt from a weighted subset of the feature space so that the ensemble of decision trees can fully exploit the useful features of search interface patterns. We have compared our ensemble approach with other well-known classification algorithms, such as SVM and C4.S. The experimental results have shown that our method is more effective in detecting search interfaces of the hidden Web. © 2008 IEEE.
{fenge}
58349085623	Building a decision cluster classification model for high dimensional data by a variable weighting k-means method	In this paper, a new classification method (ADCC) for high dimensional data is proposed. In this method, a decision cluster classification model (DCC) consists of a set of disjoint decision clusters, each labeled with a dominant class that determines the class of new objects falling in the cluster. A cluster tree is first generated from a training data set by recursively calling a variable weighting k-means algorithm. Then, the DCC model is selected from the tree. Anderson-Darling test is used to determine the stopping condition of the tree growing. A series of experiments on both synthetic and real data sets have shown that the new classification method (ADCC) performed better in accuracy and scalability than the existing methods of k-NN, decision tree and SVM. It is particularly suitable for large, high dimensional data with many classes. © 2008 Springer Berlin Heidelberg.
{fenge}
70350741542	Improved blog clustering through automated weighting of text block	In this paper, a new clustering algorithm is proposed for blog data clustering. Considering the structure information of text blocks in blog data, we group the features of blog data into three groups and extend the Ic-means clustering algorithm to automatically calculate a weight for each feature group in the clustering process. We introduce a new objective function with group weight variables and present the Lagrangian method to derive the formula to calculate the group weights. This formula is added as a new step in the standard kmeans iterative clustering process to automatically compute the group weights according to the distribution of features, This new process guarantees the convergency of the clustering process to a local optimal solution. The experimental results have shown that this new algorithm performed better than k-means without group feature weighting on different blog data sets. © 2009 IEEE.
{fenge}
7444220174	Mining of web-page visiting patterns with continuous-time Markov models	This paper presents a new prediction model for predicting when an online customer leaves a current page and which next Web page the customer will visit. The model can forecast the total number of visits of a given Web page by all incoming users at the same time. The prediction technique can be used as a component for many Web based applications. The prediction model regards a Web browsing session as a continuous-time Markov process where the transition probability matrix can be computed from Web log data using the Kolmogorov's backward equations. The model is tested against real Web-log data where the scalability and accuracy of our method are analyzed.
{fenge}
77957556167	Knowledge-based vector space model for text clustering	This paper presents a new knowledge-based vector space model (VSM) for text clustering. In the new model, semantic relationships between terms (e.g., words or concepts) are included in representing text documents as a set of vectors. The idea is to calculate the dissimilarity between two documents more effectively so that text clustering results can be enhanced. In this paper, the semantic relationship between two terms is defined by the similarity of the two terms. Such similarity is used to re-weight term frequency in the VSM. We consider and study two different similarity measures for computing the semantic relationship between two terms based on two different approaches. The first approach is based on the existing ontologies like WordNet and MeSH. We define a new similarity measure that combines the edge-counting technique, the average distance and the position weighting method to compute the similarity of two terms from an ontology hierarchy. The second approach is to make use of text corpora to construct the relationships between terms and then calculate their semantic similarities. Three clustering algorithms, bisecting k-means, feature weighting k-means and a hierarchical clustering algorithm, have been used to cluster real-world text data represented in the new knowledge-based VSM. The experimental results show that the clustering performance based on the new model was much better than that based on the traditional term-based VSM. © 2009 Springer-Verlag London Limited.
{fenge}
78049241090	Exploiting word cluster information for unsupervised feature selection	This paper presents an approach to integrate word clustering information into the process of unsupervised feature selection. In our scheme, the words in the whole feature space are clustered into groups based on the co-occurrence statistics of words. The resulted word clustering information and the bag-of-word information are combined together to measure the goodness of each word, which is our basic metric for selecting discriminative features. By exploiting word cluster information, we extend three well-known unsupervised feature selection methods and propose three new methods. A series of experiments are performed on three benchmark text data sets (the 20 Newsgroups, Reuters-21578 and CLASSIC3). The experimental results have shown that the new unsupervised feature selection methods can select more discriminative features, and in turn improve the clustering performance. © 2010 Springer-Verlag Berlin Heidelberg.
{fenge}
78650188784	CPLDP: An efficient large dataset processing system built on cloud platform	Data intensive applications are widely existed, such as massive data mining, search engine and high-throughput computing in bioinformatics, etc. Data processing becomes a bottleneck as the scale keeps bombing. However, the cost of processing the large scale dataset increases dramatically in traditional relational database, because traditional technology inclines to adopt high performance computer. The boost of cloud computing brings a new solution for data processing due to the characteristics of easy scalability, robustness, large scale storage and high performance. It provides a cost effective platform to implement distributed parallel data processing algorithms. In this paper, we proposed CPLDP (Cloud based Parallel Large Data Processing System), which is an innovative MapReduce based parallel data processing system developed to satisfy the urgent requirements of large data processing. In CPLDP system, we proposed a new method called operation dependency analysis to model data processing workflow and furthermore, reorder and combine some operations when it is possible. Such optimization reduces intermediate file read and write. The performance test proves that the optimization of processing workflow can reduce the time and intermediate results. © 2010 Springer-Verlag.
{fenge}
78650921557	Hierarchical Information-Theoretic Co-Clustering for high dimensional data	Hierarchical clustering is an important technique for hierarchical data exploration applications. However, most existing hierarchial methods are based on traditional one-side clustering, which is not effective for handling high dimensional data. In this paper, we develop a partitional hierarchical co-clustering framework and propose a Hierarchical Information-Theoretical Co-Clustering (HITCC) algorithm. The algorithm conducts a series of binary partitions of objects on a data set via the Information- Theoretical Co-Clustering (ITCC) procedure, and generates a hierarchical management of object clusters. Due to simultaneously clustering of features and objects in the process of building a cluster tree, the HITCC algorithm can identify subspace clusters at different-level abstractions and acquire good clustering hierarchies. Compared with the flat ITCC algorithm and six state-of-the-art hierarchical clustering algorithms on various data sets, the new algorithm demonstrated much better performance. ICIC International © 2011 ISSN.
{fenge}
79952029613	Fuzzy soft subspace clustering method for gene co-expression network analysis	Clustering techniques for building gene co-expression networks sutter greatly from biological complexities. This paper proposes a fuzzy soft subspace clustering method for detecting overlapped clusters of locally co-expressed genes that may participate in multiple cellular processes and take on different biological functions. Process-specific feature subspaces of clusters and interrelations among different clusters can be extracted by this method, providing useful clues for gene coexpression network analysis. Experiments on yeast cell cycle data have shown that this method is effective in extracting biological relationships between functional gene clusters, and enhancing gene co-expression network analysis. ©2010 IEEE.
{fenge}
79951729670	Minimum spanning tree based classification model for massive data with MapReduce implementation	Rapid growth of data has provided us with more information, yet challenges the tradition techniques to extract the useful knowledge. In this paper, we propose MCMM, a Minimum spanning tree (MST) based Classification model for Massive data with MapReduce implementation. It can be viewed as an intermediate model between the traditional K nearest neighbor method and cluster based classification method, aiming to overcome their disadvantages and cope with large amount of data. Our model is implemented on Hadoop platform, using its MapReduce programming framework, which is particular suitable for cloud computing. We have done experiments on several data sets including real world data from UCI repository and synthetic data, using Downing 4000 clusters, installed with Hadoop. The results show that our model outperforms KNN and some other classification methods on a general basis with respect to accuracy and scalability. © 2010 IEEE.
{fenge}
79956316231	Mining trajectory corridors using Fréchet distance and meshing grids	With technology advancement and increasing popularity of location-aware devices, trajectory data are ubiquitous in the real world. Trajectory corridor, as one of the moving patterns, is composed of concatenated sub-trajectory clusters which help analyze the behaviors of moving objects. In this paper we adopt a three-phase approach to discover trajectory corridors using Fréchet distance as a dissimilarity measurement. First, trajectories are segmented into sub-trajectories using meshing-grids. In the second phase, a hierarchical method is utilized to cluster intra-grid sub-trajectories for each grid cell. Finally, local clusters in each single grid cell are concatenated to construct trajectory corridors. By utilizing a grid structure, the segmentation and concatenation need only single traversing of trajectories or grid cells. Experiments demonstrate that the unsupervised algorithm correctly discovers trajectory corridors from the real trajectory data. The trajectory corridors using Fréchet distance with temporal information are different from those having only spatial information. By choosing an appropriate grid size, the computing time could be reduced significantly because the number of sub-trajectories in a single grid cell is a dominant factor influencing the speed of the algorithms. © 2010 Springer-Verlag Berlin Heidelberg.
{fenge}
79957969911	High-order co-clustering text data on semantics-based representation model	The language modeling approach is widely used to improve the performance of text mining in recent years because of its solid theoretical foundation and empirical effectiveness. In essence, this approach centers on the issue of estimating an accurate model by choosing appropriate language models as well as smooth techniques. Semantic smoothing, which incorporates semantic and contextual information into the language models, is effective and potentially significant to improve the performance of text mining. In this paper, we proposed a high-order structure to represent text data by incorporating background knowledge, Wikipedia. The proposed structure consists of three types of objects, term, document and concept. Moreover, we firstly combined the high-order co-clustering algorithm with the proposed model to simultaneously cluster documents, terms and concepts. Experimental results on benchmark data sets (20Newsgroups and Reuters-21578) have shown that our proposed high-order co-clustering on high-order structure outperforms the general co-clustering algorithm on bipartite text data, such as document-term, document-concept and document-(term+concept). © 2011 Springer-Verlag.
{fenge}
80051673006	Integrating constraints to support legally flexible business processes	Flexible collaboration is a notable attribute of Web 2.0, which is often in the form of multiple users participating different activities that together complete a whole business process. In such an environment, business processes may be dynamically customized or adjusted, as well as the participants may be selected or attend uncertainly. So how to ensure the legitimacy of a business process for both security and business is increasingly critical. In this paper, we investigate this problem and introduce a novel method to support legally flexible business processes. The proposed Constraint-based Business Process Management Model incorporates constraints into the standard activities composing a business process, where the security constraints place restrictions on participants performing the activities and business constraints restrict the dependencies between multiple activities. By the assembly operations, business processes can be dynamically generated and adjusted with activities, that are obliged to the specified constraints. Several algorithms are presented to verify the consistency of constraints and the soundness of the generated business processes, as well as to perform the execution planning to guarantee the correct execution of a business process on the precondition of satisfying all constraints. We present an illustrative example and implement a prototype for the proposed model that is an application of property rights exchange for supporting legal business processes. © Springer Science + Business Media, LLC 2009.
{fenge}
80052747011	A feature group weighting method for subspace clustering of high-dimensional data	This paper proposes a new method to weight subspaces in feature groups and individual features for clustering high-dimensional data. In this method, the features of high-dimensional data are divided into feature groups, based on their natural characteristics. Two types of weights are introduced to the clustering process to simultaneously identify the importance of feature groups and individual features in each cluster. A new optimization model is given to define the optimization process and a new clustering algorithm FG-k-means is proposed to optimize the optimization model. The new algorithm is an extension to k-means by adding two additional steps to automatically calculate the two types of subspace weights. A new data generation method is presented to generate high-dimensional data with clusters in subspaces of both feature groups and individual features. Experimental results on synthetic and real-life data have shown that the FG-k-means algorithm significantly outperformed four k-means type algorithms, i.e., k-means, W-k-means, LAC and EWKM in almost all experiments. The new algorithm is robust to noise and missing values which commonly exist in high-dimensional data. © 2011 Elsevier Ltd. All rights reserved.
{fenge}
84155181035	Topic oriented community detection through social objects and link analysis in social networks	Community detection is an important issue in social network analysis. Most existing methods detect communities through analyzing the linkage of the network. The drawback is that each community identified by those methods can only reflect the strength of connections, but it cannot reflect the semantics such as the interesting topics shared by people. To address this problem, we propose a topic oriented community detection approach which combines both social objects clustering and link analysis. We first use a subspace clustering algorithm to group all the social objects into topics. Then we divide the members that are involved in those social objects into topical clusters, each corresponding to a distinct topic. In order to differentiate the strength of connections, we perform a link analysis on each topical cluster to detect the topical communities. Experiments on real data sets have shown that our approach was able to identify more meaningful communities. The quantitative evaluation indicated that our approach can achieve a better performance when the topics are at least as important as the links to the analysis. © 2011 Elsevier B.V. All rights reserved.
{fenge}
84859722802	Scalable subspace logistic regression models for high dimensional data	Although massive, high dimensional data in the real world provide more information for logistic regression classification, yet it also means a huge challenge for us to build models accurately and efficiently. In this paper, we propose a scalable subspace logistic regression algorithm. It can be viewed as an advanced classification algorithm based on a random subspace sampling method and the traditional logistic regression algorithm, aiming to effectively deal with massive, high dimensional data. Our algorithm is particularly suitable for distributed computing environment, which we have proved, and it is implemented on Hadoop platform with MapReduce programming framework in practice. We have done several experiments using real and synthetic datasets and demonstrated better performance of our algorithm in comparison with other logistic regression algorithms. © 2012 Springer-Verlag Berlin Heidelberg.
{fenge}
84861420246	Scalable random forests for massive data	This paper proposes a scalable random forest algorithm SRF with MapReduce implementation. A breadth-first approach is used to grow decision trees for a random forest model. At each level of the trees, a pair of map and reduce functions split the nodes. A mapper is dispatched to a local machine to compute the local histograms of subspace features of the nodes from a data block. The local histograms are submitted to reducers to compute the global histograms from which the best split conditions of the nodes are calculated and sent to the controller on the master machine to update the random forest model. A random forest model is built with a sequence of map and reduce functions. Experiments on large synthetic data have shown that SRF is scalable to the number of trees and the number of examples. The SRF algorithm is able to build a random forest of 100 trees in a little more than 1 hour from 110 Gigabyte data with 1000 features and 10 million records. © 2012 Springer-Verlag.
{fenge}
84862198779	Multi-layer network for influence propagation over microblog	Microblog has become ubiquitous for social networking and information sharing. A few studies on information propagation over microblog reveal that the majority of users like to publish and share the news on microblog. The public opinion over the internet sometimes plays important role in national or international security. In this paper, we propose a new social network data model named Multi-Layer Network (MLN) over microblog. In the model, different layers represent different kinds of relationships between individuals. We present a new influence propagation model based on the MLN model. Finally, we conduct experiments on real-life microblog data of four recent hot topics. The experimental results show that our MLN model and influence propagation model are more effective in finding new and accurate active individuals comparing with the single layer data model and the linear threshold model. © 2012 Springer-Verlag.
{fenge}
84863183658	A new Markov model for clustering categorical sequences	Clustering categorical sequences remains an open and challenging task due to the lack of an inherently meaningful measure of pairwise similarity between sequences. Model initialization is an unsolved problem in model-based clustering algorithms for categorical sequences. In this paper, we propose a simple and effective Markov model to approximate the conditional probability distribution (CPD) model, and use it to design a novel two-tier Markov model to represent a sequence cluster. Furthermore, we design a novel divisive hierarchical algorithm for clustering categorical sequences based on the two-tier Markov model. The experimental results on the data sets from three different domains demonstrate the promising performance of our models and clustering algorithm. © 2011 IEEE.
{fenge}
84863337579	SMART: A subspace clustering algorithm that automatically identifies the appropriate number of clusters	This paper presents a subspace κ-means clustering algorithm for high-dimensional data with automatic selection of κ. A new penalty term is introduced to the objective function of the fuzzy κ-means clustering process to enable several clusters to compete for objects, which leads to merging some cluster centres and the identification of the 'true' number of clusters. The algorithm determines the number of clusters in a dataset by adjusting the penalty term factor. A subspace cluster validation index is proposed and employed to verify the subspace clustering results generated by the algorithm. The experimental results from both the synthetic and real data have demonstrated that the algorithm is effective in producing consistent clustering results and the correct number of clusters. Some real datasets are used to demonstrate how the proposed algorithm can determine interesting sub-clusters in the datasets. Copyright © 2009 Inderscience Enterprises Ltd.
{fenge}
84863163326	Rating: Privacy preservation for multiple attributes with different sensitivity requirements	Motivated by the insufficiency of the existing framework that could not process multiple attributes with different sensitivity requirements on modeling real world privacy requirements for data publishing, we present a novel method, rating, for publishing sensitive data. Rating releases AT (Attribute Table) and IDT (ID Table) based on different sensitivity coefficients for different attributes. This approach not only protects privacy for multiple sensitive attributes, but also keeps a large amount of correlations of the microdata. We develop algorithms for computing AT and IDT that obey the privacy requirements for multiple sensitive attributes, and maximize the utility of published data as well. We prove both theoretically and experimentally that our method has better performance than the conventional privacy preserving methods on protecting privacy and maximizing the utility of published data. To quantify the utility of published data, we propose a new measurement named classification measurement. © 2011 IEEE.
{fenge}
84864152396	Privacy preserving distributed DBSCAN clustering	DBSCAN is a well-known density-based clustering algorithm which offers advantages for finding clusters of arbitrary shapes compared to partitioning and hierarchical clustering methods. However, there are few papers studying the DBSCAN algorithm under the privacy preserving distributed data mining model, in which the data is distributed between two or more parties, and the parties cooperate to obtain the clustering results without revealing the data at the individual parties. In this paper, we address the problem of two-party privacy preserving DBSCAN clustering. We first propose two protocols for privacy preserving DBSCAN clustering over horizontally and vertically partitioned data respectively and then extend them to arbitrarily partitioned data. We also provide analysis of the performance and proof of privacy of our solution. Copyright 2012 ACM.
{fenge}
84865744431	Using a variable weighting κ-means method to build a decision cluster classification model	In this paper, a new classification method (ADCC) for high-dimensional data is proposed. In this method, a decision cluster classification (DCC) model consists of a set of disjoint decision clusters, each labeled with a dominant class that determines the class of new objects falling in the cluster. A cluster tree is first generated from a training data set by recursively calling a variable weighting κ-means algorithm. Then, the DCC model is extracted from the tree. Various tests including AndersonDarling test are used to determine the stopping condition of the tree growing. A series of experiments on both synthetic and real data sets have been conducted. Their results show that the new classification method (ADCC) performed better in accuracy and scalability than existing methods like κ-NN, decision tree and SVM. ADCC is particularly suitable for large, high-dimensional data with many classes. © 2012 World Scientific Publishing Company.
{fenge}
0037261108	A data cube model for prediction-based web prefetching	Reducing the web latency is one of the primary concerns of Internet research. Web caching and web prefetching are two effective techniques to latency reduction. A primary method for intelligent prefetching is to rank potential web documents based on prediction models that are trained on the past web server and proxy server log data, and to prefetch the highly ranked objects. For this method to work well, the prediction model must be updated constantly, and different queries must be answered efficiently. In this paper we present a data-cube model to represent Web access sessions for data mining for supporting the prediction model construction. The cube model organizes session data into three dimensions. With the data cube in place, we apply efficient data mining algorithms for clustering and correlation analysis. As a result of the analysis, the web page clusters can then be used to guide the prefetching system. In this paper, we propose an integrated web-caching and web-prefetching model, where the issues of prefetching aggressiveness, replacement policy and increased network traffic are addressed together in an integrated framework. The core of our integrated solution is a prediction model based on statistical correlation between web objects. This model can be frequently updated by querying the data cube of web server logs. This integrated data cube and prediction based prefetching framework represents a first such effort in our knowledge.
{fenge}
84871457192	Post-processing strategies for improving local gene expression pattern analysis	This paper proposes a new analytical process highlighted by a soft subspace clustering method, a changing window technique, and a series of post-processing strategies to enhance the identification and characterisation of local gene expression patterns. The proposed method can be conducted in an interactive way, facilitating the exploration and analysis of local gene expression patterns in real applications. Experimental results have shown that the proposed method is effective in identification and characterization of functional gene groups in terms of both local expression similarities and biological coherence of genes in a cluster. Copyright © 2013 Inderscience Enterprises Ltd.
{fenge}
84874615171	TW-(k)-means: Automated two-level variable weighting clustering algorithm for multiview data	This paper proposes TW-(k)-means, an automated two-level variable weighting clustering algorithm for multiview data, which can simultaneously compute weights for views and individual variables. In this algorithm, a view weight is assigned to each view to identify the compactness of the view and a variable weight is also assigned to each variable in the view to identify the importance of the variable. Both view weights and variable weights are used in the distance function to determine the clusters of objects. In the new algorithm, two additional steps are added to the iterative (k)-means clustering process to automatically compute the view weights and the variable weights. We used two real-life data sets to investigate the properties of two types of weights in TW-(k)-means and investigated the difference between the weights of TW-(k)-means and the weights of the individual variable weighting method. The experiments have revealed the convergence property of the view weights in TW-(k)-means. We compared TW-(k)-means with five clustering algorithms on three real-life data sets and the results have shown that the TW-(k)-means algorithm significantly outperformed the other five clustering algorithms in four evaluation indices. © 2012 IEEE.
{fenge}
84875232724	An ensemble of decision cluster crotches for classification of high dimensional data	This paper presents a Crotch Ensemble classification model for high dimensional data. A Crotch Ensemble is obtained from a decision cluster tree built by calling a clustering algorithm recursively. A crotch is an inner node of the tree together with its direct children. If the children of a crotch have more than one dominant class, the crotch is defined as a crotch predictor. Each crotch predictor constructs a classifier by itself. A Crotch Ensemble consists of a set of crotch predictors. When classifying a new object, a subset of crotch predictors is selected according to the distances between the object and the crotch predictors. A classification is made on the object as the class predicted by the crotch predictors with the maximum accumulative weights. The experimental results on both synthetic and real data have shown that the Crotch Ensemble model can get better classification results on high dimensional data than other classification methods. © 2013 Elsevier B.V. All rights reserved.
{fenge}
84877076489	Privacy preserving distributed DBSCAN clustering	DBSCAN is a well-known density-based clustering algorithm which offers advantages for finding clusters of arbitrary shapes compared to partitioning and hierarchical clustering methods. However, there are few papers studying the DBSCAN algorithm under the privacy preserving distributed data mining model, in which the data is distributed between two or more parties, and the parties cooperate to obtain the clustering results without revealing the data at the individual parties. In this paper, we address the problem of two-party privacy preserving DBSCAN clustering. We first propose two protocols for privacy preserving DBSCAN clustering over horizontally and vertically partitioned data respectively and then extend them to arbitrarily partitioned data. We also provide performance analysis and privacy proof of our solution.
{fenge}
0142245850	Uni-grid P&T: A toolkit for building customizable grid portals	This paper presents an architecture and functional design of Uni-Grid P&T, a toolkit for building customizable Grid portals for different application domains in a typical university environment in China. Based on a layered architecture, the functions of the toolkit are designed on top of the new Open Grid Services Architecture (OGSA) and based on the standards of Web services. We particularly address the issues of collaborative portals and portlet implementations of the toolkit. The development of the toolkit is part of the initiative of the ChinaGrid project launched in 2002 by the Ministry of Education of China that is aimed to build a China national education Grid to link more than 100 major universities across China. © Springer-Verlag Berlin Heidelberg 2003.
{fenge}
84893595207	A concept-drifting detection algorithm for categorical evolving data	In data streams analysis, detecting concept-drifting is a very important problem for real-time decision making. In this paper, we propose a new method for detecting concept drifts by measuring the difference of distributions between two concepts. The difference is defined by approximation accuracy of rough set theory, which can also be used to measure the change speed of concepts. We propose a concept-drifting detection algorithm and analyze its complexity. The experimental results on a real data set with a half million records have shown that the proposed algorithm is not only effective in discovering the changes of concepts but also efficient in processing large data sets. © Springer-Verlag 2013.
{fenge}
84893625242	A hybrid optimization method for acceleration of building linear classification models	Linear classification is an important technique in machine learning and data mining, and development of fast optimization methods for training linear classification models is a hot research topic. Stochastic gradient descent (SGD) can achieve relatively good results quickly, but unstable to converge. Limited-memory BFGS (L-BFGS) method converges, but takes a long time to train the model, as it needs to compute the gradient from the entire data set to make an update. In this paper, we investigate a hybrid method that integrates SGD and L-BFGS into a new optimization process SGD-LBFGS to take advantages of both optimization methods. In SGD-LBFGS, SGD is used to run initial iterations to obtain a suboptimal result, and then L-BFGS takes over to continue the optimization process until the process converges and a better model is built. We present a theoretical result to prove that SGD-LBFGS converges faster than SGD and L-BFGS. Experiment analysis on 6 real world data sets have shown that SGD-LBFGS converged 77% faster than L-BFGS on average and demonstrated more stable results than SGD. © 2013 IEEE.
{fenge}
84894200395	A cross cluster-based collaborative filtering method for recommendation	As the clustering-based model has better scalability than typical collaborative filtering methods, it has become one of the most successful approaches for recommender systems. However, since clustering-based algorithms often result in nearby users being divided into different clusters, they only recommend items being rated by users belonging to the same cluster with the active user, and recommendation opportunities are missed for some users because of the loss of nearby users. In this paper, we propose a cross cluster-based method to take more recommendation opportunities by considering nearby users through merging of neighbors in user clusters. We define an associate degree to find the neighboring clusters. Experimental results on real data sets have shown that the proposed method can improve the accuracy of recommendation. © 2013 IEEE.
{fenge}
84901198425	Trend analysis of categorical data streams with a concept change method	This paper proposes a new method to trend analysis of categorical data streams. A data stream is partitioned into a sequence of time windows and the records in each window are assumed to carry a number of concepts represented as clusters. A data labeling algorithm is proposed to identify the concepts or clusters of a window from the concepts of the preceding window. The expression of a concept is presented and the distance between two concepts in two consecutive windows is defined to analyze the change of concepts in consecutive windows. Finally, a trend analysis algorithm is proposed to compute the trend of concept change in a data stream over the sequence of consecutive time windows. The methods for measuring the significance of an attribute that causes the concept change and the outlier degrees of objects are presented to reveal the causes of concept change. Experiments on real data sets are presented to demonstrate the benefits of the trend analysis method. © 2013 Elsevier Inc. All rights reserved.
{fenge}
84901260865	Extensions to quantile regression forests for very high-dimensional data	This paper describes new extensions to the state-of-the-art regression random forests Quantile Regression Forests (QRF) for applications to high-dimensional data with thousands of features. We propose a new subspace sampling method that randomly samples a subset of features from two separate feature sets, one containing important features and the other one containing less important features. The two feature sets partition the input data based on the importance measures of features. The partition is generated by using feature permutation to produce raw importance feature scores first and then applying p-value assessment to separate important features from the less important ones. The new subspace sampling method enables to generate trees from bagged sample data with smaller regression errors. For point regression, we choose the prediction value of Y from the range between two quantiles Q
{fenge}
84901676539	A LDA feature grouping method for subspace clustering of text data	This paper proposes a feature grouping method for clustering of text data. In this new method, the vector space model is used to represent a set of documents. The LDA algorithm is applied to the text data to generate groups of features as topics. The topics are treated as group features which enable the recently published subspace clustering algorithm FG-k-means to be used to cluster high dimensional text data with two level features, the word level and the group level. In generating the group level features with LDA, an entropy based word filtering method is proposed to remove the words with low probabilities in the word distribution of the corresponding topics. Experiments were conducted on three real-life text data sets to compare the new method with three existing clustering algorithms. The experiment results have shown that the new method improved the clustering performance in comparison with other methods. © 2014 Springer International Publishing.
{fenge}
84915754745	QRM: A probabilistic model for search engine query recommendation	This paper proposes a query ranking model (QRM) for query recommendation to the Web users of a search engine. Given an initial query in a search session, a set of queries for the user to select as the next query are ranked based on the joint probability that the query is to be selected by the user and that the result of the query is to be clicked by the user, and that the clicked result will satisfy the user’s information requirement. We define three utilities to solve the model, including a query level utility and two document level utilities that are the perceived utility representing user’s action on the query result and the posterior utility representing user’s satisfaction on the search result. We present the methods to compute the three utilities from the query log data. Experiment results on real query log data have demonstrated that the proposed query ranking model outperformed six baseline methods in generating recommendation queries.
{fenge}
84915798630	Ensemble clustering of high dimensional data with fastmap projection	In this paper, we propose an ensemble clustering method for high dimensional data which uses FastMap projection to generate subspace component data sets. In comparison with popular random sampling and random projection, FastMap projection preserves the clustering structure of the original data in the component data sets so that the performance of ensemble clustering is improved significantly. We present two methods to measure preservation of clustering structure of generated component data sets. The comparison results have shown that FastMap preserved the clustering structure better than random sampling and random projection. Experiments on three real data sets were conducted with three data generation methods and three consensus functions. The results have shown that the ensemble clustering with FastMap projection outperformed the ensemble clusterings with random sampling and random projection.
{fenge}
10444288974	Adaptive grid job scheduling with genetic algorithms	This paper proposes two models for predicting the completion time of jobs in a service Grid. The single service model predicts the completion time of a job in a Grid that provides only one type of service. The multiple services model predicts the completion time of a job that runs in a Grid which offers multiple types of services. We have developed two algorithms that use the predictive models to schedule jobs at both system level and application level. In application-level scheduling, genetic algorithms are used to minimize the average completion time of jobs through optimal job allocation on each node. The experimental results have shown that the scheduling system using the adaptive scheduling algorithms can allocate service jobs efficiently and effectively. © 2004 Elsevier B.V. All rights reserved.
