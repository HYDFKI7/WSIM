{fenge}
8844225732	Kernel density estimation and its application to clustering algorithm construction	Kernel density estimation provides solid foundation for density based clustering algorithm 1 construction. While binned approximation is shown to be an efficient mechanism for fast kernel density computation, it is also proven to be a promising approach to construct robust clustering algorithms. This paper deals with formation and accuracy of the binned kernel density estimators, presents mean squared error bounds for the closeness of such estimators to the unbinned kernel density estimators. To improve the 1 accuracy of the binning method, a naïve grid-level approximated density estimator is constructed, followed by a detailed proof of its mean squared error bounds. The improved approach constructs binned density estimator by substituting the center of a grid with the gravity center of the data points, which results in better estimation accuracy without loss of computation efficiency. As a main concern, the close relation between the density-based clustering algorithms and the kernel estimation methods is revealed.
{fenge}
33845908041	DENCLUE-M: Boosting DENCLUE algorithm by mean approximation on grids	Many data mining applications require clustering of large amount of data. Most clustering algorithms, however, do not work and efficiently when facing such kind of dataset. This paper presents an approach to boost one of the most prominent density-based algorithms, called DENCLUE. We show analytically that the method of adjusted mean approximation on the grid is not only a powerful tool to relieve the burden of heavy computation and memory usage, but also a close proximity of the original algorithm. An adjusted mean approximation based clustering algorithm called DENCLUE-M is constructed which exploits more advantages from the grid partition mechanism. Results of experiments also demonstrate promising performance of this approach. © Springer-Verlag Berlin Heidelberg 2003.
{fenge}
77954289999	Identifying key people from a single document using people event map	Event-based summarization and biography are involved with processing people information, but they still have some deficiencies in identifying key people. We present a method of identifying key people from a single document in this paper. After analyzing and quantifying three kinds of associative strengths between people and events, between people and between events, a people event map is constructed to represent a document, and the people's importance can be computed using the PageRank algorithm based on people event map. We apply two methods to identify key people from 834 documents. The average accuracy achieves 92.7% using the method based on people event map to identify key people, and the average accuracy is 80.2% by the method of counting the people's frequencies. Copyright © 2010 Binary Information Press.
{fenge}
79951601271	Feature based sequence image stitching method	Image mosaic is useful for the traditional image process. We proposed a novel feature based sequence image stitching method for the image mosaic system. In this method, the whole image registration process has five steps, which includes feature point detection, feature descriptor extraction, feature points matching, estimation of the motion model parameters and stitching process. In these steps, difference of Gaussian image is used to get the extreme points as the feature points, then the SIFT descriptor operator is used to describe the feature, finally random sample consensus method is used for estimating the motion parameters. We test our method on two kinds of sequence images, the artificial images and the real images. The results show the stitching method is robust. ©2010 IEEE.
{fenge}
80052356147	A novel method for fluence map optimization with dose volume constraints in IMRT	Doctors judge whether the inverse plan can meet the clinical requirements according to their experiences on the dose volume curve, which was used to characterize the dose deposition of each organ in radiation therapy. However, when the dose volume constraints were added to the inverse planning model, the resulting non-convex programming problem needs a complex solving process, which may not lead to a good solution. In order to solving this problem efficiently, a new heuristic algorithm was introduced. In this method, rather than using the traditional dose sorting information, a new geometry distance, getting from the transformed standard quadratic form, was used to guide the constrained voxels' selection. Combining with the effective voxels adding scheme and the quick local optimum searching by quadratic programming, the whole solving process is very fast. Through testing on the head-neck case and prostate case, the results show the new heuristic information is more suitable for adding constraints than the dose dependent way. Therefore, it is a stable solving algorithm for the fluence map optimization process with dose volume constraints. © 2011 Springer-Verlag.
{fenge}
84861492657	Event ontology reasoning based on event class influence factors	An event ontology is a shared, objective and formal specification of an event class system model. In comparison with the conventional ontology, the event ontology represents knowledge with a higher granularity. For event classes, non-classified relations are more important such as cause-effect relation and follow relation, and there are certain associative strengths between them, called event class influence factor. In this paper, we propose a method of event ontology reasoning based on event class influence factors. After introducing event, event class and event ontology model, we illustrate some related theorems and inferences. The experimental results show that the method proposed has a certain significance for analyzing event scenarios and pre-warning disasters. © 2011 Springer-Verlag.
{fenge}
84861933186	A method of query expansion based on event ontology	The human have suffered and have been suffering all kinds of emergency, and getting event information has become the key component for the users. Event ontology is a shared, objective and formal specification of an event class system model, and it represents knowledge with a higher granularity. The existing methods of query expansion based on ontology do not distinguish the functions of different event elements, and apply the same strategy to expand query terms. Aiming at a lot of event-oriented query requirements, we propose a method of query expansion based on event ontology (denoted by EO-QE). The paper emphatically discusses the concept of event four-tuple and the different query expansion strategy based on different event elements. The results show that EO-QE offers more effective performance for retrieving events, compared with pseudo relevance feedbackbased mechanism (denoted by PRF-QE) and local context analysis mechanism (denoted by LCA-QE).
{fenge}
84876814953	A RFID lightweight security communication protocol based on hybrid encrypting approach	Based on the analysis of the features and issues pertinent to the current radio frequency identification (RFID) communication protocols, a lightweight security communication protocol (LSCP) was proposed combining public key encryption and symmetric cryptosystem. The formal proof of correctness of new protocol was given based on logic. The proposed protocol can be used to prevent many security problems including information leakage, replay attack, location tracing, spoofing attack and forward security etc. It has good security and privacy. Moreover, the performance and cost of the protocol had also been analyzed, results show that the protocol is suitable for practical use in RFID system.
{fenge}
70350721881	Texture retrieval by scale and rotation invariant directional empirical mode decomposition	In this paper, a method for texture retrieval based on Scale and Rotation Invariant Directional Empirical Mode Decomposition (SRIDEMD) is presented. Different from other filtering based techniques such as wavelet and Gabor decomposition, EMD uses the nonlinear filtering process called 'sifting' which attains its scale adaptivity and obtained Intrinsic Mode Functions (IMFs) which has approximate orthogonality. We extend DEMD which is a fast technique of extending 1-D EMD to 2-D case by introducing scale and rotation invariance. Features including frequency and envelopes of IMFs are extracted after 2-D Hilbert transform. Decomposition in several directions is made for rotation invariance and main direction is used. Scaleinvariant features are attained by further processing the results and using fractal dimensions of the residues and IMFs. We validate the effectiveness of this method by experiments for textures from public texture database. © 2009 IEEE.
{fenge}
70449680375	Density function based medical image clustering analysis and research	Image clustering analysis is one of the core techniques for image indexing, classification, identification and segmentation for image processing. On the basis of investigation on the laws of gray distribution of medical image, density function construction based medical image clustering analysis method is designed. For the special kind of data object - medical images, firstly its density function is constructed; secondly, a method of density function construction based on medical image clustering analysis and its implementation algorithm, that is, hill climbing, is offered; At last, abdomen medical images are used to do experiments according to those two algorithms. Experiment results show that medical image clustering on the ground of density function construction achieves good effects and can clearly express the content and semanteme of medical images. © 2006 Springer.
{fenge}
71549131937	Design and implementation of virtual fault diagnosis system for photoelectric tracking devices based on OpenGL	In view of the crucial deficiency of the traditional diagnosis approaches for photoelectric tracking devices and the output of more sufficient diagnosis information, in this paper, an virtual fault diagnosis system based on open graphic library(OpenGL) is proposed. Firstly, some interrelated key principles and technology of virtual reality, visualization and intelligent fault diagnosis technology are put forward. Then, the demand analysis and architecture of the system are elaborated. Next, details of interrelated essential implementation issues are also discussed, including the the 3D modeling of the related diagnosis equipments, key development process and design via OpenGL. Practical applications and experiments illuminate that the proposed approach is feasible and effective. © 2009 Copyright SPIE - The International Society for Optical Engineering.
{fenge}
76549113056	Learning to recommend product with the content of web page	Recommender systems improve access to relevant products and information by making suggestions based on page ranking technology. Existing approaches to learning to rank, however, did not consider the pages in the deep web which have valuable information. In this paper, we present a novel product recommendation algorithm based on the content of web pages including the product information and customer reviews. Our algorithm uses the customer reviews to calculate the score of dynamic web pages. The paper further focus on classifying the semantic orientation of the customer reviews through a progressed Bayesian Classifier and calculating the support value of each review. In addition, we also analyze the change tendency of customer reviews based on the temporal dimension. Experimental results shows that this approach can produce accurate recommendations. © 2009 IEEE.
{fenge}
77749286324	A novel image segmentation method based on random walk	Image segmentation based on random walk model in graph theory can be transformed into large-scale sparse linear equations to solve problem. The final solution of the equation and the iteration convergence rate is depending on the selection of the initial value. It is a significant disadvantage for segmenting the large scale image while selecting initial value randomly. In this paper, we proposed a novel image segmentation method based on random walk model. First of all, we down-sampling the original large image to the small image which can be solved fast, then the small image segmentation leads to sparse linear equations of much smaller scale. After getting the solution, the probability results will be up-sampling to the up layer, and then solve the sparse linear equations in this layer; repeating this up-sampling process until to the top layer which is the original image. At last, segment the final probability image with a pre-set threshold. We test our algorithm on two natural images and compare the segmentation results with that from the original random walk algorithm. The segmentation results show that ours are much better. Our algorithm takes the low-scale image probability result as the init ial value of the high-scale image segmentation process. Therefore, under the same calculation time, segmentation result by our algorithm is much better than that by the original random walk segmentation algorithm. ©2009 IEEE.
{fenge}
77949534671	Quantum interference crossover-based Clonal Selection Algorithm and its application to traveling salesman problem	Clonal Selection Algorithm (CSA), based on the clonal selection theory proposed by Burnet, has gained much attention and wide applications during the last decade. However, the proliferation process in the case of immune cells is asexual. That is, there is no information exchange during different immune cells. As a result the traditional CSA is often not satisfactory and is easy to be trapped in local optima so as to be premature convergence. To solve such a problem, inspired by the quantum interference mechanics, an improved quantum crossover operator is introduced and embedded in the traditional CSA. Simulation results based on the traveling salesman problems (TSP) have demonstrated the effectiveness of the quantum crossover-based Clonal Selection Algorithm. Copyright © 2009 The Institute of Electronics.
{fenge}
77958593323	Collision resistance may be unnecessary: Signing messages with randomized hashing	The hash-then-sign signature scheme's security relies on the collision resistance of the underlying hash function. To free this reliance, people introduced random value into the construction of the hash function, called randomized hashing. When the message provider and the signer are not the same person, the known randomized hashing-then-sign signature can protect the signer from the malicious message provider, even the hash function used is not collision resistant. Unfortunately, this scheme is vulnerable to the attacks by the signer himself. In this paper, we try to solve this problem. To give a fair treat to both the signer and the message provider, we suggest to enhance the known diagram of producing signature with randomized hashing. When using the enhanced randomized hashing-then-sign signature scheme, all the forgers (include the signer himself) need to find the second preimage of the target message. In addition, we analysis the security of one randomized hashing construction and compare it with RMX construction. The result is that the security of this randomized hashing construction is as well as that of RMX construction and even better in some special cases. © 2010 IEEE.
{fenge}
78149332763	Adaptive smoothing method of fluence map in IMRT	This paper focuses on the fluence map optimization problem in Intensity Modulated Radiation Therapy. This technology has been considered as a major breakthrough in cancer therapy. The dose distribution produced by IMRT is more desirable than other radiation therapy techniques that can guarantee killing cancer cells while protecting normal tissue from complications as much as possible. The advantages and disadvantages of the total variance smoothing method and the quadratic smoothing method have been analyst in detail in this paper. By using the local gradient information of the intensity map, the two smoothing methods are combined to form a novel adaptive smoothing strategy. This method allows not only reducing the complexity, but also to maintain good signal steepness for the intensity map in the map decomposition process, which will lead to good performance in leaf sequencing process by multileaf collimator finally in the aspects of total number of monitor units. This paper test the new method on an opening head neck clinical case, as well as the quadratic smoothing method. The results show we can get lower bound of total number of monitor units for the intensity map decomposition. © 2010 IEEE.
{fenge}
78649299874	An event ontology construction approach to web crime mining	Along with the rapid popularity of the Internet, crime information on the web is becoming increasingly rampant, and the majority of them are in the form of text. Because a lot of crime information in documents is described through events, event-based semantic technology can be used to study the patterns and trends of web-oriented crimes. In our research project on cyber crime mining, we construct event ontology to extract the attributes and relations in web pages and reconstruct the scenario for crime mining. This article discusses the methods of analyzing and reconstructing the features of Chinese web pages. Event ontology and Support Vector Machine (SVM) classification are used to validate the proposed methods. As an example, a prototype system is implemented for Chinese text classification. The experimental results show that event ontology has many advantages in web crime mining application. ©2010 IEEE.
{fenge}
78649604941	Algorithm and implementation of TCP segments reorganization in network behavior management system	For the system of network behavior management, the traditional algorithm based on data packet pattern match could not meet practical usage, it seems especially to be useless when the data transmission is based on the TCP protocol. Thus, a new TCP segments reorganization algorithm is proposed in this paper, and the key questions to the TCP segments reorganization process, for example, the multi connections management, the duplicate segments processing, the out-of-order segments processing, are elaborated. Experiment results showed that the new algorithm is efficient and stable. © 2010 IEEE.
{fenge}
78651277969	Admission polices for outperforming the efficiency of search engines	We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time. In this paper, we propose an architecture that uses a combination of cache result and admission policy to improve the efficiency of search engines. In our system, we divide the cache into two layers to ensure the high hit ratio of the cache. We propose a admission policy to prevent infrequent queries from taking space of more frequent queries in the cache. We also introduce new eviction policy to update the result cache, which is more general than traditional heuristics such as LRU. We experiment with real query logs and a large document collection, and show that the hybrid cache enables efficient reduction of the query processing costs and thus is practical to use in Web search engines. ©2010 IEEE.
{fenge}
78651286156	Research on innovative talents training mode based on off-campus high-end practice environment	Through an analysis of the advantages and disadvantages of current undergraduate professional talents training modes and traditional examination-oriented education system, the importance of the vantage teaching resources complementation between the off-campus high-end practice environment and the in-campus innovative talents cultivation system is studied and discussed. At the same time, the new problems that how high-end courses teaching method, management mechanism and knowledge system will be naturally suited and extended to outside high-end practice environment and training organization are addressed. These problems must be systemally resolved through reformation, and some key thoughts and methods resolved these problems are introduced in this paper. These thoughts and methods have a good reference for Chinese higher education at present. © (2011) Trans Tech Publications.
{fenge}
78651549380	Distance maintaining compact quantum crossover based clonal selection algorithm	Premature convergence is one of the major difficulties with Clonal Selection Algorithm (CSA). It has been observed that this problem is closely tied to the problem of losing diversity in the population. In this paper, we propose a distance maintaining compact quantum crossover based CSA. The compact quantum crossover is useful for information exchanging between different solutions, whereas uses fewer antibodies. On the other hand, distance maintaining scheme permits the algorithm to fine tune its solutions. Simulation results on Traveling Salesman Problems (TSP) show that the novel algorithm is able to effectively balance exploration and exploitation of the search space and can also present the optimal or near-optimal solutions.
{fenge}
79951766833	Improved quantum crossover based Clonal Selection Algorithm	In this paper, we propose a novel quantum Clonal Selection Algorithm (NQCSA) which combines the traditional Clonal Selection Algorithm (CSA) and the improved quantum crossover for Traveling Salesman Problems (TSP). The NQCSA integrates the characteristics of both CSA and quantum mechanics. By using CSA which is derived from clonal selection theory, the solution space can be exploited and further explored parallel with more efficiency. Furthermore, the probability of local minimum can be reduced because of the quantum interference mechanics. The algorithm is applied to numerous bench-mark problems of TSP and the obtained results show effectiveness of the proposed algorithm. © 2010 IEEE.
{fenge}
79953728223	Improvement on navigation behavior algorithm of agent in visual simulation	In the visual simulation system, especially large visual game or military simulation training systems, the navigation behavior of agent controls movement of agent through a series of actions response to external environment. The combination of multiple navigation behaviors can create complex behaviors of agent. In the current commonly used behavior determination methods, the combination of different behaviors may cause unreasonable result in some circumstance. Based on navigation behavior, the paper brought out an improved algorithm for navigation behavior of agents, which used heuristic method to balance the local and global behavior priority level, so that the selection of navigation behavior is more reasonable. © (2011) Trans Tech Publications.
{fenge}
79957836484	NDOD: An efficient neighboring dependent outlier detector for bias distributed large datasets	Outlier detection is an important problem for many domains, including fraud detection, network intrusion and medical diagnosis. Discovery of unexpected knowledge revealed from outliers is becoming an integral aspect of data mining. Existing works in this field fall short of the adaptability to the distributive feature of the dataset. This paper presents a novel approach for outlier detection with high efficiency and the ability to closely monitor the neighboring density characteristics around outliers. A generalized neighboring dependent outlier is defined, followed by a cell-based detection algorithm. Results of extensive experimental studies on real-world and synthetic datasets demonstrate the effectiveness of the algorithm with respect to the size, the bias distributive structure of the datasets. © 2011 IEEE.
{fenge}
79960129642	Improved clonal selection algorithm for knapsack problem	Clonal selection theory is one of the important theories in immunological area. Applying clonal selection to optimization problems can also be found in many papers. In this paper, the classical Clonal Selection Algorithm (CSA) is extended by introducing the receptor editing mechanism which provides a chance for autoreactive cells to survive during affinity maturation process. To demonstrate the effectiveness and applicability of the novel CSA (NCSA), experiments are carried out on the knapsack problem. Simulation results show that the NCSA has a better performance when compared to the traditional CSA and other algorithms.
{fenge}
79961077395	Receptor editing-based clonal selection algorithm (REbCSA) for numerical optimization problem	According to the clonal selection theory, the antigen imposes a selective pressure on the antibody population by allowing only those cells which specifically recognize the antigen to be selected for proliferation and differentiation. However, recent investigations indicate that a small proportion of immune cells do not succumb to clonal deletion but rather, through secondary receptor gene recombination events, eliminate the autoreactive specificity and express a novel non-autoreactive antigen receptor. In this paper, we extend the traditional CSA by introducing the receptor editing mechanism which provides a chance for autoreactive cells to survive during affinity maturation process. To demonstrate the effectiveness and applicability of the improved CSA (REbCSA), several experiments are carried out on a class of numerical optimization problems. The results show that the REbCSA has a better performance when compared to the traditional CSA and other algorithms.
{fenge}
80053958483	A method of ranking event classes for event ontology	An event ontology is a shared, objective, and formal specification of an event class system model. In comparison with the conventional ontology, the event ontology represents knowledge with a higher granularity. For event ontology, non-classified relations are more important such as cause-effect relation and follow relation, and there are certain associative strengths between them. In this paper, we propose a method of ranking event classes for event ontology, which is the foundation for evaluating event ontology and computing the weights of event classes. The experimental results show that the method proposed can achieve significantly better ranking results over current existing algorithms.
{fenge}
80054001361	Stochastic compact quantum crossover based clonal selection algorithm for traveling salesman problems	Traditional Clonal Selection Algorithm (CSA) is not often satisfactory and is easy to be trapped in local optima so as to be premature convergence. To overcome such a problem, improved quantum interference crossover had been used for information exchanging during different immune cells. However, the population size in the improved quantum crossover must equal the number of cities of Traveling Salesman Problem. To reduce the complexity of the improved quantum crossover, a novel compact quantum crossover is introduced and embedded in this paper. This stochastic compact crossover uses less antibodies than that of traditional quantum crossover. Simulation results on Traveling Salesman Problems (TSP) show that the novel compact crossover based CSA is better than traditional CSA or other improved CSAs.
{fenge}
80054049429	An improved clonal selection algorithm with feedback quantum interference crossover	Clonal Section Algorithm (CSA), an artificial immune system (AIS) inspired by natural adaptive immune system, has been verified as a powerful approach for solving optimization problems. However, CSA is frequently faced with a problem of premature convergence. This premature convergence occurs when the population of a CSA reaches a suboptimal state that the clonal operators can no longer produce antibodies with a better performance. In order to balance the exploration and exploitation search, a feedback quantum interference crossover is proposed and introduced to the CSA for information exchanging between different antibodies. The experimental results using Traveling Salesman Problems (TSP) demonstrate the novel CSA performs better than classical CSA and other quantum crossover based CSAs.
{fenge}
80455162620	Chaotic quantum genetic algorithm and its application	In order to enhance the global and local search ability of genetic algorithm (GA) in solution space, an improved GA is introduced in this paper. First, the chaotic initialization is introduced into GA to improve its global search performance. Furthermore, improved quantum crossover is introduced to exchange information between different chromosomes, this action is useful for improving local search ability of GA. The performance of the proposed algorithm is evaluated by simulating a number of traveling salesman problems(TSP). Simulation results show that the proposed algorithm can avoid the premature convergence and has superior ability of searching the global optimal or near-optimum solutions.
{fenge}
84555206647	An improved Abrams-Strogatz model based protocol for agent competition and strategy designing	Strategy designing is a key task in today's market place competition and many other application fields. While dominating over competitors is unrealistic or too expensive to seek after, finding dynamic equilibrium in the competition progress becomes critical and practical in lots of scenarios. Inspired by the Abrams and Strogatz model originally proposed for monitoring language death and competition, this paper presented a novel mechanism for agent's competition equilibrium point finding and strategy designing. By analyzing historical data and status property of the two competition agents, an agent could evaluate its own status and forecast the long range trends of the game based upon the improved Abrams and Strogatz model. A posteriori status evaluation based strategy designing protocol is purposed and implemented. Experiments and simulation results showed desirable consistence with theoretical analysis. The protocol presented is valuable for adoption with proper settings to practical applications. © 2011 Springer-Verlag Berlin Heidelberg.
{fenge}
84855170703	Dynamic quantum crossover based clonal selection algorithm for solving traveling salesman problem	The applications of Clonal Selection Algorithms (CSAs) in solving combinatorial problems are frequently faced with a premature convergence problem and the maturation processes are trapped into a local optimum. This premature convergence problem occurs when the population of a clonal selection algorithm reaches a suboptimal state that the immune operators can no longer produce new immune cells with high affinities. To overcome this problem, different crossover operators have been investigated and embedded into CSAs. In this research, a dynamic quantum interference crossover is built up during maturation process to effectively accumulate useful receptor editing operation. The proposed crossover operator can be applied to improve the global convergence behavior of CSA. The experimental results focus on Traveling Salesman Problems (TSPs) show that the proposed algorithm is very effective in preventing the premature convergence problem when compared with other approaches.
{fenge}
84863310417	Adaptive clonal selection algorithm for numerical optimization	According to the clonal selection theory, random point mutation is an important mechanism for the maturation process. In previous algorithms, constant number points are selected to be performed random hypermutation. However, a large proportion of the cloned population becomes dysfunctional or develops into harmful anti-self cells after the mutation. In addition, large number points mutation is little effective to improve the affinity in latter process. As a results, we propose an adaptive clonal selection algorithm in this paper. This new algorithm reduces the mutation point number gradually during the maturation process. The algorithm is applied to numerous bench-mark problems of numerical optimization problems and the obtained results show effectiveness of the proposed algorithm.
{fenge}
84865582799	Review on innovative practice teaching and its quality evaluation system for software engineering specialty	Based on the analysis of training status of practice innovation talents in Chinese universities, The construction and development mechanism of high-end practice teaching environment and innovative practice teaching system are researched and discussed thoroughly. And then, the evaluation system and monitoring mechanism of practical teaching quality based on the high-end practice environment and innovative practice teaching sisyem are systematically introduced. Through actual teaching practice, thoughts and approach represented by this paper are very effective and important to train practice innovation abilities and entrepreneurial and employment awareness of the undergraduates today. © 2012 Springer-Verlag GmbH.
{fenge}
84867242360	Fluence map optimization (FMO) with dose-volume constraints in IMRT using the geometric distance sorting method	A new heuristic algorithm based on the so-called geometric distance sorting technique is proposed for solving the fluence map optimization with dose-volume constraints which is one of the most essential tasks for inverse planning in IMRT. The framework of the proposed method is basically an iterative process which begins with a simple linear constrained quadratic optimization model without considering any dose-volume constraints, and then the dose constraints for the voxels violating the dose-volume constraints are gradually added into the quadratic optimization model step by step until all the dose-volume constraints are satisfied. In each iteration step, an interior point method is adopted to solve each new linear constrained quadratic programming. For choosing the proper candidate voxels for the current dose constraint adding, a so-called geometric distance defined in the transformed standard quadratic form of the fluence map optimization model was used to guide the selection of the voxels. The new geometric distance sorting technique can mostly reduce the unexpected increase of the objective function value caused inevitably by the constraint adding. It can be regarded as an upgrading to the traditional dose sorting technique. The geometry explanation for the proposed method is also given and a proposition is proved to support our heuristic idea. In addition, a smart constraint adding/deleting strategy is designed to ensure a stable iteration convergence. The new algorithm is tested on four cases including head-neck, a prostate, a lung and an oropharyngeal, and compared with the algorithm based on the traditional dose sorting technique. Experimental results showed that the proposed method is more suitable for guiding the selection of new constraints than the traditional dose sorting method, especially for the cases whose target regions are in non-convex shapes. It is a more efficient optimization technique to some extent for choosing constraints than the dose sorting method. By integrating a smart constraint adding/deleting scheme within the iteration framework, the new technique builds up an improved algorithm for solving the fluence map optimization with dose-volume constraints. © 2012 Institute of Physics and Engineering in Medicine.
{fenge}
84867756229	A method of personal attribute extraction combining rules and statistics	With the rapid development of Internet information, how to obtain the required information from the vast amounts of unstructured texts is becoming an important issue today. Text retrieval, classification, extraction and other information processing technologies have made considerable progress, but the automatic information extraction for personal attributes does not cause enough attention. A method of combining rules and statistics is proposed for personal attribute extraction in this paper. First, rule sare configured to describe various personal attributes. Secondly, regular distance patterns of various attributes in the texts are obtained through statistics. Then, the algorithm of extracting personal attributes combining rules and statistics is proposed. Finally, the experiment is implemented for 200 documents about scientists. The results show that the average precision of extracting personal attributes is 0.84, the average recall is 0.83, and the average F-measure is 0.83.
{fenge}
84867761666	Automatic extraction of node information for chemical molecular structure images	A chemical molecular image represents the connection relations among atoms, which uses nodes to stand for atoms of compound structure and uses edges to express connection bonds. The extraction of node information is focused on in the paper, and the main works include: (1) dividing nodes into two kinds in terms of node contents: intersection and endpoint; (2) identifying intersections through computing the deviation of pixels, which are lager deviation of others; and (3) extracting atomic contents of nodes after identifying nodes and merging them. Through implementing experiments for BMP images of chemical molecular structure, it shows that the accuracy of extracting atomic information of nodes by the method proposed is very high. The work lays foundations for automatic extraction information for chemical molecular structure images, and for chemical molecular structure images-based retrieval. © 2012 IEEE.
{fenge}
84869237559	Optimized query execution in E-commerce sites	Result caching is an efficient technique for reducing the query processing load, hence it is commonly used in search engines. Caching is a crucial performance component of large-scale web search engines, as it greatly helps reducing average query response times and query processing workloads on backend search clusters. In this paper, we study query result caching and propose a cache management policy for achieving higher hit ratios compared to traditional heuristics methods. Our cache management policy comprises an eviction policy and an admission policy, and it divides the memory allocated for caching into two parts. In this paper, we propose a new set of feature-based cache eviction policies that achieve significant improvement over previous methods, and study an admission policy which uses state features to prevent infrequent queries from polluting the cache, substantially improving the hit ratios greatly. Evaluation using a real workload shows that our algorithm can achieve hit rate improvements as well as reduction in average hit ages.
{fenge}
84870537744	Provably secure certificate-based signature scheme for Ad Hoc networks	Certificate-based cryptography proposed by Gentry in Eurocrypt 2003 combines the advantages of traditional public key cryptography (PKI) and identitybased cryptography, and removes the certificate management problem and the private key escrow security concern. Based on computational Diffie-Hellman assumption, a certificate-based signature scheme is constructed to insure the security of communication in mobile Ad hoc networks,. The security of the scheme is proved under the Random Oracle Model. The scheme is also efficient, since the signing algorithm does not need the computation of the bilinear pairing and the verification algorithm needs that computation only once. Thus it is particularly useful in Ad hoc networks. © 2012 ACADEMY PUBLISHER.
{fenge}
84871352295	Optimizing the web search engines with features and caching	Large web search engines have to answer thousands of queries per second with interactive response times under tight latency constraints. Query processing is a major cost factor in operating large web search engines. To keep up with this immense workload, a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. We focus on two techniques, inverted index compression and feature-based caching. We perform a comparison and evaluation of several inverted compression algorithms. We then set a new feature-based eviction policies that achieve significant improvements over previous methods. Experimental results shows that this approach can achieve hit rate improvements. © 2010 IEEE.
{fenge}
84876172893	Event ontology model and event class ranking	The authors define an event and an event class, and illustrate the relations between event classes. Then event-oriented ontology model is constructed, which will contribute new methods and technologies to semantic-based knowledge processing. Furthermore, taking event class ranking as an example based on event ontology, the method of HARank-based event class ranking is introduced for event ontology, and corresponding experimental and evaluating results are presented. In comparison with conventional methods, event ontology represents knowledge with a higher granularity, and it will be more suitable for the representation of the knowledge about the movement and state changes in the real world. © 2013 Peking University.
{fenge}
84880708685	Research on Innovative Practice Teaching System Based on the High-End Practice Teaching Environment for Software Engineering Speciality	Through the analysis of current culture status of undergraduate engineering applied talents, the paper points out that the main reason causing the lack of student integrated application and practice innovation abilities is the poor construction of high-end practice environment. And then, how to enhance the practice environment construction and practical teaching innovation as well as building an appropriate innovation practice teaching system for engineering applied talents are systematically discussed. It is very obvious that the application and promotion of this kind of innovative practice teaching system could enhance the practice innovation abilities and entrepreneurial and employment awareness of the graduates. © Springer-Verlag Berlin Heidelberg 2011.
{fenge}
84880714976	Improved techniques for caches of search engines results	Result caching is an efficient technique for reducing the query processing load, hence it is commonly used in search engines. In this paper, we study query result caching and proposes a cache management policy for achieving higher hit ratios compared to traditional heuristics methods. Our cache management policy comprises an eviction policy and an admission policy, and it divides the memory allocated for caching into two parts. Our first contribution is a new set of feature-based cache eviction policies that achieve significant improvement over previous methods. Our second contribution is a study an admission policy which uses stateful features to prevent infrequent queries from polluting the cache, substantially improving the hit ratios greatly. Experiment results on two different query logs show that our policy achieves higher hit ratios when compared to previously methods. © 2010 IEEE.
{fenge}
84881163322	On the electromagnetic reflectivity properties of selected materials	With the high service requirement of mobile communication systems, complex electromagnetic reflectivity properties of common building materials and signals scattering effect in complex indoor environment must be carefully analyzed. This paper put forward a set of detailed studies of a variety of common building materials' reflectivity properties in the process of signal transmission. Measurements involve the shape, size and direction of aluminum, wood, perspex and plasterboard under an anechoic chamber environment. Experiments results proved that the aluminum board and plasterboard show good reflection ability while wood material proves to be less satisfied reflector. Absorber put behind Perspex board is demonstrated able to make a big difference on Perspex board's reflection performance. © 2012 AICIT.
{fenge}
84884154500	Inverse planning optimization method for intensity modulated radiation therapy	In order to facilitate the leaf sequencing process in intensity modulated radiation therapy (IMRT), and design of a practical leaf sequencing algorithm, it is an important issue to smooth the planned fluence maps. The objective is to achieve both high-efficiency and high-precision dose delivering by considering characteristics of leaf sequencing process. The key factor which affects total number of monitor units for the leaf sequencing optimization process is the max flow value of the digraph which formulated from the fluence maps. Therefore, we believe that one strategy for compromising dose conformity and total number of monitor units in dose delivery is to balance the dose distribution function and the max flow value mentioned above. However, there are too many paths in the digraph, and we don't know the flow value of which path is the maximum. The maximum flow value among the horizontal paths was selected and used in the objective function of the fluence map optimization to formulate the model. The model is a traditional linear constrained quadratic optimization model which can be solved by interior point method easily. We believe that the smoothed maps from this model are more suitable for leaf sequencing optimization process than other smoothing models. A clinical head-neck case and a prostate case were tested and compared using our proposed model and the smoothing model which is based on the minimization of total variance. The optimization results with the same level of total number of monitor units (TNMU) show that the fluence maps obtained from our model have much better dose performance for the target/non-target region than the maps from total variance based on the smoothing model. This indicates that our model achieves better dose distribution when the algorithm suppresses the TNMU at the same level. Although we have just used the max flow value of the horizontal paths in the diagraph in the objective function, a good balance has been achieved between the dose conformity and the total number of monitor units. This idea can be extended to other fluence map optimization model, and we believe it can also achieve good performance. © Adenine Press (2013).
{fenge}
84887536457	Research on personal education background extraction using rules	With the explosive growth of Internet information, how to obtain the required information from the vast amounts of text information is becoming an important issue today. Extraction of personal attribute has made considerable progress, including name, sex, place of birth, date of birth, related events, etc. But the extraction of personal education background information does not arouse researcher's enough attention. The attribute of personal education background is very complex in text and involves all kinds of education structures such as primary school, middle school and university. We put forward a new method of extracting personal attributes from texts based on rules. Firstly, rules of personal education background are formulated after analyzing a lot of texts about people. Secondly, the related algorithm is designed to extract personal education background from unstructured texts based on rules. Finally, the experiment is implemented for 100 documents about people. The results show that the average precision of extracting personal education background is 0.898, the average recall is 0.8959, and the average F-measure is 0.8968.
{fenge}
0142060787	Mean approximation approach to a class of grid-based clustering algorithms	In recent years, the explosively growing amount of data in numerous clustering tasks attracts considerable interest in boosting the existing clustering algorithms to large datasets. The mean approximation approach is discussed to improve a spectrum of partition-oriented density-based algorithms. This approach filters out the data objects in the crowded grids and approximates their influence to the rest by their gravity centers. Strategies on implementation issues as well as the error bound of the mean approximation are presented. Mean approximation leads to less memory usage and simplifies computational complexity with minor lose of the clustering accuracy. Results of exhaustive experiments reveal the promising performance of this approach.
{fenge}
84890217868	Web news oriented event multi-elements retrieval	To meet the demand of effectively acquiring event information, a method of Web news-oriented event multi-elements retrieval is studied through analyzing characteristics of Web news and event multi-elements retrieval process. Firstly, a model of Web news-oriented event multi-elements retrieval is proposed. Secondly, event multi-elements query terms are formally defined by using the BNF (Backus-Naur form). Finally, incorporating the importance of event action element, Web news title and the distance between event terms and constrained terms, a method of computing the relevance between query terms and the document is proposed. Sixteen event query topics are created to implement the experiments. With the proposed method, this paper evaluates the index P@n based on the Baidu search engine, getting average P@10 of 0.85 and average P@20 of 0.83. This paper also evaluates the index F-measure through manually labeling the corpus with same method, obtaining average F-measure of 0.74. The results show that the proposed method offers more effective performances. © 2013 ISCAS.
{fenge}
84901336589	Mining core motivations among motivational agents	Motivation is an important factor in reasoning about rational behavior of intelligent agents and analyzing the property of social network circles. Recent study on motivational agent paid their main attention on the mechanism of reasoning and multi-agent Cooperation. How motivation affects the internal structure of the allied agent groups are less considered. This paper proposes a methodology for motivational agent clustering, cohesion property analyzing and core motivational agent identifying. The methodology first finds clustered agents from the underlying graph that captures the similarity based interconnection topology of the agents. Then, the subgroups of agents that have high degrees of connectivity are extracted which can be thought of as the key representatives of the whole agent clusters. Our empirical results on real survey data and simulation platform show that our method is quite favorable for clearly partitioning large body of motivational agents and helping the analyzer to identify internal structure of the agent groups. Our algorithms can be adapted in various ways for social network behavior analyzing, intrusion detection and marketplace bidding strategy designing. © 2013 IEEE.
{fenge}
84921836908	A Method of Multi-Topic Crawling Based on Search Strategy	Aiming at the low efficiency of multi-topic crawling, the difference between built-in search engines (BSEs) and general search engines (GSEs) is investigated. The idea and method of dividing topic rules into atomic rules are proposed respectively, and three relations (equating relation, exchanging relation and containing relation) between atomic rules are analyzed. Based on atomic rule relations, the different allocation strategies for BSEs and GSEs are designed, which can not only improve the precision of topic-specific crawling, but also reduce crawling times. Furthermore, a method of sentence cluster-based relevance computing between topics and documents is proposed to solve the low precision problem of directly crawling information by atomic rules. We conduct an experiment with 138 topic rules (containing 8223 atomic rules), 14 BSEs and 4 GSEs for evaluating the number of crawling information and related information in unit time. The results show that the proposed method offers more effective performances.
