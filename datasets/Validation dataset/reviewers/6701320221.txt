{fenge}
84932621265	Hydra: Efficient detection of multiple concurrency bugs on fused CPU-GPU architecture	Detecting concurrency bugs, such as data race, atomicity violation and order violation, is a cumbersome task for programmers. This situation is further being exacerbated due to the increasing number of cores in a single machine and the prevalence of threaded programming models. Unfortunately, many existing software-based approaches usually incur high runtime overhead or accuracy loss, while most hardware-based proposals usually focus on a specific type of bugs and thus are inflexible to detect a variety of concurrency bugs. In this paper, we propose Hydra, an approach that leverages massive parallelism and programmability of fused GPU architecture to simultaneously detect multiple types of concurrency bugs, including data race, atomicity violation and order violation. Hydra instruments and collects program behavior on CPU and transfers the traces to GPU for bug detection through on-chip interconnect. Furthermore, to achieve high speed, Hydra exploits bloom filter to filter out unnecessary detection traces. Hydra incurs small hardware complexity and requires no changes to internal critical-path processor components such as cache and its coherence protocol, and is with about 1.1% hardware overhead under a 32-core configuration. Experimental results show that Hydra only introduces about 0.35% overhead on average for detecting one type of bugs and 0.92% overhead for simultaneously detecting multiple bugs, yet with the similar detectability of a heavyweight software bug detector (e.g., Helgrind).
{fenge}
0032281384	Cross-loop reuse techniques for cache optimization on multiprocessors	The use of the cache reduces the gap between the CPU speed and the memory latency, so the cache hit ratio becomes an important factor which affects the performance of multiprocessor system. Researchers have developed a number of optimization to enhance data locality, increase the cache hit ratio and bring the multiprocessor system performance into a better play. These techniques focus on how to enhance data locality within a parallel loop, reduce and even eliminate the cache line thrashing due to true or false sharing of the cache line. Exploitation and utilization of cross-loop reuse on multiprocessors are seldom discussed. How to exploit and utilize these cross-loop reuse, and put forward some feasible and easy ways for implementation are discussed. Application of these methods can increase the cache hit ratio, thus improve the performance of multiprocessor system.
{fenge}
23944482169	Vectorization for real-life multimedia applications on processors' multimedia extensions	Almost all vendors have added multimedia extensions (MME) to their processors to speedup multimedia applications. However, researches on automatic vectorization of compiler so far have not fully utilized these MMEs to boost the performance of real-life multimedia applications. This results from their focus on vectorization for normal arithmetic operations which rarely have speedup and their failure to fully exploit benefits from MME support for multimedia specific operations. These multimedia specific operations have various forms in source code, especially those expressed in multiple statements and scattered in program. This fact greatly hindered their vectorization. In this paper, the authors resolve this problem by enhancing the classic vectorization algorithm to flexibly and uniformly vectorize beneficial normal arithmetic and multimedia specific operations. The authors mainly added two extra steps: one to uniform the appearance of operations and the other to recognize vectorizable operations. The experiment shows that above algorithm has satisfactory performance improvement in several real-life multimedia applications. The results reach 43.9% maximum and 7.4% average speedup for Accelerating Suite of Berkeley Multimedia Workload. Furthermore, any system based on the algorithm the authors proposed can be extended to vectorize more complicate cases by simply adding corresponding rules.
{fenge}
24644524664	Boosting the performance of multimedia applications using SIMD instructions	Modern processors' multimedia extensions (MME) provide SIMD ISAs to boost the performance of typical operations in multimedia applications. However, automatic vectorization support for them is not very mature. The key difficulty is how to vectorize those SIMD-ISA-supported idioms in source code in an efficient and general way. In this paper, we introduce a powerful and extendable recognition engine to solve this problem, which only needs a small amount of rules to recognize many such idioms and generate efficient SIMD instructions. We integrated this engine into the classic vectorization framework and obtained very good performance speedup for some real-life applications. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
26444502069	Overflow controlled SIMD arithmetic	Although the "SIMD within a register" parallel architectures have existed for almost 10 years, the automatic optimizations for such architectures are not well developed yet. Since most optimizations for SIMD architectures are transplanted from traditional vectorization techniques, many special features of SIMD architectures, such as packed operations, have not been thoroughly considered. As operands are tightly packed within a register, there is no spare space to indicate overflow. To maintain the accuracy of automatic SIMDized programs, the operands should be unpacked to preserve enough space for interim overflow. By doing this, great overhead would be introduced. Furthermore, the instructions for handling interim overflows can sometimes prevent other optimizations. In this paper, a new technique, OCSA (overflow controlled SIMD arithmetic), is proposed to reduce the negative effects caused by interim overflow handling and eliminate the interference of interim overflows. We have applied our algorithm to the multimedia benchmarks of Berkeley. The experimental results show that the OCSA algorithm can significantly improve the performance of ADPCM-Decoder (110%), MESA-Reflect (113%) and DJVU-Encoder (106%). © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
29144450066	ADDI: An agent-based extension to UDDI for supply chain management	This paper proposes a new mediate module ADDI to organize the UDDI nodes published by the different corporations on supply chain and establish a UDDI dynamic ranking model. We suggest a novel model for supply chain conduct classification, and define the supply chain entity conduct types as a standard for web service classification and UDDI ranlang. In this model, Web service-oriented technologies and protocols are deployed for modeling, managing and executing business-oriented functionalities and environments. We focus on the efficient integration of supply chain as key points to harmonize these technologies. And agentorientation concepts and technologies are applied for SCM construction and interaction patterns.
{fenge}
3042642654	Increase parallel granularity and data locality by unimodular metrics	We discusses a loop transformation method which would increase the granularity of the loop body and improve the data locality of the transformed loop. By analyzing the dependence vector set of the given nested double-loop, we could merge several nodes in the iteration space, which have same outer loop variable value and different inner loop variable value into one node in the folded iteration space, while preserving the parallelism of inner loop at the same time. Thus, we increased the granularity of the parallel loop body. Furthermore, we discussed how to find a unimodular metrics to transform the given iteration space with the given dependence into an iteration space in which iteration nodes could be merged using our methods given above. We also present a method to preserve the locality of the original loop while doing our loop transformation and iteration space folding. Our method discussed in this article is the generalization of the wavefront method. Compared with the wavefront method, our method can achieve higher performance due to larger granularity and better data locality. We apply our method to ygx, a program of the IAPCM Benchmark, to evaluate the effect of the technique. The experiment data show that our method can spare the execution time by 22% compared with the wavefront method when the program is parallel processed by 4 CPU's on SGI origin 200 system which is a typical 4 CPU's SMP architecture.
{fenge}
33744483508	A Web service-based framework for supply chain management	This paper proposes a framework based on Web Service to organize the corporation nodes on supply chain. We have defined a strategy for aggregating the agents, including the normal agent and the mobile agent, into the Web service architecture and the functionalities for them to control the business conducts. We also devise a UDDI ranking frame based on analysis of supply chain activities. In this frame, Web Service-oriented technologies and protocols are deployed for modeling, managing and executing business-oriented functionalities. We focus on the efficient integration of supply chain as key points to harmonize these technologies. Agent- orientation concepts and technologies are applied to SCM construction and interaction patterns.
{fenge}
33744915522	Practical zero-knowledge arguments from ∑-protocols	Zero-knowledge (ZK) plays a central role in the field of modern cryptography and is a very powerful tool for constructing various cryptographic protocols, especially cryptographic protocols in E-commerce. Unfortunately, most ZK protocols are for general NP languages with going through general NP-reductions, and thus cannot be directly employed in practice. On the other hand, a large number of protocols, named ∑-protocols, are developed in industry and in the field of applied cryptography for specific number-theoretic languages (e.g. DLP and RSA), which preserves the ZK property only with respect to honest verifiers (i.e., they are not real ZK) but are highly practical. In this work, we show a generic yet practical transformation from ∑-protocols to practical (real) ZK arguments without general NP-reductions under either the DLP or RSA assumptions. © Springer-Verlag Berlin Heidelberg 2005.
{fenge}
33745963010	Live updating operating systems using virtualization	Many critical IT infrastructures require non-disruptive operations. However, the operating systems thereon are far from perfect that patches and upgrades are frequently applied, in order to close vulnerabilities, add new features and enhance performance. To mitigate the loss of availability, such operating systems need to provide features such as live update through which patches and upgrades can be applied without having to stop and reboot the operating system. Unfortunately, most current live updating approaches cannot be easily applied to existing operating systems: some are tightly bound to specific design approaches (e.g. object-oriented); others can only be used under particular circumstances (e.g. quiescence states). In this paper, we propose using virtualization to provide the live update capability. The proposed approach allows a broad range of patches and upgrades to be applied at any time without the requirement of a quiescence state. Moreover, such approach shares good portability for its OS-transparency and is suitable for inclusion in general virtualization systems. We present a working prototype, LUCOS, which supports live update capability on Linux running on Xen virtual machine monitor. To demonstrate the applicability of our approach, we use real-life kernel patches from Linux kernel 2.6.10 to Linux kernel 2.6.11, and apply some of those kernel patches on the fly. Performance measurements show that our implementation incurs negligible performance overhead: a less than 1 % performance degradation compared to a Xen-Linux. The time to apply a patch is also very minimal. Copyright © 2006 ACM.
{fenge}
33746037572	Optimizing compiler for shared-memory multiple SIMD architecture	With the rapid growth of multimedia and game, these applications put more and more pressure on the processing ability of modern processors. Multiple SIMD architecture is widely used in multimedia processing field as a multimedia accelerator. With the consideration of power consumption and chip size, shared memory multiple SIMD architecture is mainly used in embedded SOCs. In order to further fit mobile environment, there is the constraint of limited register number as well. Although shared memory multiple SIMD architecture simplify the chip design, these constraints are the major obstacles to map the real multimedia applications to these architectures. Until now, to our best knowledge, there is little research on the optimizing techniques for shared memory multiple SIMD architecture. In this paper, we present a compiler framework, which aims at automatically generating high performance codes for shared memory multiple SIMD architecture. In this framework, we reduce the competition of shared data bus through Increasing the register locality, improve the utilization of data bus by read-only data vector replication and solve the problem of limited register number through a resource allocation algorithm. The framework also handlers the issues concerning on data transformation. As the experimental results shown, this framework is successful in mapping real multimedia applications to shared memory multiple SIMD architecture. It leads to an average speedup by a factor of 3.19 and an average utilization of SM-SIMD architecture with 8 SIMD units by a factor of 52.6%. Copyright © 2006 ACM.
{fenge}
33749029011	Optimizing compiler for shared-memory multiple SIMD architecture	With the rapid growth of multimedia and game, these applications put more and more pressure on the processing ability of modern processors. Multiple SIMD architecture is widely used in multimedia processing field as a multimedia accelerator. With the consideration of power consumption and chip size, shared memory multiple SIMD architecture is mainly used in embedded SOCs. In order to further fit mobile environment, there is the constraint of limited register number as well. Although shared memory multiple SIMD architecture simplify the chip design, these constraints are the major obstacles to map the real multimedia applications to these architectures. Until now, to our best knowledge, there is little research on the optimizing techniques for shared memory multiple SIMD architecture. In this paper, we present a compiler framework, which aims at automatically generating high performance codes for shared memory multiple SIMD architecture. In this framework, we reduce the competition of shared data bus through increasing the register locality, improve the utilization of data bus by read-only data vector replication and solve the problem of limited register number through a resource allocation algorithm. The framework also handlers the issues concerning on data transformation. As the experimental results shown, this framework is successful in mapping real multimedia applications to shared memory multiple SIMD architecture. It leads to an average speedup by a factor of 3.19 and an average utilization of SM-SIMD architecture with 8 SIMD units by a factor of 52.6%. Copyright © 2006 ACM.
{fenge}
33751580453	Instruction schedule based on shared vector algorithm for two-dimensional SIMD architecture	This paper presents a scheduling algorithm to solve the problem of data vector reuse. The algorithm first computes the reuse data vectors (shared vector) among different SIMD instructions using representative element of data vectors and then schedules the SIMD instructions based on the information of shared vector. This algorithm can concretely reduce the cost of loading data and improve the parallelism of applications on architecture restricted two-dimensional SIMD architecture. The experimental results show that the algorithm can have an average speedup of 2.967 and an average SIMD instructions parallelism of 3.86 for real applications.
{fenge}
34547993985	Optimizing software cache performance of packet processing applications	Network processors (NPs) are widely used in many types of networking equipment due to their high performance and flexibility. For most NPs, software cache is used instead of hardware cache due to the chip area, cost and power constraints. Therefore, programmers should take full responsibility for software cache management which is neither intuitive nor easy to most of them. Actually, without an effective use of it, long memory access latency will be a critical limiting factor to overall applications. Prior researches like hardware multi-threading, wide-word accesses and packet access combination for caching have already been applied to help programmers to overcome this bottleneck. However, most of them do not make enough use of the characteristics of packet processing applications and often perform intraprocedural optimizations only. As a result, the binary codes generated by those techniques often get lower performance than that comes from hand-tuned assembly programming for some applications. In this paper, we propose an algorithm including two techniques - Critical Path Based Analysis (CPBA) and Global Adaptive Localization (GAL), to optimize the software cache performance of packet processing applications. Packet processing applications usually have several hot paths and CPBA tries to insert localization instructions according to their execution frequencies. For further optimizations, GAL eliminates some redundant localization instructions by interprocedural analysis and optimizations. Our algorithm is applied on some representative applications. Experiment results show that it leads to an average speedup by a factor of 1.974. Copyright © 2007 ACM.
{fenge}
34548713464	POLUS: A pOwerful live updating system	This paper presents POLUS, a software maintenance tool capable of iteratively evolving running software into newer versions. POLUS's primary goal is to increase the dependability of contemporary server software, which is frequently disrupted either by external attacks or by scheduled upgrades. To render POLUS both practical and powerful, we design and implement POLUS aiming to retain backward binary compatibility, support for multithreaded software and recover already tainted state of running software, yet with good usability and very low runtime overhead. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd and apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead: a less than 1% performance degradation (but 5% for one case). The time to apply an update is also minimal. © 2007 IEEE.
{fenge}
38049083830	Content recommendation system based on private dynamic user profile	As the amount of the accessible information in the Internet is overwhelming, personalized content recommendation system offers spam filtering service and suggests useful information to the end users. It is a hotspot in the research area of content management on WWW. Traditional recommendation systems do the data mining on web access logs, discover user's access patterns, and filter the information on behalf of the user at the server side. One critical limitation of traditional recommendation system is the lack of user's private daily data, such as schedules, favorite websites and personal emails. The reason for this limitation is the privacy leak issue when the server holds much more private user data. To solve this problem, this paper presents an agent-based personalized recommendation method called Content REcommendation System based on private Dynamic User Profile (CRESDUP). The system collects and mines the private data of user at the client side, discovers, stores and updates private Dynamic User Profile (DUP) at the client side. The system fetches preferred message from the content server according to DUP. An important usage of this technology is a personalized advertising system in the RSS (Rich Site Summary, or RDF Site Summary) reader application. Our experiment shows that the system can utilize DUP to identify the customers' potential preferences and deliver the more preferred messages, especially the advertisements, to people who are interested. © 2007 IEEE.
{fenge}
37649004406	An ontological engineering approach for automating inspection and quarantine at airports	Customs and quarantine departments are applying information systems to automate their inspection processes and improve their inspection efficiency and accuracy. The product codes from the Harmonized System (HS codes) are the essential elements of the system's integration, automation and intelligence. The identified HS codes are well-accepted and precise product references used by customs authorities, to match applicable policies to the products being inspected and taxed. Domain ontology for importing and exporting industry can be used to acquire HS codes for given products, and is a prerequisite for an integrated and intelligent automated inspection system. The authors have proposed and implemented an importing and exporting domain ontology. The ontology is composed of an integrated and comprehensive knowledge base derived from static dictionaries and the HS specification, and dynamic processing data. Based on this ontology, a reasoning engine is developed to generate HS codes intelligently for the given product names. Information systems can use the engine to get HS codes for submitted products and find applicable policies automatically. The ontology and the engine have been implemented in a Java-based platform and published as a HS Web service. In this paper, knowledge structure, reasoning mechanism and implementation details for the domain ontology and reasoning engine are presented. A test bed in the application environment has been conducted and experimental results have been obtained. The ontology and the service have the potential to be widely used by authorities and international traders of importing and exporting industry around the world. © 2007 Elsevier Inc. All rights reserved.
{fenge}
0033141087	Intraprocedural alias analysis for pointer array	The pointer alias analysis play an important role in the parallelizing optimization of C programming, however all the previous analyzing algorithms can only be used to analyze the pointer scalar. The extended pointer representation for alias information of pointer arrays was proposed, which can represent not only the alias information of pointer scalar, but also the alias information of pointer array. The algorithm for intraprocedural alias analysis of pointer array was presented, which comprises both the pointer scalar and pointer array analyses. It can be used to solve some problems which couldn't be solved by the previous algorithms.
{fenge}
38149109896	Data pipeline optimization for shared memory multiple-SIMD architecture	The rapid growth of multimedia applications has been putting high pressure on the processing capability of modern processors, which leads to more and more modern multimedia processors employing parallel single instruction multiple data (SIMD) units to achieve high performance. In embedded system on chips (SOCs), shared memory multiple-SIMD architecture becomes popular because of its less power consumption and smaller chip size. In order to match the properties of some multimedia applications, there are interconnections among multiple SIMD units. In this paper, we present a novel program transformation technique to exploit parallel and pipelined computing power of modern shared-memory multiple-SIMD architecture. This optimizing technique can greatly reduce the conflict of shared data bus and improve the performance of applications with inherent data pipeline characteristics. Experimental results show that our method provides impressive speedup. For a shared memory multiple-SIMD architecture with 8 SIMD units, this method obtains more than 3.6X speedup for the multimedia programs. © Springer-Verlag Berlin Heidelberg 2007.
{fenge}
34248665610	Multi-projector display wall system for windows desktop applications	Large-format high-resolution display systems are increasingly adopted in virtual reality, digital museum, scientific visualization, and other application fields. Traditionally, wall size display system is driven by a cluster of PCs or special high end graphics supercomputer. With the rapid advance of PC graphics hardware, one inexpensive mainstream PC has the power to drive multiple high-resolution displays. This paper proposes one multi-projector display wall system driven by single PC. Besides graphics intensive applications, this system can transparently and efficiently display windows desktop applications without modifying the source code. It takes advantage of the bandwidth of PCI Express bus, and supports at least triple projectors, which is suitable for constructing immersive virtual reality center. Design geometric alignment and photometric correction methods are given to display the frame buffer content seamlessly on the planar display screen. Experimental results show that this system can achieve real time refresh rates. It does little side effect on the performance of common non-graphics windows desktop applications.
{fenge}
84861603645	Mercury: Combining performance with dependability using self-virtualization	Virtualization has recently gained popularity largely due to its promise in increasing utilization, improving availability and enhancing security. Very often, the role of computer systems needs to change as the business environment changes. Initially, the system may only need to host one operating system and seek full execution speed. Later, it may be required to add other functionalities such as allowing easy software/hardware maintenance, surviving system failures and hosting multiple operating systems. Virtualization allows these functionalities to be supported easily and effectively. However, virtualization techniques generally incur non-negligible performance penalty. Fortunately, many virtualization- enabled features such as online software/hardware maintenance and fault tolerance do not require virtualization standby all the time. Based on this observation, this paper proposes a technique, called Self-virtualization, which provides the operating system with the capability to turn on and off virtualization on demand, without disturbing running applications. This technique enables computer systems to reap most benefits from virtualization without sacrificing performance. This paper presents the design and implementation of Mercury, a working prototype based on Linux and Xen virtual machine monitor. The performance measurement shows that Mercury incurs very little overhead: about 0.2 ms on 3 GHz Xeon CPU to complete a mode switch, and negligible performance degradation compared to Linux. © 2012 Springer Science+Business Media, LLC & Science Press, China.
{fenge}
84866672217	CFIMon: Detecting violation of control flow integrity using performance counters	Many classic and emerging security attacks usually introduce illegal control flow to victim programs. This paper proposes an approach to detecting violation of control flow integrity based on hardware support for performance monitoring in modern processors. The key observation is that the abnormal control flow in security breaches can be precisely captured by performance monitoring units. Based on this observation, we design and implement a system called CFIMon, which is the first non-intrusive system that can detect and reason about a variety of attacks violating control flow integrity without any changes to applications (either source or binary code) or requiring special-purpose hardware. CFIMon combines static analysis and runtime training to collect legal control flow transfers, and leverages the branch tracing store mechanism in commodity processors to collect and analyze runtime traces on-the-fly to detect violation of control flow integrity. Security evaluation shows that CFIMon has low false positives or false negatives when detecting several realistic security attacks. Performance results show that CFIMon incurs only 6.1% performance overhead on average for a set of typical server applications. © 2012 IEEE.
{fenge}
41949093184	Affine partition algorithm based on representative element	Partition is an optimization technique that distributes computations and data onto the different processors of parallel systems to get the maximizing parallelism and minimizing communication. The effect of partition algorithm can directly affect the performance of parallel systems. But there are many obstacles to effective partition in practical programs, such as imperfect loop nests and different array access scope. Previous partition algorithms can only finish the partition of sequences of perfect loop nests or cannot solve data partition consistent problem for different array access scope. This paper presents an affine partition algorithm based on representative element. When constructing the constraint relation for partition, it only remains the array references, which have contributes to partition constraint relation indeed and remove the trivial partition conflicts through discarding redundant array references to same array. This paper also presents a consistent partition algorithm to solve the data partition consistent problem. The algorithms can not only solve more practical partition problems, but also effectively reduce data reorganization communication in data partition. This technique has been implemented in AFT2004 parallel compiling system and can get better results for some practical programs.
{fenge}
47249085221	Mercury: Combining performance with dependability using self-virtualization	There has recently been increasing interests in using system virtualization to improve the dependability of HPC cluster systems. However, it is not cost-free and may come with some performance degradation, uncertain QoS and loss of functionalities. Meanwhile, many virtualization-enabled features such as online maintenance and fault tolerance do not require virtualization being always on. This paper proposes a technique, called self-virtualization, that supports dynamically attaching and detaching a full-fledged virtual machine monitor (VMM) beneath an operating system, without disturbing applications thereon, and rid the system of potential overhead when the virtualization is not needed. This technique enables HPC clusters to reap most benefits from virtualization without sacrificing performance. This paper presents the design and implementation of Mercury, a working prototype based on Linux and Xen VMM. Our performance measurement shows that Mercury incurs very little overhead: about 0.2 ms to complete a mode switch, and negligible performance degradation compared to Linux. © 2007 IEEE.
{fenge}
48149112954	Distance measurement in panorama	Computing world distances of scene features from the captured images is a common task in image analysis and scene understanding. Previous projective geometry based methods focus on measuring distance from one single image. Hence, the scope of measurable scene is limited by the field-of-view (FOV) of one single camera. In this paper, we propose one method of measuring distances of line segments in real world scene using panorama representation. With a full view panorama, the scope of measurable scene is increased and can fully cover the sphere of 360 × 360 FOV. With panorama representation, distance of long-range features, which can not be fully captured by a single image, can be measured from the panoramic image. A prototype system called PanoMeasure is developed for enabling user to interactively measure the distances of line segments. Experiments with simulated data and real measurement results verify that the method offers high accuracy. © 2007 IEEE.
{fenge}
52649112833	From speculation to security: Practical and efficient information flow tracking using speculative hardware	Dynamic information flow tracking (also known as taint tracking) is an appealing approach to combat various security attacks. However, the performance of applications can severely degrade without hardware support for tracking taints. This paper observes that information flow tracking can be efficiently emulated using deferred exception tracking in microprocessors supporting speculative execution. Based on this observation, we propose SHIFT, a low-overhead, software-based dynamic information flow tracking system to detect a wide range of attacks. The key idea is to treat tainted state (describing untrusted data) as speculative state (describing deferred exceptions). SHIFT leverages existing architectural support for speculative execution to track tainted state in registers and needs to instrument only load and store instructions to track tainted state in memory using a bitmap, which results in significant performance advantages. Moreover, by decoupling mechanisms for taint tracking from security policies, SHIFT can detect a wide range of exploits, including high-level semantic attacks. We have implemented SHIFT using the Itanium processor, which has support for deferred exceptions, and by modifying GCC to instrument loads and stores. A security assessment shows that SHIFT can detect both low-level memory corruption exploits as well as high-level semantic attacks with no false positives. Performance measurements show that SHIFT incurs about 1% overhead for server applications. The performance slowdown for SPEC-INT2000 is 2.81X and 2.27X for tracking at byte-level and word-level respectively. Minor architectural improvements to the Itanium processor (adding three simple instructions) can reduce the performance slowdown down to 2.32X and 1.8X for byte-level and word-level tracking, respectively. © 2008 IEEE.
{fenge}
56849083544	Metadata-driven optimization for sampling simulation	This paper analyzes the disadvantage of mainstream sampling simulation techniques using fixed-length samples and proposes a metadata-driven optimization for sampling simulation, BigLoopSP. In the approach, the compiler selects candidate loops and annotates the boundaries of those loops as metadata. Those metadata are used to divide the execution into varied-length candidate samples, for which each candidate sample corresponds to one iteration of the chosen loop. Since the program execution exhibits dynamic behaviors, our approach combines the knowledge from the metadata and the dynamic profiles to guide phase partition and selects simulation points for those phases. This approach effectively reduces the number of representative samples while preserving the good quality of them. So, compared with those mainstream sampling simulation techniques, such as SimPoint, our approach achieves better accuracy and reduces more simulation time (a speedup of 2.63X over SimPoint).
{fenge}
67650280617	Optimizing software cache performance of packet processing applications	Network processors (NPs) are widely used in many types of networking equipment due to their high performance and flexibility. For most NPs, software cache is used instead, of hardware cache due to the chip area, cost and power constraints. Therefore, programmers should take full responsibility for software cache management which, is neither intuitive nor easy to most of them. Actually, without an effective use of it, long memory access latency will be a critical limiting factor to overall applications. Prior researches like hardware multi-threading, wide-word accesses and packet access combination for caching have already been applied to help programmers to overcome this bottleneck. However, most of them do not make enough use of the characteristics of packet processing applications and often perform intraprocedural optimizations only. As a result, the binary codes generated by those techniques often get lower performance than that comes from hand-tuned assembly programming for some applications. In this paper, we propose an algorithm including two techniques - Critical Path Based Analysis (CPBA) and Global Adaptive Localization (GAL), to optimize the software cache performance of packet processing applications. Packet processing applications usually have several hot paths and CPBA tries to insert localization instructions according to their execution frequencies. For further optimizations, GAL eliminates some redundant localization instructions by interprocedural analysis and optimizations. Our algorithm is applied on some representative applications. Experiment results show that it leads to an average speedup by a factor of 1.974. Copyright © 2007 ACM.
{fenge}
70349125853	Undergraduate education in the computer system of software school, Fudan University	Recently, the multi-core technology is adopted on more and more computing platforms. As a result, the IT industry demands programmers to have solid knowledge in the whole computer system, so that they can develop highly efficient programs for various multi-core platforms. Consequently, it becomes a great challenge for undergraduate education to foster such programmers. To meet this challenge, the Software School of Fudan University builds up the current curriculum architecture to achieve effective education in the computer system. It is a progressive chain of four courses and several methods are adopted for it. We choose the most suitable textbooks and give them explanation and complement by teachers. Students are provided with full practical opportunities as well as open topics for innovative survey and analysis. The education plans are also flexibly adapted to students' career plans. Copyright 2008 ACM.
{fenge}
70350647286	Evaluating SPLASH-2 applications using MapReduce	MapReduce has been prevalent for running data-parallel applications. By hiding other non-functionality parts such as parallelism, fault tolerance and load balance from programmers, MapReduce significantly simplifies the programming of large clusters. Due to the mentioned features of MapReduce above, researchers have also explored the use of MapReduce on other application domains, such as machine learning, textual retrieval and statistical translation, among others. In this paper, we study the feasibility of running typical supercomputing applications using the MapReduce framework. We port two applications (Water Spatial and Radix Sort) from the Stanford SPLASH-2 suite to MapReduce. By completely evaluating them in Hadoop, an open-source MapReduce framework for clusters, we analyze the major performance bottleneck of them in the MapReduce framework. Based on this, we also provide several suggestions in enhancing the MapReduce framework to suite these applications. © 2009 Springer.
{fenge}
0035335813	A new approach to pointer analysis for assignments	Pointer analysis is a technique to identify at compile-time the potential values of the pointer expressions in a program, which promises significant benefits for optimizing and parallelizing compilers. In this paper, a new approach to pointer analysis for assignments is presented. In this approach, assignments are classified into three categories: pointer assignments, structure (union) assignments and normal assignments which don't affect the point-to information. Pointer analyses for these three kinds of assignments respectively make up the integrated algorithm. When analyzing a pointer assignment, a new method called expression expansion is used to calculate both the left targets and the right targets. The integration of recursive data structure analysis into pointer analysis is a significant originality of this paper, which uniforms the pointer analysis for heap variables and the pointer analysis for stack variables. This algorithm is implemented in Agassiz, an analyzing tool for C programs developed by Institute of Parallel Processing, Fudan University. Its accuracy and effectiveness are illustrated by experimental data.
{fenge}
72949117425	Optimizing techniques for saturated arithmetic with first-order linear recurrence	Saturated arithmetic is a typical operation in multimedia applications, most multimedia extensions in the instruction set architecture (ISA) of modern processors provide saturation instructions for such operation. Therefore, extensive researches have focused on how to utilize saturation instructions to optimize programs. Previous algorithms mainly focus on purely saturated arithmetic, however saturated arithmetic is often mingled with first-order linear recurrence (FOLR) in real life applications. When FLOR pattern appears in the program, previous algorithms can not identify the saturated arithmetic as well. In fact, the saturated arithmetic with FOLR (SAWF) is a new and significant pattern, especially, SAWF with one as coefficient is frequently used in multimedia applications. Hence, it is necessary to explore a method with which such pattern can be efficiently vectorized. This paper discusses how to vectorize SAWF, explores the efficient method to vectorize SAWF with one as coefficient and gives its evaluation and implement a library for the optimizing technique. Such an implementation manner can make compilers are able to exploit it more easily. The experimental results shows the optimizing technique can achieve a speedup of 1.19 to 1.46 on Pentium IV processor. At the same time, the optimizing techniques in this paper can also be used to develop a library for SAWF so a programmer can benefit even without changing the compiler. Copyright 2009 ACM.
{fenge}
75849128315	Optimizing the SIMD parallelism through bitwidth analysis	Although the SIMD units have been widely used in different architecture designs, the automatic optimizations for such architectures are not well developed yet. Since most optimizations for SIMD architectures are transplanted from traditional vectorization techniques, many special features of SIMD architectures, such as packed operations, have not been thoroughly considered. While operands are tightly packed within a register, there is no spare space to indicate overflow. To maintain the accuracy of automatic SIMDized programs, the operands should be unpacked to preserve enough space for interim overflow. However, such a strategy would lead to great overhead. Moreover, the additional instructions for handling overflows can sometimes prevent other optimizations. In this paper, a new technique, BCSA (Bitwidth controlled SIMD arithmetic), is proposed to reduce the negative effects caused by interim overflow handling and eliminate the interference of interim overflows. The algorithm is applied to the multimedia benchmarks of Berkeley. The experimental results show that the algorithm can significantly improve the performance of multimedia applications.
{fenge}
76749151451	Control flow obfuscation with information flow tracking	Recent micro-architectural research has proposed various schemes to enhance processors with additional tags to track various properties of a program. Such a technique, which is usually referred to as information flow tracking, has been widely applied to secure software execution (e.g., taint tracking), protect software privacy and improve performance (e.g., control speculation). In this paper, we propose a novel use of information flow tracking to obfuscate the whole control flow of a program with only modest performance degradation, to defeat malicious code injection, discourage software piracy and impede malware analysis. Specifically, we exploit two common features in information flow tracking: the architectural support for automatic propagation of tags and violation handling of tag misuses. Unlike other schemes that use tags as oracles to catch attacks (e.g., taint tracking) or speculation failures, we use the tags as flow-sensitive predicates to hide normal control flow transfers: the tags are used as predicates for control flow transfers to the violation handler, where the real control flow transfer happens. We have implemented a working prototype based on Itanium processors, by leveraging the hardware support for control speculation. Experimental results show that BOSH can obfuscate the whole control flow with only a mean of 26.7% (ranging from 4% to 59%) overhead on SPECINT2006. The increase in code size and compilation time is also modest. Copyright 2009 ACM.
{fenge}
77952400192	Optimizing crash dump in virtualized environments	Crash dump, or core dump is the typical way to save memory image on system crash for future offline debugging and analysis. However, for typical server machines with likely abundant memory, the time of core dump can significantly increase the mean time to repair (MTTR) by delaying the reboot-based recovery, while not dumping the failure context for analysis would risk recurring crashes on the same problems. In this paper, we propose several optimization techniques for core dump in virtualized environments, in order to shorten the MTTR of consolidated virtual machines during crashes. First, we parallelize the process of crash dump and the process of rebooting the crashed VM, by dynamically reclaiming and allocating memory between the crashed VM and the newly spawned VM. Second, we use the virtual machine management layer to introspect the critical data structures of the crashed VM to filter out the dump of unused memory. Finally, we implement disk I/O rate control between core dump and the newly spawned VM according to user-tuned rate control policy to balance the time of crash dump and quality of services in the recovery VM. We have implemented a working prototype, Vicover, that optimizes core dump on system crash of a virtual machine in Xen, to minimize the MTTR of core dump and recovery as a whole. In our experiment on a virtualized TPC-W server, Vicover shortens the downtime caused by crash dump by around 5X. Copyright © 2010 ACM.
{fenge}
77956586569	Why software hangs and what can be done with it	Software hang is an annoying behavior and forms a major threat to the dependability of many software systems. To avoid software hang at the design phase or fix it in production runs, it is desirable to understand its characteristics. Unfortunately, to our knowledge, there is currently no comprehensive study on why software hangs and how to deal with it. In this paper, we study the reported hangrelated bugs of four typical open-source software applications, aiming to gain insight into characteristics of software hang and provide some guidelines to fix them at the first place or remedy them in production runs. © 2010 IEEE.
{fenge}
78149252729	Tiled-MapReduce: Optimizing resource usages of data-parallel applications on multicore with tiling	The prevalence of chip multiprocessor opens opportunities of running data-parallel applications originally in clusters on a single machine with many cores. MapReduce, a simple and elegant programming model to program large scale clusters, has recently been shown to be a promising alternative to harness the multicore platform. The differences such as memory hierarchy and communication patterns between clusters and multicore platforms raise new challenges to design and implement an efficient MapReduce system on multicore. This paper argues that it is more efficient for MapReduce to iteratively process small chunks of data in turn than processing a large chunk of data at one time on shared memory multicore platforms. Based on the argument, we extend the general MapReduce programming model with "tiling strategy", called Tiled-MapReduce (TMR). TMR partitions a large MapReduce job into a number of small sub-jobs and iteratively processes one subjob at a time with efficient use of resources; TMR finally merges the results of all sub-jobs for output. Based on Tiled-MapReduce, we design and implement several optimizing techniques targeting multicore, including the reuse of input and intermediate data structure among sub-jobs, a NUCA/NUMA-aware scheduler, and pipelining a sub-job's reduce phase with the successive sub-job's map phase, to optimize the memory, cache and CPU resources accordingly. We have implemented a prototype of Tiled-MapReduce based on Phoenix, an already highly optimized MapReduce runtime for shared memory multiprocessors. The prototype, namely Ostrich, runs on an Intel machine with 16 cores. Experiments on four different types of benchmarks show that Ostrich saves up to 85% memory, causes less cache misses and makes more efficient uses of CPU cores, resulting in a speedup ranging from 1.2X to 3.3X. © 2010 ACM.
{fenge}
79952779458	COREMU: A scalable and portable parallel full-system emulator	This paper presents the open-source COREMU, a scalable and portable parallel emulation framework that decouples the complexity of parallelizing full-system emulators from building a mature sequential one. The key observation is that CPU cores and devices in current (and likely future) multiprocessors are loosely-coupled and communicate through well-defined interfaces. Based on this observation, COREMU emulates multiple cores by creating multiple instances of existing sequential emulators, and uses a thin library layer to handle the inter-core and device communication and synchronization, to maintain a consistent view of system resources. COREMU also incorporates lightweight memory transactions, feedback-directed scheduling, lazy code invalidation and adaptive signal control to provide scalable performance. To make COREMU useful in practice, we also provide some preliminary tools and APIs that can help programmers to diagnose performance problems and (concurrency) bugs. A working prototype, which reuses the widely-used QEMU as the sequential emulator, is with only 2500 lines of code (LOCs) changes to QEMU. It currently supports x64 and ARM platforms, and can emulates up to 255 cores running commodity OSes with practical performance, while QEMU cannot scale above 32 cores. A set of performance evaluation against QEMU indicates that, COREMU has negligible uniprocessor emulation overhead, performs and scales significantly better than QEMU. We also show how COREMU could be used to diagnose performance problems and concurrency bugs of both OS kernel and parallel applications. Copyright © 2011 ACM.
{fenge}
79955946773	A case for scaling applications to many-core with OS clustering	This paper proposes an approach to scaling UNIX-like operating systems for many cores in a backward-compatible way, which still enjoys common wisdom in new operating system designs. The proposed system, called Cerberus, mitigates contention on many shared data structures within OS kernels by clustering multiple commodity operating systems atop a VMM, and providing applications with the traditional shared memory interface. Cerberus extends a traditional VMM with efficient support for resource sharing and communication among the clustered operating systems. It also routes system calls of an application among operating systems, to provide applications with the illusion of running on a single operating system. We have implemented a prototype system based on Xen/Linux, which runs on an Intel machine with 16 cores and an AMD machine with 48 cores. Experiments with an unmodified MapReduce application, dbench, Apache Web Server and Memcached show that, given the nontrivial performance overhead incurred by the virtualization layer, Cerberus achieves up to 1.74X and 4.95X performance speedup compared to native Linux. It also scales better than a single Linux configuration. Profiling results further show that Cerberus wins due to mitigated contention and more efficient use of resources. Copyright © 2011 ACM.
{fenge}
0036641901	Run-time data-flow analysis	Parallelizing compilers have made great progress in recent years. However, there still remains a gap between the current ability of parallelizing compilers and their final goals. In order to achieve the maximum parallelism, run-time techniques were used in parallelizing compilers during last few years. First, this paper presents a basic run-time privatization method. The definition of run-time dead code is given and its side effect is discussed. To eliminate the imprecision caused by the run-time dead code, backward data-flow information must be used. Proteus Test, which can use backward information in run-time, is then presented to exploit more dynamic parallelism. Also, a variation of Proteus Test, the Advanced Proteus Test, is offered to achieve partial parallelism. Proteus Test was implemented on the parallelizing compiler AFT. In the end of this paper the program fpppp.f of Spec95fp Benchmark is taken as an example, to show the effectiveness of Proteus Test.
{fenge}
79957480604	A comprehensive analysis and parallelization of an image retrieval algorithm	The prevalence of the Internet and cloud computing has made multimedia data, such as image data and video data, become major data types in our daily life. For example, many data-intensive applications, such as health care and video recommendation, involve collecting, indexing and retrieving tera-scale multimedia data every day. With such a huge amount of multimedia data to process, the processing speed has been one of the major challenges to guarantee real-time requirements. The advent of multi-core hardware has opened new opportunities to improve the effectiveness of multimedia data processing. In this paper, we make a comprehensive analysis on different potential parallelism, including pipeline parallelism, task parallelism at both scale level and block level, data parallelism, and their combinations, in a typical image retrieval algorithm called SURF, which is the core algorithm of many multimedia (i.e., image and video) retrieval applications. Experimental results show the following observations of parallelism in SURF: 1) when only one level parallelism is exploited, block-level parallelism is more efficient and scalable than other alternatives; 2) data parallelism cannot be ignored especially when parallel resources increase and 3) the combination of block-level parallelism and pipeline parallelism is the most efficient parallelizing manner for the studied image retrieval algorithm. Based on these observations, we have implemented a parallel image retrieval algorithm. It can be easily mapped onto different multi-core platforms with good scalability. On a commodity server machine with 16-core, the parallel implementation achieves a speedup of 13X, which is 84% faster than P-SURF, a previous state-of-the-art parallelization of SURF on CPU; while on GPGPU, it achieves a speedup of 46X, which is 53% faster than CUDA SURF, a previous state-of-the-art parallelization of SURF on GPGPU. © 2011 IEEE.
{fenge}
79959895844	Optimizing crash dump in virtualized environments	Crash dump, or core dump is the typical way to save memory image on system crash for future offline debugging and analysis. However, for typical server machines with likely abundant memory, the time of core dump can significantly increase the mean time to repair (MTTR) by delaying the reboot-based recovery, while not dumping the failure context for analysis would risk recurring crashes on the same problems. In this paper, we propose several optimization techniques for core dump in virtualized environments, in order to shorten the MTTR of consolidated virtual machines during crashes. First, we parallelize the process of crash dump and the process of rebooting the crashed VM, by dynamically reclaiming and allocating memory between the crashed VM and the newly spawned VM. Second, we use the virtual machine management layer to introspect the critical data structures of the crashed VM to filter out the dump of unused memory. Finally, we implement disk I/O rate control between core dump and the newly spawned VM according to user-tuned rate control policy to balance the time of crash dump and quality of services in the recovery VM. We have implemented a working prototype, Vicover, that optimizes core dump on system crash of a virtual machine in Xen, to minimize the MTTR of core dump and recovery as a whole. In our experiment on a virtualized TPC-W server, Vicover shortens the downtime caused by crash dump by around 5X. © 2010 ACM.
{fenge}
80051563957	Lifetime privacy and self-destruction of data in the cloud	Data privacy protection is one of the primary concerns and major challenges for online services, such as cloud computing and outsourced data center. The concern is getting serious with the computing practices shifting towards cloud computing. Once user data is uploaded, end users are hard to guarantee that the data is protected and can be completely destructed by any means. Users can only rely on blind trust on the online service vendors. However, the privacy of user data can be compromised in multiple ways including careless operations of cloud administrators, bugs and vulnerabilities inside cloud infrastructure and even malicious cloud vendors. In this research, we seek to provide users with a concrete way to protect or destroy uploaded data. We utilize the technique of trusted computing as the trusted root in the hardware layer, and the hypervisor as the trusted agent in the software layer. The trusted hypervisor is responsible for protecting sensitive user data or destructing them at user's command. Even administrators of the cloud cannot bypass the protection. This paper presents Dissolver, a novel system that keeps the data privacy in the whole life-time and ensures the destruction at the user's command. Performance evaluation shows that the prototype system imposes reasonably low runtime overhead.
{fenge}
80052143494	Limiting cache-based side-channel in multi-tenant cloud using dynamic page coloring	Multi-tenant cloud, which features utility-like computing resources to tenants in a pay-as-you-go style, has been commercially popular for years. As one of the sole purposes of such a cloud is maximizing resource usages to increase its revenue, it usually uses virtualization to consolidate VMs from different and even mutually-malicious tenants atop a powerful physical machine. This, however, also enables a malicious tenant to steal security-critical information such as crypto keys from victims, due to the shared physical resources such as caches. In this paper, we show that stealing crypto keys in a virtualized cloud may be a real threat by evaluating a cache-based side-channel attack against an encryption process. To mitigate such attacks while not notably degrading performance, we propose an approach that leverages dynamic cache coloring: when an application is doing security-sensitive operations, the VMM is notified to swap the associated data to a safe and isolated cache line. This approach may eliminate cache-based side-channel for security-critical operations, yet ensure efficient resource sharing during normal operations. We demonstrate the applicability by illustrating a preliminary implementation based on Xen and its performance overhead. © 2011 IEEE.
{fenge}
80053600172	Dynamic software updating using a relaxed consistency model	Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a "bidirectional write-through" synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applicationsa less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal. © 2011 IEEE.
{fenge}
80053970123	COREMU: A scalable and portable parallel full-system emulator	Copyright © 2011 ACM.This paper presents the open-source COREMU, a scalable and portable parallel emulation framework that decouples the complexity of parallelizing full-system emulators from building a mature sequential one. The key observation is that CPU cores and devices in current (and likely future) multiprocessors are loosely-coupled and communicate through well-defined interfaces. Based on this observation, COREMU emulates multiple cores by creating multiple instances of existing sequential emulators, and uses a thin library layer to handle the inter-core and device communication and synchronization, to maintain a consistent view of system resources. COREMU also incorporates lightweight memory transactions, feedback-directed scheduling, lazy code invalidation and adaptive signal control to provide scalable performance. To make COREMU useful in practice, we also provide some preliminary tools and APIs that can help programmers to diagnose performance problems and (concurrency) bugs. A working prototype, which reuses the widely-used QEMU as the sequential emulator, is with only 2500 lines of code (LOCs) changes to QEMU. It currently supports x64 and ARM platforms, and can emulates up to 255
{fenge}
82655179240	CloudVisor: Retrofitting protection of virtual machines in multi-tenant cloud with nested virtualization	Multi-tenant cloud, which usually leases resources in the form of virtual machines, has been commercially available for years. Unfortunately, with the adoption of commodity virtualized infrastructures, software stacks in typical multi-tenant clouds are non-trivially large and complex, and thus are prone to compromise or abuse from adversaries including the cloud operators, which may lead to leakage of security-sensitive data. In this paper, we propose a transparent, backward-compatible approach that protects the privacy and integrity of customers' virtual machines on commodity virtualized infrastructures, even facing a total compromise of the virtual machine monitor (VMM) and the management VM. The key of our approach is the separation of the resource management from security protection in the virtualization layer. A tiny security monitor is introduced underneath the commodity VMM using nested virtualization and provides protection to the hosted VMs. As a result, our approach allows virtualization software (e.g., VMM, management VM and tools) to handle complex tasks of managing leased VMs for the cloud, without breaking security of users' data inside the VMs. We have implemented a prototype by leveraging commercially-available hardware support for virtualization. The prototype system, called CloudVisor, comprises only 5.5K LOCs and supports the Xen VMM with multiple Linux and Windows as the guest OSes. Performance evaluation shows that CloudVisor incurs moderate slow-down for I/O intensive applications and very small slowdown for other applications. © 2011 ACM.
{fenge}
84863080007	A hierarchical approach to maximizing MapReduce efficiency	MapReduce has been widely recognized for its elastic scalability and fault tolerance, with the efficiency being relatively disregarded, which, however, is equally important in "pay-as-you-go" cloud systems such as Amazon's Elastic Map Reduce. This paper argues that there are multiple levels of data locality and parallelism in typical multicore clusters that affect performance. By characterizing the performance limitations of typical Map Reduce applications on multi-core based Hadoop clusters, we show that current JVM-based runtime (i.e., Task Worker) fails to exploit data locality and task parallelism at single-node level. Based on the study, we extend Hadoop with a hierarchical Map Reduce model and seamlessly integrate an efficient multicore Map Reduce runtime to Hadoop, resulting in a system we called Azwraith. Such a hierarchical scheme enables Map Reduce applications to explore locality and parallelism at both cluster level and single-node level. To reuse data across job boundary, we also extend Azwraith with an effective in-memory cache scheme that significantly reduces networking and disk traffics. Performance evaluation on a small-scale cluster show that, Azwraith, combined with the optimizations, outperforms Hadoop from 1.4x to 3.5x. © 2011 IEEE.
{fenge}
84863150008	Security breaches as PMU deviation: Detecting and identifying security attacks using performance counters	This paper considers and validates the applicability of leveraging pervasively-available performance counters for detecting and reasoning about security breaches. Our key observation is that many security breaches, which typically cause abnormal control flow, usually incur precisely identifiable deviation in performance samples captured by processors. Based on this observation, we implement a prototype system called Eunomia, which is the first non-intrusive system that can detect emerging attacks based on return-oriented programming without any changes to applications (either source or binary code) or special-purpose hardware. Our security evaluation shows that Eunomia can detect some realistic attacks including code-injection attacks, return-to-libc attacks and return-oriented programming attacks on unmodified binaries with relatively low overhead. © 2011 ACM.
{fenge}
84863355694	Swift: A register-based JIT compiler for embedded JVMs	Code quality and compilation speed are two challenges to JIT compilers, while selective compilation is commonly used to trade-off these two issues. Meanwhile, with more and more Java applications running in mobile devices, selective compilation meets many problems. Since these applications always have flat execution profile and short live time, a lightweight JIT technique without losing code quality is extremely needed. However, the overhead of compiling stack-based Java bytecode to heterogeneous register-based machine code is significant in embedded devices. This paper presents a fast and effective JIT technique for mobile devices, building on a register-based Java bytecode format which is more similar to the underlying machine architecture. Through a comprehensive study on the characteristics of Java applications, we observe that virtual registers used by more than 90% Java methods can be directly fulfilled by 11 physical registers. Based on this observation, this paper proposes Swift, a novel JIT compiler on register-based bytecode, which generates native code for RISC machines. After mapping virtual registers to physical registers, the code is generated efficiently by looking up a translation table. And the code quality is guaranteed by the static compiler which is used to generate register-based bytecode. Besides, we design two lightweight optimizations and an efficient code unloader to make Swift more suitable for embedded environment. As the prevalence of Android, a prototype of Swift is implemented upon DEX bytecode which is the official distribution format of Android applications. Swift is evaluated with three benchmarks (SPECjvm98, EmbeddedCaffeineMark3 and JemBench2) on two different ARM SOCs: S3C6410 (armv6) and OMAP3530 (armv7). The results show that Swift achieves a speedup of 3.13 over the best-performing interpreter on the selected benchmarks. Compared with the state-of-the-art JIT compiler in Android, JITC-Droid, Swift achieves a speedup of 1.42. © 2012 ACM.
{fenge}
84863369713	A case for secure and scalable hypervisor using safe language	System virtualization has been a new foundation for system software, which is evidenced in many systems and innovations, as well as numerous commercial successes in desktop, datacenter and cloud. However, with more and more functionality being built into the virtualization layer, the trustworthiness of the hypervisor layer has been a severe issue and should no longer be an "elephant in the room". Further, the advent and popularity of multi-core and many-core platforms, the scalability of the virtualization layer would also be a serious challenge to the scalability of the whole software stack. In this position paper, we argue that it is the time to rethink the design and implementation of the virtualization layer using recent advances in language, compilers and system designs. We point out that the use of safe languages with scalable system design could address the trustworthiness and scalability issues with virtualization. We also argue that applying language innovations to the hypervisor layer avoids the need of an evolutionary path, as it is relatively small in scale and has little backward compatibility issue. © 2012 ACM.
{fenge}
84863390913	A GPU-based high-throughput image retrieval algorithm	With the development of Internet and cloud computing, multimedia data, such as images and videos, has become one of the most common data types being processed. As the scale of multimedia data being still increasing, it is vitally important to efficiently extract useful information from such a huge amount of multimedia data. However, due to the complexity of the core algorithms, multimedia retrieval applications are not only data intensive but also computationally intensive. Therefore, it has been a major challenge to accelerate the processing speed of such applications to satisfy the real-time requirement. As Graphic Processing Unit (GPU) has entered the general-propose computing domain (GPGPU), it has become one of the most popular accelerators for the applications with real-time requirements. In this paper, we parallelize a widely-used image retrieval algorithm called SURF on GPGPU, which is the core algorithm for many video and image retrieval applications. We first analyze the parallelism within SURF to guarantee that there are sufficient tasks being mapped to the large-scale computation resources in GPGPU. We then exploit some inherent GPGPU characteristics, such as 2D memory, to further boost the performance. Finally, we provide some optimization to the cooperation between CPU and GPGPU, which is generally ignored in previous designs. Experimental results show that our parallelization and optimization achieve a throughput of 340.5 frames/s on a NVIDIA GTX295 GPGPU, which is 15X faster than the maximal optimized CPU version. Compared to CUDA SURF, a state-of-the-art parallelization of SURF on GPGPU, our system achieves a speedup by a factor of 2.3X. © 2012 ACM.
{fenge}
84863558271	Transformer: A functional-driven cycle-accurate multicore simulator	Full-system simulators are extremely useful in evaluating design alternatives for multicore. However, state-of-the-art multicore simulators either lack good extensibility due to their tightly-coupled design between functional model (FM) and timing model (TM), or cannot guarantee cycle-accuracy. This paper conducts a comprehensive study on factors affecting cycle-accuracy and uncovers several contributing factors ignored before. Based on the study, we propose a loosely-coupled functional-driven full-system simulator for multicore, namely Transformer. To ensure extensibility and cycle-accuracy, Transformer leverages an architecture-independent interface between FM and TM and uses a lightweight scheme to detect and recover from execution divergence between FM and TM. Based on Transformer, a graduate student only needs to write about 180 lines of code and takes about two months to extend an X86 functional model (QEMU) in Transformer. Moreover, the loosely-coupled design also removes the complex interaction between FM and TM and opens the opportunity to parallelize FM and TM to improve performance. Experimental results show that Transformer achieves an average of 8.4% speedup over GEMS while guaranteeing the cycle-accuracy. A further parallelization between FM and TM leads to 35.3% speedup. © 2012 ACM.
{fenge}
84864140605	Improving dynamic prediction accuracy through multi-level phase analysis	Phase analysis, which classifies the set of execution intervals with similar execution behavior and resource requirements, has been widely used in a variety of dynamic systems, including dynamic cache reconfiguration, prefetching and race detection. While phase granularity has been a major factor to the accuracy of phase prediction, it has not been well investigated yet and most dynamic systems usually adopt a fine-grained prediction scheme. However, such a scheme can only take account of recent local phase information and could be frequently interfered by temporary noises due to instant phase changes, which might notably limit the prediction accuracy. Copyright © 2012 ACM.
{fenge}
84866337751	Improving dynamic prediction accuracy through multi-level phase analysis	Phase analysis, which classifies the set of execution intervals with similar execution behavior and resource requirements, has been widely used in a variety of dynamic systems, including dynamic cache reconfiguration, prefetching and race detection. While phase granularity has been a major factor to the accuracy of phase prediction, it has not been well investigated yet and most dynamic systems usually adopt a fine-grained prediction scheme. However, such a scheme can only take account of recent local phase information and could be frequently interfered by temporary noises due to instant phase changes, which might notably limit the prediction accuracy. In this paper, we make the first investigation on the potential of multi-level phase analysis (MLPA), where different granularity phase analysis are combined together to improve the overall accuracy. The key observation is that a coarse-grained interval, which usually consists of stably-distributed fine-grained intervals, can be accurately identified based on the fine-grained intervals at the beginning of its execution. Based on the observation, we design and implement a MLPA scheme. In such a scheme, a coarse-grained phase is first identified based on the fine-grained intervals at the beginning of its execution. The following fine-grained phases in it are then predicted based on the sequence of fine-grained phases in the coarse-grained phase. Experimental results show such a scheme can notably improve the prediction accuracy. Using Markov finegrained phase predictor as the baseline, MLPA can improve prediction accuracy by 20%, 39% and 29% for next phase, phase change and phase length prediction for SPEC2000 accordingly, yet incur only about 2% time overhead and 40% space overhead (about 360 bytes in total). To demonstrate the effectiveness of MLPA, we apply it to a dynamic cache reconfiguration system which dynamically adjusts the cache size to reduce the power consumption and access time of data cache. Experimental results show that MLPA can further reduce the average cache size by 15% compared to the fine-grained scheme. Copyright © 2012 ACM.
{fenge}
84871104186	Adaptive pipeline parallelism for image feature extraction algorithms	Currently, multimedia data has become one of the major data types processed and transferred on the Internet. With the rapid growth of multimedia data, it is vitally important to find an efficient way to extract useful information from a large amount of data. SIFT and SURF, as the most popular multimedia feature extraction algorithms, have been widely used in many applications. However, the limited processing speed~(about 1.8 and 2.6 images or frames per second for SIFT and SURF respectively on an ordinary CPU) makes it impossible to apply them in many real-world applications with real-time requirements. Therefore, it has become one of the major challenges that how to improve the processing speed of these multimedia feature extraction algorithms. The popularity of multi-core architecture and the increase of computation resources on different platforms provide a new opportunity to accelerate the processing speed of these image feature extraction algorithms. In this paper, we first systematically analyze the major parallel constraints in SIFT and SURF, such as imbalanced workload and indeterminate time distribution. Then, based on these analysis, we design and implement an adaptive pipeline parallel scheme~(AD-PIPE) for both SIFT and SURF to alleviate these limitations. In our scheme, we dynamically check the workload in different pipeline stages and adjust the thread number in different stages to achieve a balanced partition. Experimental results show that our approach is efficient and scalable. It can achieve a speedup of 16.88X and 20.33X respectively for SIFT and SURF on a 16-core machine and a real time processing speed with about 27 and 52 images or frames per second. © 2012 IEEE.
{fenge}
84874389442	A data partition algorithm for arbitrary dimensional Line-Sweep computing	Data partition is a key optimization technique that parallelizes applications on mainstream high performance computing platforms. This technique is composed of two parts: dividing data and assigning data among processors. Line-Sweep computing is one of the most widely-used kernels in a variety of numerical methods and how to parallelize it has been a research hotspot. Until now, most prior researches try to parallelize Line-Sweep algorithm through applying a Multi-Partition, which tries to guarantee every processor the same amount of computation, memory access and communication. However, it cannot achieve the best overall performance because it brings up too much memory access and communication under some conditions. To overcome this problem, we propose a new data partition scheme named Balance-Partition for Line-Sweep algorithm. Our scheme can effectively balance the cost of computation, memory access and communication by reducing unessential constraints. This algorithm contains three main key techniques. Firstly, a performance model is designed. In this model, the performance will only be determined by the way it divides data. And then, we reduce the search space of Balance-Partitions based on this model and find the best way to divide data. Finally, we design a processor assignment function to generate a Balance-Partition. To test the effectiveness of our approach, we applied it to the NPB/SP benchmark and a real world polymer material computing application named LineABC. Experiment results show that when the best Balance-Partition and the best Multi-Partition share the same way to divide data, their performances are almost the same. However, when they divide data differently (which accounts for 38.7% and 37.9% for SP and LineABC each), Balance-Partition outperforms Multi-Partition by 44.45% and 22.15% for SP and LineABC respectively.
{fenge}
84878504055	Swift: A register-based JIT compiler for embedded JVMs	Code quality and compilation speed are two challenges to JIT compilers, while selective compilation is commonly used to trade-off these two issues. Meanwhile, with more and more Java applications running in mobile devices, selective compilation meets many problems. Since these applications always have flat execution profile and short live time, a lightweight JIT technique without losing code quality is extremely needed. However, the overhead of compiling stack-based Java bytecode to heterogeneous register-based machine code is significant in embedded devices. This paper presents a fast and effective JIT technique for mobile devices, building on a register-based Java bytecode format which is more similar to the underlying machine architecture. Through a comprehensive study on the characteristics of Java applications, we observe that virtual registers used by more than 90% Java methods can be directly fulfilled by 11 physical registers. Based on this observation, this paper proposes Swift, a novel JIT compiler on register-based bytecode, which generates native code for RISC machines. After mapping virtual registers to physical registers, the code is generated efficiently by looking up a translation table. And the code quality is guaranteed by the static compiler which is used to generate register-based bytecode. Besides, we design two lightweight optimizations and an efficient code unloader to make Swift more suitable for embedded environment. As the prevalence of Android, a prototype of Swift is implemented upon DEX bytecode which is the official distribution format of Android applications. Swift is evaluated with three benchmarks (SPECjvm98, EmbeddedCaffeineMark3 and JemBench2) on two different ARM SOCs: S3C6410 (armv6) and OMAP3530 (armv7). The results show that Swift achieves a speedup of 3.13 over the best-performing interpreter on the selected benchmarks. Compared with the state-of-the-art JIT compiler in Android, JITC-Droid, Swift achieves a speedup of 1.42. Copyright © 2012 ACM.
{fenge}
84880879632	Defending against VM rollback attack	Recently it became a hot topic to protect VMs from a compromised or even malicious hypervisor. However, most previous systems are vulnerable to rollback attack, since it is hard to distinguish from normal suspend/resume and migration operations that an IaaS platform usually offers. Some of the previous systems simply disable these features to defend rollback attack, while others heavily need user involvement. In this paper, we propose a new solution to make a balance between security and functionality. By securely logging all the suspend/resume and migration operation inside a small trusted computing base, a user can audit the log to check malicious rollback and constrain the operations on the VMs. The solution considers several practical issues including hardware limitations and minimizing user's interaction, and has been implemented on a recent VM protection system. © 2012 IEEE.
{fenge}
84880881458	TinyChecker: Transparent protection of VMs against hypervisor failures with nested virtualization	The increasing amount of resources in a single machine constantly increases the level of server consolidation for virtualization. However, along with the improvement of server efficiency, the dependability of the virtualization layer is not being progressed towards the right direction; instead, the hypervisor level is more vulnerable to diverse failures due to the increasing complexity and scale of the hypervisor layer. This makes tens to hundreds of production VMs in a machine easily risk a single point of failure. This paper tries to mitigate this problem by proposing a technique called TinyChecker, which uses a tiny nested hypervisor to transparently protect guest VMs against failures in the hypervisor layer. TinyChecker is a very small software layer designated for transparent failure detection and recovery, whose reliability can be guaranteed by its small size and possible further formal verification. TinyChecker records all the communication context between VM and hypervisor, protects the critical VM data, detects and recovers the hypervisors among failures. TinyChecker is currently still in an early stage, we report our design consideration and initial evaluation results. © 2012 IEEE.
{fenge}
84883063525	Schedule Processes, not VCPUs	Multiprocessor virtual machines expose underlying abundant computing resources to applications. This, however, also worsens the double scheduling problem where the hypervisor and a guest operating system will both do the CPU scheduling. Prior approaches try to mitigate the semantic gap between the two levels of schedulers by leveraging hints and heuristics, but only work in a few specific cases. This paper argues that instead of scheduling virtual CPUs (vCPUs) in the hypervisor layer, it is more beneficial to dynamically increase and decrease the vCPUs according to available CPU resources when running parallel workloads, while letting the guest operating system to schedule vCPUs to processes. Such a mechanism, which we call vCPU ballooning (VCPU-Bal for short), may avoid many problems inherent in double scheduling. To demonstrate the potential benefit of VCPU-Bal, we simulate the mechanism in both Xen and KVM by assigning an optimal amount of vCPUs for guest VMs. Our evaluation results on a 12-core Intel machine show that VCPU-Bal can achieve a performance speedup from 1.5% to 57.9% on Xen and 8.2% to 63.8% on KVM. Copyright 2013 ACM.
{fenge}
84885629292	Multi-level phase analysis for sampling simulation	Extremely long simulation time of architectural simulators has been a major impediment to their wide applicability. To accelerate architectural simulation, prior researchers have proposed representative sampling simulation to trade small loss of accuracy for notable speed improvement. Generally, they use fine-grained phase analysis to select only a small representative portion of program execution intervals for detailed cycle-accurate simulation, while functionally simulating the remaining portion. However, though phase granularity is one of the most important factors to simulation speed, it has not been well investigated and most prior researches explore a fine-grained scheme. This limits their effectiveness in further improving simulation speed with the requirement of increasingly complex architectural designs and new lengthy benchmarks. In this paper, by analyzing the impact of phase granularity on simulation speed, we observe that coarse-grained phases can better capture the overall program characteristics with a less number of phases and the last representative phase could be classified in a very early program position, leading to fewer execution internals being functionally simulated. By contrast, fine-grained phases usually have much shorter execution intervals and thus the overall detailed simulation time could be reduced. Based on the above observation, we design a multi-level sampling simulation technique that combines both fine-grained and coarsegrained phase analysis for sampling simulation. Such a scheme uses fine-grained simulation points to represent only the selected coarse-grained simulation points instead of the entire program execution, thus it could further reduce both the functional and detailed simulation time. Experimental results using SPEC2000 show such a framework is effective: using the Sim Point method as baseline, it can reduce about 90% functional simulation time and about 50% detailed simulation time. It finally achieves a geometric average speedup of 14.04X over Sim Point with comparable accuracy. © 2013 EDAA.
{fenge}
84889068951	Vetting undesirable behaviors in Android apps with permission use analysis	Android platform adopts permissions to protect sensitive resources from untrusted apps. However, after permissions are granted by users at install time, apps could use these permissions (sensitive resources) with no further restrictions. Thus, recent years have witnessed the explosion of undesirable behaviors in Android apps. An important part in the defense is the accurate analysis of Android apps. However, traditional syscall-based analysis techniques are not well-suited for Android, because they could not capture critical interactions between the application and the Android system. This paper presents VetDroid, a dynamic analysis platform for reconstructing sensitive behaviors in Android apps from a novel permission use perspective. VetDroid features a systematic framework to effectively construct permission use behaviors, i.e., how applications use permissions to access (sensitive) system resources, and how these acquired permission-sensitive resources are further utilized by the application. With permission use behaviors, security analysts can easily examine the internal sensitive behaviors of an app. Using real-world Android malware, we show that VetDroid can clearly reconstruct fine-grained malicious behaviors to ease malware analysis. We further apply VetDroid to 1,249 top free apps in Google Play. VetDroid can assist in finding more information leaks than TaintDroid, a state-of-the-art technique. In addition, we show how we can use VetDroid to analyze fine-grained causes of information leaks that TaintDroid cannot reveal. Finally, we show that VetDroid can help identify subtle vulnerabilities in some (top free) applications otherwise hard to detect. © 2013 ACM.
{fenge}
84894225294	X10-FT: Transparent fault tolerance for APGAS language and runtime	The asynchronous partitioned global address space (APGAS) model is a programming model aiming at unifying programming on multicore and clusters, with good productivity. However, it currently lacks support for fault tolerance (FT) such that a single transient failure may render hours to months of computation useless. In this paper, we thoroughly analyze the feasibility of providing fault tolerance for APGAS model and make the first attempt to add fault tolerance support to an APGAS language called X10. Based on the analysis, we design and implement a fault-tolerance framework called X10-FT that leverages renowned techniques in distributed systems like distributed file systems and Paxos, as well as specific solutions based on the characteristics of the APGAS model to make checkpoints and consensus. This allows the system to transparently handle machine failures at different granularities. Using the features of the APGAS model, we extend the X10 compiler to automatically locate execution points to checkpoint program states without any intervention from programmers. Evaluation using a set of benchmarks shows that the cost for fault tolerance is modest. © 2013 Elsevier B.V. All rights reserved.
{fenge}
84903954261	X10-PM: Transparent and efficient place migration for APGAS language	This paper presents X10-PM, a framework that provides APGAS-based programs with place migration support for flexible and efficient resource management, such as server consolidation and coarse-grained load balancing. X10-PM makes the first attempt to add migration support at the granularity of a place to the X10 programs, which is transparent to the application developers with the help of the X10-PM compiler and runtime. The evaluation results show that the migration overhead is modest. This is mainly because X10-PM chooses global consistent points to ensure consistency of migration, by leveraging the key features of the APGAS model and the X10 language. © 2013 IEEE.
{fenge}
84903987392	Concurrent and consistent virtual machine introspection with hardware transactional memory	Virtual machine introspection, which provides tamperresistant, high-fidelity 'out of the box' monitoring of virtual machines, has many prominent security applications including VM-based intrusion detection, malware analysis and memory forensic analysis. However, prior approaches are either intrusive in stopping the world to avoid race conditions between introspection tools and the guest VM, or providing no guarantee of getting a consistent state of the guest VM. Further, there is currently no effective means for timely examining the VM states in question. In this paper, we propose a novel approach, called TxIntro, which retrofits hardware transactional memory (HTM) for concurrent, timely and consistent introspection of guest VMs. Specifically, TxIntro leverages the strong atomicity of HTM to actively monitor updates to critical kernel data structures. Then TxIntro can mount introspection to timely detect malicious tampering. To avoid fetching inconsistent kernel states for introspection, TxIntro uses HTM to add related synchronization states into the read set of the monitoring core and thus can easily detect potential inflight concurrent kernel updates. We have implemented and evaluated TxIntro based on Xen VMM on a commodity Intel Haswell machine that provides restricted transactional memory (RTM) support. To demonstrate the effectiveness of TxIntro, we implemented a set of kernel rootkit detectors using TxIntro. Evaluation results show that TxIntro is effective in detecting these rootkits, and is efficient in adding negligible performance overhead. © 2014 IEEE.
{fenge}
84904409515	Computation and communication efficient graph processing with distributed immutable view	Cyclops is a new vertex-oriented graph-parallel framework for writing distributed graph analytics. Unlike existing distributed graph computation models, Cyclops retains simplicity and computation-efficiency by synchronously computing over a distributed immutable view, which grants a vertex with read-only access to all its neighboring vertices. The view is provided via readonly replication of vertices for edges spanning machines during a graph cut. Cyclops follows a centralized computation model by assigning a master vertex to update and propagate the value to its replicas unidirectionally in each iteration, which can significantly reduce messages and avoid contention on replicas. Being aware of the pervasively available multicore-based clusters, Cyclops is further extended with a hierarchical processing model, which aggregates messages and replicas in a single multicore machine and transparently decomposes each worker into multiple threads ondemand for different stages of computation. We have implemented Cyclops based on an open-source Pregel clone called Hama. Our evaluation using a set of graph algorithms on an in-house multicore cluster shows that Cyclops outperforms Hama from 2.06X to 8.69X and 5.95X to 23.04X using hash-based and Metis partition algorithms accordingly, due to the elimination of contention on messages and hierarchical optimization for the multicore-based clusters. Cyclops (written in Java) also has comparable performance with PowerGraph (written in C++) despite the language difference, due to the significantly lower number of messages and avoided contention. Copyright © 2014 ACM.
{fenge}
84906872126	Building trusted path on untrusted device drivers for mobile devices	Mobile devices are frequently used as terminals to interact with many security-critical services such as mobile payment and online banking. However, the large client software stack and the continuous proliferation of malware expose such interaction under various threats, including passive attacks like phishing and active ones like direct code manipulation. This paper proposes TrustUI, a new trusted path design for mobile devices that enables secure interaction between end users and services based on ARM's TrustZone technology. TrustUI is built with a combination of key techniques including cooperative randomization of the trusted path and secure delegation of network interaction. With such techniques, TrustUI not only requires no trust of the commodity software stack, but also takes a step further by excluding drivers for user-interacting devices like touch screen from its trusted computing base (TCB). Hence, TrustUI has a much smaller TCB, requires no access to device driver code, and may easily adapt to many devices. A prototype of TrustUI has been implemented on a Samsung Exynos 4412 board and evaluation shows that TrustUI provides strong protection of users interaction. © 2014 ACM.
{fenge}
84906875577	PreCrime to the rescue: Defeating mobile malware one-step ahead	Prior mobile malware defensive means is usually retroactive, which may either lead to high false negatives or can hardly recover systems states from malware activities. PreCrime is a proactive malware detection scheme that detects and stops malware activities from happening. PreCrime creates mirrors of a mobile device in a resource-rich and trusted cloud, which speculatively executes multiple likely user operations concurrently to detect potential tampering and information leakage. Our preliminary evaluation shows that PreCrime introduces small performance overhead on smartphones and feasible delay during speculative execution on the cloud. © 2014 ACM.
{fenge}
84906877762	Bipartite-oriented distributed graph partitioning for big learning	Many machine learning and data mining (MLDM) problems like recommendation, topic modeling and medical diagnosis can be modeled as computing on bipartite graphs. However, most distributed graph-parallel systems are oblivious to the unique characteristics in such graphs and existing online graph partitioning algorithms usually causes excessive replication of vertices as well as significant pressure on network communication. This article identifies the challenges and opportunities of partitioning bipartite graphs for distributed MLDM processing and proposes BiGraph, a set of bipartite-oriented graph partitioning algorithms. BiGraph leverages observations such as the skewed distribution of vertices, discriminated computation load and imbalanced data sizes between the two subsets of vertices to derive a set of optimal graph partition algorithms that result in minimal vertex replication and network communication. BiGraph has been implemented on PowerGraph and is shown to have a performance boost up to 17.75X (from 1.38X) for four typical MLDM algorithms, due to reducing up to 62% vertex replication, and up to 96% network traffic. © 2014 ACM.
{fenge}
84908068169	Permission use analysis for vetting undesirable behaviors in android apps	The android platform adopts permissions to protect sensitive resources from untrusted apps. However, after permissions are granted by users at install time, apps could use these permissions (sensitive resources) with no further restrictions. Thus, recent years have witnessed the explosion of undesirable behaviors in Android apps. An important part in the defense is the accurate analysis of Android apps. However, traditional syscall-based analysis techniques are not well-suited for Android, because they could not capture critical interactions between the application and the Android system. This paper presents VetDroid, a dynamic analysis platform for generally analyzing sensitive behaviors in Android apps from a novel permission use perspective. VetDroid proposes a systematic permission use analysis technique to effectively construct permission use behaviors, i.e., how applications use permissions to access (sensitive) system resources, and how these acquired permission-sensitive resources are further utilized by the application. With permission use behaviors, security analysts can easily examine the internal sensitive behaviors of an app. Using real-world Android malware, we show that VetDroid can clearly reconstruct fine-grained malicious behaviors to ease malware analysis. We further apply VetDroid to 1249 top free apps in Google Play. VetDroid can assist in finding more information leaks than TaintDroid, a state-of-the-art technique. In addition, we show how we can use VetDroid to analyze fine-grained causes of information leaks that TaintDroid cannot reveal. Finally, we show that VetDroid can help to identify subtle vulnerabilities in some (top free) applications otherwise hard to detect.
{fenge}
84921300908	Measuring microarchitectural details of multi- and many-core memory systems through microbenchmarking	As multicore and many-core architectures evolve, their memory systems are becoming increasingly more complex. To bridge the latency and bandwidth gap between the processor and memory, they often use a mix of multilevel private/shared caches that are either blocking or nonblocking and are connected by high-speed network-on-chip. Moreover, they also incorporate hardware and software prefetching and simultaneous multithreading (SMT) to hide memory latency. On such multi- and many-core systems, to incorporate various memory optimization schemes using compiler optimizations and performance tuning techniques, it is crucial to have microarchitectural details of the target memory system. Unfortunately, such details are often unavailable from vendors, especially for newly released processors. In this article, we propose a novel microbenchmarking methodology based on short elapsed-time events (SETEs) to obtain comprehensive memory microarchitectural details in multi- and many-core processors. This approach requires detailed analysis of potential interfering factors that could affect the intended behavior of such memory systems. We lay out effective guidelines to control and mitigate those interfering factors. Taking the impact of SMT into consideration, our proposed methodology not only can measure traditional cache/memory latency and off-chip bandwidth but also can uncover the details of software and hardware prefetching units not attempted in previous studies. Using the newly released Intel Xeon Phi many-core processor (with in-order cores) as an example, we show how we can use a set of microbenchmarks to determine various microarchitectural features of its memory system (many are undocumented from vendors). To demonstrate the portability and validate the correctness of such a methodology, we use the welldocumented Intel Sandy Bridge multicore processor (with out-of-order cores) as another example, where most data are available and can be validated. Moreover, to illustrate the usefulness of the measured data, we do a multistage coordinated data prefetching case study on both Xeon Phi and Sandy Bridge and show that by using the measured data, we can achieve 1.3X and 1.08X performance speedup, respectively, compared to the state-of-the-art Intel ICC compiler. We believe that these measurements also provide useful insights into memory optimization, analysis, and modeling of such multicore and many-core architectures.
