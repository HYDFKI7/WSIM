{fenge}
84937501112	A Stereo-Vision-Assisted model for depth map super-resolution	In this paper, we propose a novel Stereo-Vision-Assisted (SVA) model for depth map super-resolution. Given a low-resolution depth map as input, we investigate to enhance its resolution or quality using the registered and potentially highresolution color stereo image pair. First, based on the mutual benefits between raw depth map and features of highresolution color image, we model the relationship with two constraint terms of local and non-local priors which sufficiently explore their complementary nature. Then by considering reliable disparity pixels calculated from stereo matching algorithm, we formulate a stereo disparity regularization term to further reinforce the preservation of fine depth detail. In addition, we employ an efficient iterative algorithm to optimize the objective function. Experimental results demonstrate that our approach can achieve high-quality depth map in terms of both spatial resolution and depth precision.
{fenge}
37848999908	Building a comprehensive ontology to refine video concept detection	Recent research has discovered that leveraging ontology is an effective way to facilitate semantic video concept detection. As an explicit knowledge representation, a formal ontology definition usually consists of a lexicon, properties, and relations. In this paper, we present a comprehensive representation scheme for video semantic ontology in which all the three components are well studied. Specifically, we leverage LSCOM to construct the concept lexicon, describe concept property as the weights of different modalities which are obtained manually or by data-driven approach, and model two types of concept relations (i.e., pairwise concept correlation and hierarchical relation). In contrast with most existing ontologies which are only focused on one or two components for domain-specific videos, the proposed ontology is more comprehensive and general. To validate the effectiveness of this ontology, we further apply it to video concept detection. The experiments on TRECVID 2005 corpus have demonstrated a superior performance compared to existing key approaches to video concept detection. Copyright 2007 ACM.
{fenge}
37849021018	Refining video annotation by exploiting pairwise concurrent relation	Video annotation is a promising and essential step for content-based video search and retrieval. Most of the state-of-the-art video annotation approaches detect multiple semantic concepts in an isolated manner, which neglect the fact that video concepts are usually correlated in semantic nature. In this paper, we propose to refine video annotation by leveraging the pairwise concurrent relation among video concepts. Such concurrent relation is explicitly modeled by a concurrent matrix and then a propagation strategy is adopted to refine the annotations. Through spreading the scores of all related concepts to each other iteratively, the detection results approach stable and optimal. In contrast with existing concept fusion methods, the proposed approach is computationally more efficient and easy to implement, not requiring to construct any contextual model. Furthermore, we show its intuitive connection with the PageRank algorithm. We conduct the experiments on TRECVID 2005 corpus and report superior performance compared to existing key approaches. Copyright 2007 ACM.
{fenge}
80052952189	Less is more: Efficient 3-D object retrieval with query view selection	The explosively increasing 3-D objects make their efficient retrieval technology highly desired. Extensive research efforts have been dedicated to view-based 3-D object retrieval for its advantage of using 2-D views to represent 3-D objects. In this paradigm, typically the retrieval is accomplished by matching the views of the query object with the objects in database. However, using all the query views may not only introduce difficulty in rapid retrieval but also degrade retrieval accuracy when there is a mismatch between the query views and the object views in the database. In this work, we propose an interactive 3-D object retrieval scheme. Given a set of query views, we first perform clustering to obtain several candidates. We then incrementally select query views for object matching: in each round of relevance feedback, we only add the query view that is judged to be the most informative one based on the labeling information. In addition, we also propose an efficient approach to learn a distance metric for the newly selected query view and the weights for combining all of the selected query views. We conduct experiments on the National Taiwan University 3D Model database, ETH 3D object collection, and Shape Retrieval Content of Non-Rigid 3D Model, and results demonstrated that our approach not only significantly speeds up the retrieval process but also achieves encouraging retrieval performance. © 2006 IEEE.
{fenge}
84877290219	Detecting group activities with multi-camera context	Human group activities detection in multi-camera CCTV surveillance videos is a pressing demand on smart surveillance. Previous works on this topic are mainly based on camera topology inference that is hard to apply to real-world unconstrained surveillance videos. In this paper, we propose a new approach for multi-camera group activities detection. Our approach simultaneously exploits intra-camera and inter-camera contexts without topology inference. Specifically, a discriminative graphical model with hidden variables is developed. The intra-camera and inter-camera contexts are characterized by the structure of hidden variables. By automatically optimizing the structure, the contexts are effectively explored. Furthermore, we propose a new spatiotemporal feature, named vigilant area (VA), to characterize the quantity and appearance of the motion in an area. This feature is effective for group activity representation and is easy to extract from a dynamic and crowded scene. We evaluate the proposed VA feature and discriminative graphical model extensively on two real-world multi-camera surveillance video data sets, including a public corpus consisting of 2.5 h of videos and a 468-h video collection, which, to the best of our knowledge, is the largest video collection ever used in human activity detection. The experimental results demonstrate the effectiveness of our approach. © 1991-2012 IEEE.
{fenge}
84887236868	Automatic image tag ranking scheme based on visual content semantic relatedness	An automatic image tag ranking scheme was proposed based on visual content semantic relatedness, which sorted the tags of the community-contributed images according to the semantic relatedness between tags and image contents. Firstly, the semantic relatedness calculation between tag and image content was formulated as a probabilistic problem based on Bayes' theorem. Then, multiple visual features were fused to obtain reasonable probability evaluation for tag-image content relatedness in different semantic themes. This method has high scalability. Extensive experiments were conducted over a dataset consisting of 149 915 images downloaded from Flickr and experimental results demonstrate the effectiveness of the proposed method.
{fenge}
84907908987	Exploiting web images for semantic video indexing via robust sample-specific loss	Semantic video indexing, also known as video annotation or video concept detection in literatures, has been attracting significant attention in recent years. Due to deficiency of labeled training videos, most of the existing approaches can hardly achieve satisfactory performance. In this paper, we propose a novel semantic video indexing approach, which exploits the abundant user-tagged Web images to help learn robust semantic video indexing classifiers. The following two major challenges are well studied: 1) noisy Web images with imprecise and/or incomplete tags; and 2) domain difference between images and videos. Specifically, we first apply a non-parametric approach to estimate the probabilities of images being correctly tagged as confidence scores. We then develop a robust transfer video indexing (RTVI) model to learn reliable classifiers from a limited number of training videos together with the abundance of user-tagged images. The RTVI model is equipped with a novel sample-specific robust loss function, which employs the confidence score of a Web image as prior knowledge to suppress the influence and control the contribution of this image in the learning process. Meanwhile, the RTVI model discovers an optimal kernel space, in which the mismatch between images and videos is minimized for tackling the domain difference problem. Besides, we devise an iterative algorithm to effectively optimize the proposed RTVI model and a theoretical analysis on the convergence of the proposed algorithm is provided as well. Extensive experiments on various real-world multimedia collections demonstrate the effectiveness of the proposed robust semantic video indexing approach.
{fenge}
51949083216	Joint multi-label multi-instance learning for image classification	In real world, an image is usually associated with multiple labels which are characterized by different regions in the image. Thus image classification is naturally posed as both a multi-label learning and multi-instance learning problem. Different from existing research which has considered these two problems separately, we propose an integrated multilabel multi-instance learning (MLMIL) approach based on hidden conditional random fields (HCRFs), which simultaneously captures both the connections between semantic labels and regions, and the correlations among the labels in a single formulation. We apply this MLMIL framework to image classification and report superior performance compared to key existing approaches over the MSR Cambridge (MSRC) and Corel data sets. ©2008 IEEE.
{fenge}
51949097138	A joint appearance-spatial distance for kernel-based image categorization	The goal of image categorization is to classify a collection of unlabeled images into a set of predefined classes to support semantic-level image retrieval. The distance measures used in most existing approaches either ignored the spatial structures or used them in a separate step. As a result, these distance measures achieved only limited success. To address these difficulties, in this paper, we propose a new distance measure that integrates joint appearance-spatial image features. Such a distance measure is computed as an upper bound of an information-theoretic discrimination, and can be computed efficiently in a recursive formulation that scales well to image size. In addition, the upper bound approximation can be further tightened via adaption learning from a universal reference model. Extensive experiments on two widely-used data sets show that the proposed approach significantly outperforms the state-of-the-art approaches. ©2008 IEEE.
{fenge}
54049089712	Unbiased active learning for image retrieval	In transductive active learning, after selecting the samples for labeling using existing sample selection strategy such as close-to-boundary, the constructed labeled set will be under a different distribution from the unlabeled set, which violates the i.i.d assumption of existing classifier. In this paper, by explicitly considering the distribution difference, we propose an algorithm called unbiased active learning. In such algorithm, the distribution difference, so-called sample selection bias, is not only considered into the classifier, but also incorporated into the sample selection process for introducing a better sample selection strategy. We apply the proposed method to image retrieval and the experimental results show that our unbiased active learning algorithm outperforms existing approaches. © 2008 IEEE.
{fenge}
54049133730	Optimized video scene segmentation	In this paper, we propose an optimized video scene segmentation approach with considering both content coherence and temporally contextual dissimilarity. First, a chain structure is constructed by connecting temporally adjacent shots to represent a video. Then the chain is partitioned such that the content within a chain segment is coherent enough and the contextual similarity of temporally adjacent chain segments is small enough. This task is formulated as a ratio function of content coherence and contextual similarity. Finally, we present an effective and efficient hierarchical chain partitioning approach to find the optimal scene segmentation. Experimental results on a set of home videos and feature movies demonstrate the superiority of the proposed approach over several existing key approaches. © 2008 IEEE.
{fenge}
60649094696	Graph-based semi-supervised learning with multiple labels	Conventional graph-based semi-supervised learning methods predominantly focus on single label problem. However, it is more popular in real-world applications that an example is associated with multiple labels simultaneously. In this paper, we propose a novel graph-based learning framework in the setting of semi-supervised learning with multiple labels. This framework is characterized by simultaneously exploiting the inherent correlations among multiple labels and the label consistency over the graph. Based on the proposed framework, we further develop two novel graph-based algorithms. We apply the proposed methods to video concept detection over TRECVID 2006 corpus and report superior performance compared to the state-of-the-art graph-based approaches and the representative semi-supervised multi-label learning methods. © 2008 Elsevier Inc. All rights reserved.
{fenge}
72449178945	Visual query suggestion	Query suggestion is an effective approach to improve the usability of image search. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This paper proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. It provides a more effective query interface to formulate an intent-specific query by joint text and image suggestions. We show that VQS is able to more precisely and more quickly help users specify and deliver their search intents. When a user submits a text query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. If the user selects one of the suggestions, the corresponding keyword will be added to complement the initial text query as the new text query, while the image collection will be formulated as the visual query. VQS then performs image search based on the new text query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS with three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and search performance. Copyright 2009 ACM.
{fenge}
77249161192	Which tags are related to visual content?	Photo sharing services allow user to share one's photos on the Web, as well as to annotate the photos with tags. Such web sites currently cumulate large volume of images and abundant tags. These resources have brought forth a lot of new research topics. In this paper, we propose to automatically identify which tags are related to the content of images, i.e. which tags are content-related. A data-driven method is developed to investigate the relatedness between a tag and the image visual content. We conduct extensive experiments over a dataset of 149,915 Flickr images. The experimental results demonstrate the effectiveness of our method. © 2010 Springer-Verlag Berlin Heidelberg.
{fenge}
77952581585	Joint learning of labels and distance metric	Machine learning algorithms frequently suffer from the insufficiency of training data and the usage of inappropriate distance metric. In this paper, we propose a joint learning of labels and distance metric (JLLDM) approach, which is able to simultaneously address the two difficulties. In comparison with the existing semi-supervised learning and distance metric learning methods that focus only on label prediction or distance metric construction, the JLLDM algorithm optimizes the labels of unlabeled samples and a Mahalanobis distance metric in a unified scheme. The advantage of JLLDM is multifold: 1) the problem of training data insufficiency can be tackled; 2) a good distance metric can be constructed with only very few training samples; and 3) no radius parameter is needed since the algorithm automatically determines the scale of the metric. Extensive experiments are conducted to compare the JLLDM approach with different semi-supervised learning and distance metric learning methods, and empirical results demonstrate its effectiveness. © 2006 IEEE.
{fenge}
77955855951	Utilizing related samples to learn complex queries in interactive concept-based video search	One of the main challenges in interactive concept-based video search is the insufficient relevant sample problem, especially for queries with complex semantics. To address this problem, in this paper, we propose to utilize "related samples" to learn the complex queries. The "related samples" refer to those video segments that are irrelevant to the query but relevant to some of the related concepts of the query. Different from the relevant samples which may be rare, the related samples are usually sufficient and easy to find in the search result list. Specifically, we learn a detector for the query by simultaneously leveraging the related concept detectors, as well as users' feedbacks including relevant, irrelevant, and related samples. The query detector is then employed to predict the presence of the query in new video segments. As a result, new search results can be obtained according to the query presence. Furthermore, our approach is developed based on incremental learning technique. Thus, the query detector can be efficiently updated in each feedback iteration. We conduct experiments on two real-world video datasets: TRECVID 2008 and Youtube datasets. The experimental results demonstrate the effectiveness and efficiency of the proposed approach. Copyright © 2010 ACM.
{fenge}
77956224771	Robust distance metric learning with auxiliary knowledge	Most of the existing metric learning methods are accomplished by exploiting pairwise constraints over the labeled data and frequently suffer from the insufficiency of training examples. To learn a robust distance metric from few labeled examples, prior knowledge from unlabeled examples as well as the metrics previously derived from auxiliary data sets can be useful. In this paper, we propose to leverage such auxiliary knowledge to assist distance metric learning, which is formulated following the regularized loss minimization principle. Two algorithms are derived on the basis of manifold regularization and log-determinant divergence regularization technique, respectively, which can simultaneously exploit label information (i.e., the pairwise constraints over labeled data), unlabeled examples, and the metrics derived from auxiliary data sets. The proposed methods directly manipulate the auxiliary metrics and require no raw examples from the auxiliary data sets, which make them efficient and flexible. We conduct extensive evaluations to compare our approaches with a number of competing approaches on face recognition task. The experimental results show that our approaches can derive reliable distance metrics from limited training examples and thus are superior in terms of accuracy and labeling efforts.
{fenge}
77956354481	Visual Query Suggestion: Towards capturing user intent in internet image search	Query suggestion is an effective approach to bridge the Intention Gap between the users' search intents and queries. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This article proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. VQS provides a more effective query interface to help users to precisely express their search intents by joint text and image suggestions. When a user submits a textual query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. Once the user selects one of the suggestions, the corresponding keyword will be added to complement the initial query as the new textual query, while the image collection will be used as the visual query to further represent the search intent. VQS then performs image search based on the new textual query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS against three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and the search performance. © 2010 ACM.
{fenge}
78349263771	Evaluation of histogram based interest point detector in web image classification and search	Local image feature has received increasing attention in various applications, such as web image classification and search. The process of local feature extraction consists of two main steps: interest point detection and local feature description. A wealth of interest point detectors have been proposed in last decades. Most of them measure pixel-wise differences in image intensity or color. Recently, a new type of interest point detector has been developed, which incorporates histogram-based representation into the process of interest point detection. In this paper, we evaluate this histogram-based interest point detector in the context of web image classification and search, as well as compare it against typical pixel-based detectors and heuristic grid-based detector. The evaluation is performed on two web image datasets: NUS-WIDE-OBJECT and MIRFLICKR-25000 datasets. The experimental results demonstrate that the histogram-based interest point detector outperforms the pixelbased and grid-based detectors in both web image classification and search tasks. © 2010 IEEE.
{fenge}
78751662152	Semi-automatic Flickr group suggestion	Flickr groups are self-organized communities to share photos and conversations with common interest and have gained massive popularity. Users in Flickr have to manually assign each image to the appropriated group. Manual assignment requires users to be familiar with existing images in each group and it is intractable and tedious. Therefore it prohibits users from exploiting the relevant groups. For solution to the problem, group suggestion has attracted increasing attention recently, which aims to suggest groups to user for a specific image. Existing works pose group suggestion as the automatic group prediction problem with a purpose of predicting the groups of each image automatically. Despite of dramatic progress in automatic group prediction, the prediction results are still not accurate enough. In this paper, we propose a semi-automatic group suggestion approach with Human-in-the-Loop. Given a user's image collection, we employ the pre-built group classifiers to predict the group of each image. These predictions are used as the initial group suggestions. We then select a small number of representative images from user's collection and ask user to assign the groups of them. Once obtaining user's feedbacks on the representative images, we infer the groups of remaining images through group propagation over multiple sparse graphs among the images. We conduct experiment on 15 Flickr groups with 127,500 images. The experimental results demonstrate the proposed framework is able to provide accurate group suggestions with quite a small amount of user effort. © 2011 Springer-Verlag Berlin Heidelberg.
{fenge}
79959761696	ShotTagger: Tag location for internet videos	Social video sharing websites allow users to annotate videos with descriptive keywords called tags, which greatly facilitate video search and browsing. However, many tags only describe part of the video content, without any temporal indication on when the tag actually appears. Currently, there is very little research on automatically assigning tags to shot-level segments of a video. In this paper, we leverage user's tags as a source to analyze the content within the video and develop a novel system named ShotTagger to assign tags at the shot level. There are two steps to accomplish the location of tags at shot level. The first is to estimate the distribution of tags within the video, which is based on a multiple instance learning framework. The second is to perform the semantic correlation of a tag with other tags in a video in an optimization framework and impose the temporal smoothness across adjacent video shots to refine the tagging results at shot level. We present different applications to demonstrate the usefulness of the tag location scheme in searching, and browsing of videos. A series of experiments conducted on a set of Youtube videos has demonstrated the feasibility and effectiveness of our approach. © 2011 ACM.
{fenge}
80052106467	Optimizing multimodal reranking for web image search	In this poster, we introduce a web image search reranking approach with exploring multiple modalities. Different from the conventional methods that build graph with one feature set for reranking, our approach integrates multiple feature sets that describe visual content from different aspects. We simultaneously integrate the learning of relevance scores, the weighting of different feature sets, the distance metric and the scaling for each feature set into a unified scheme. Experimental results on a large data set that contains more than 1,100 queries and 1 million images demonstrate the effectiveness of our approach.
{fenge}
80155127338	Matching Content-based Saliency regions for partial-duplicate image retrieval	In traditional partial-duplicate image retrieval, images are commonly represented using the Bag-of-Visual-Words (BOV) model built from image local features, such as SIFT. Actually, there is only a small similar portion between partial-duplicate images so that such representation on the whole image is not adequate for the partial-duplicate image retrieval task. In this paper, we propose a novel perspective to retrieval partial-duplicate images with Contented-based Saliency Region (CSR). CSRs are such sub-regions with abundant visual content and high visual attention in the image. The content of CSR is represented with the BOV model while saliency analysis is employed to ensure the high visual attention of CSR. Each CSR is regarded as an independent unit to be retrieved in the dataset. To effectively retrieve the CSRs, we design a relative saliency ordering constraint, which captures a weak saliency relative layout among interest points in the CSR. Comparison experiments with four state-of-the-art methods on the standard partial-duplicate image dataset clearly verify the effectiveness of our scheme. Further, our approach can provide a more diverse retrieval result, which facilitates the interaction of portable-device users. © 2011 IEEE.
{fenge}
82055197368	Utilizing related samples to enhance interactive concept-based video search	One of the main challenges in interactive concept-based video search is the problem of insufficient relevant samples, especially for queries with complex semantics. In this paper, related samples are exploited to enhance interactive video search. The related samples refer to those video segments that are relevant to part of the query rather than the entire query. Compared to the relevant samples which may be rare, the related samples are usually plentiful and easy to find in search results. Generally, the related samples are visually similar and temporally neighboring to the relevant samples. Based on these two characters, we develop a visual ranking model that simultaneously exploits the relevant, related, and irrelevant samples, as well as a temporal ranking model to leverage the temporal relationship between related and relevant samples. An adaptive fusion method is then proposed to optimally explore these two ranking models to generate search results. We conduct extensive experiments on two real-world video datasets: TRECVID 2008 and YouTube datasets. As the experimental results show, our approach achieves at least 96% and 167% performance improvements against the state-of-the-art approaches on the TRECVID 2008 and YouTube datasets, respectively. © 2006 IEEE.
{fenge}
84455202780	Learning"Verb-object" Concepts for semantic image annotation	In real-world image understanding and retrieval applications, there exists a large number of images containing"verb-object" semantic. The most existing image annotation approaches which mainly focus on annotating images with "object" concepts may not well describe the image semantics. In this paper, we propose a novel image annotation approach by learning "verb-object" concepts. The "verb-object" concept learning method is developed based on the assumption that the classifiers of the "verb-object" concepts which contain the same object usually share a common structure. We formulate each "verb-object" concept classifier as a combination of a private part and a common part shared by all the "verb-object" concepts containing the same object. These classifiers are learned simultaneously through a joint optimization process. Experiments on a Web image data set containing 22,812 images with 28 concepts demonstrate that the proposed approach can achieve promising performance compared to the baseline method. Copyright 2011 ACM.
{fenge}
84455204985	Integrating rich information for video recommendation with multi-task rank aggregation	Video recommendation is an important approach for helping people to access interesting videos. In this paper, we propose a scheme to integrate rich information for video recommendation. We regard video recommendation as a ranking problem and generate multiple ranking lists by exploring different information sources. A multitask rank aggregation approach is proposed to integrate the ranking lists for different users in a joint manner. Our scheme is flexible and can easily incorporate other methods by adding their generated ranking lists into our multi-task learning algorithm. We conduct experiments with 76 users and more than 10, 000 videos. The results demonstrate the feasibility and effectiveness of our approach. Copyright 2011 ACM.
{fenge}
84455206053	Query expansion by spatial co-occurrence for image retrieval	The well-known bag-of-features (BoF) model is widely utilized for large scale image retrieval. However, BoF model lacks the spatial information of visual words, which is informative for local features to build up meaningful visual patches. To compensate for the spatial information loss, in this paper, we propose a novel query expansion method called Spatial Co-occurrence Query Expansion (SCQE), by utilizing the spatial co-occurrence information of visual words mined from the database images to boost the retrieval performance. In offline phase, for each visual word in the vocabulary, we treat the visual words that are frequently co-occurred with it in the database images as neighbors, base on which a spatial co-occurrence graph is built. In online phase, a query image can be expanded with some spatial co-occurred but unseen visual words according to the spatial co-occurrence graph, and the retrieval performance can be improved by expanding these visual words appropriately. Experimental results demonstrate that, SCQE achieves promising improvements over the typical BoF baseline on two datasets comprising 5K and 505K images respectively. Copyright 2011 ACM.
{fenge}
84455206057	Difficulty guided image retrieval using linear multiview embedding	Existing image retrieval systems suffer from a radical performance variance for different queries. The bad initial search results for "difficult" queries may greatly degrade the performance of their subsequent refinements, especially the refinement that utilizes the information mined from the search results, e.g., pseudo relevance feedback based reranking. In this paper, we tackle this problem by proposing a query difficulty guided image retrieval system, which selectively performs reranking according to the estimated query difficulty. To improve the performance of both reranking and difficulty estimation, we apply multiview embedding (ME) to images represented by multiple different features for integrating a joint subspace by preserving the neighborhood information in each feature space. However, existing ME approaches suffer from both "out of sample" and huge computational cost problems, and cannot be applied to online reranking or offline large-scale data processing for practical image retrieval systems. Therefore, we propose a linear multiview embedding algorithm which learns a linear transformation from a small set of data and can effectively infer the subspace features of new data. Empirical evaluations on both Oxford and 500K ImageNet datasets suggest the effectiveness of the proposed difficulty guided retrieval system with LME. Copyright 2011 ACM.
{fenge}
84455161694	Learning concept bundles for video search with complex queries	Classifiers for primitive visual concepts like "car", "sky" have been well developed and widely used to support video search on simple queries. However, it is usually ineffective for complex queries like "one or more people at a table or desk with a computer visible", as they carry semantics far more complex and different from simply aggregating the meanings of their constituent primitive concepts. To facilitate video search of complex queries, we propose a higher-level semantic descriptor named "concept bundle", which integrates multiple primitive concepts, such as "(soccer, fighting)", "(lion, hunting, zebra)" etc, to describe the visual representation of the complex semantics. The proposed approach first automatically selects informative concept bundles. It then builds a novel concept bundle classifier based on multi-task learning by exploiting the relatedness between concept bundle and its primitive concepts. To model a complex query, it proposes an optimal selection strategy to select related primitive concepts and concept bundles by considering both their classifier performance and semantic relatedness with respect to the query. The final results are generated by fusing the individual results from these selected primitive concepts and concept bundles. Extensive experiments are conducted on two video datasets: TRECVID 2008 and YouTube datasets. The experimental results indicate that: (a) our concept bundle learning approach outperforms the state-of-the-art methods by at least 19% and 29% on TRECVID 2008 and YouTube datasets, respectively; and (b) the use of concept bundles can improve the search performance for complex queries by at least 37.5% on TRECVID 2008 and 52% on YouTube datasets. © 2011 ACM.
{fenge}
84859075572	Semantic-gap-oriented active learning for multilabel image annotation	User interaction is an effective way to handle the semantic gap problem in image annotation. To minimize user effort in the interactions, many active learning methods were proposed. These methods treat the semantic concepts individually or correlatively. However, they still neglect the key motivation of user feedback: to tackle the semantic gap. The size of the semantic gap of each concept is an important factor that affects the performance of user feedback. User should pay more efforts to the concepts with large semantic gaps, and vice versa. In this paper, we propose a semantic-gap-oriented active learning method, which incorporates the semantic gap measure into the information-minimization- based sample selection strategy. The basic learning model used in the active learning framework is an extended multilabel version of the sparse-graph-based semisupervised learning method that incorporates the semantic correlation. Extensive experiments conducted on two benchmark image data sets demonstrated the importance of bringing the semantic gap measure into the active learning process. © 2011 IEEE.
{fenge}
84862815270	K-Partite graph reinforcement and its application in multimedia information retrieval	In many example-based information retrieval tasks, example query actually contains multiple sub-queries. For example, in 3D object retrieval, the query is an object described by multiple views. In content-based video retrieval, the query is a video clip that contains multiple frames. Without prior knowledge, the most intuitive approach is to treat the sub-queries equally without difference. In this paper, we propose a k-partite graph reinforcement approach to fuse these sub-queries based on the to-be-retrieved database. The approach first collects the top retrieved results. These results are regarded as pseudo-relevant samples and then a k-partite graph reinforcement is performed on these samples and the query. In the reinforcement process, the weights of the sub-queries are updated by an iterative process. We present experiments on 3D object retrieval and content-based video clip retrieval, and the results demonstrate that our method effectively boosts retrieval performance. © 2012 Elsevier Inc. All rights reserved.
{fenge}
84862909120	Interactive video indexing with statistical active learning	Video indexing, also called video concept detection, has attracted increasing attentions from both academia and industry. To reduce human labeling cost, active learning has been introduced to video indexing recently. In this paper, we propose a novel active learning approach based on the optimum experimental design criteria in statistics. Different from existing optimum experimental design, our approach simultaneously exploits sample's local structure, and sample relevance, density, and diversity information, as well as makes use of labeled and unlabeled data. Specifically, we develop a local learning model to exploit the local structure of each sample. Our assumption is that for each sample, its label can be well estimated based on its neighbors. By globally aligning the local models from all the samples, we obtain a local learning regularizer, based on which a local learning regularized least square model is proposed. Finally, a unified sample selection approach is developed for interactive video indexing, which takes into account the sample relevance, density and diversity information, and sample efficacy in minimizing the parameter variance of the proposed local learning regularized least square model. We compare the performance between our approach and the state-of-the-art approaches on the TREC video retrieval evaluation (TRECVID) benchmark. We report superior performance from the proposed approach. © 2006 IEEE.
{fenge}
84862918067	Parallel lasso for large-scale video concept detection	Existing video concept detectors are generally built upon the kernel based machine learning techniques, e.g., support vector machines, regularized least squares, and logistic regression, just to name a few. However, in order to build robust detectors, the learning process suffers from the scalability issues including the high-dimensional multi-modality visual features and the large-scale keyframe examples. In this paper, we propose parallel lasso (Plasso) by introducing the parallel distributed computation to significantly improve the scalability of lasso (the regularized least squares). We apply the parallel incomplete Cholesky factorization to approximate the covariance statistics in the preprocess step, and the parallel primal-dual interior-point method with the Sherman-Morrison-Woodbury formula to optimize the model parameters. For a dataset with samples in a -dimensional space, compared with lasso, Plasso significantly reduces complexities from the original for computational time and for storage space to and respectively, if the system has $m$ processors and the reduced dimension is much smaller than the original dimension. Furthermore, we develop the kernel extension of the proposed linear algorithm with the sample reweighting schema, and we can achieve similar time and space complexity improvements [time complexity from to and the space complexity from to for a dataset with training examples]. Experimental results on TRECVID video concept detection challenges suggest that the proposed method can obtain significant time and space savings for training effective detectors with limited communication overhead. © 2006 IEEE.
{fenge}
84863771833	Oracle in image search: A content-based approach to performance prediction	This article studies a novel problem in image search. Given a text query and the image ranking list returned by an image search system, we propose an approach to automatically predict the search performance. We demonstrate that, in order to estimate the mathematical expectations of Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG), we only need to predict the relevance probability of each image. We accomplish the task with a query-adaptive graph-based learning based on the images' ranking order and visual content. We validate our approach with a large-scale dataset that contains the image search results of 1, 165 queries from 4 popular image search engines. Empirical studies demonstrate that our approach is able to generate predictions that are highly correlated with the real search performance. Based on the proposed image search performance prediction scheme, we introduce three applications: image metasearch, multilingual image search, and Boolean image search. Comprehensive experiments are conducted to validate our approach. © 2012 ACM.
{fenge}
84863824684	Active learning for social image retrieval using Locally Regressive Optimal Design	In this paper, we propose a novel active learning algorithm, called Locally Regressive Optimal Design (LROD), to improve the effectiveness of relevance feedback-based social image retrieval. Our algorithm assumes that for each data point, the label values of both this data point and its neighbors can be well estimated using a locally regressive function. Specifically, we adopt a local linear regression model to predict the label value of each data point in a local patch. The regularized local model predication error of the local patch is defined as our local loss function. Then, a unified objective function is proposed to minimize the summation of these local loss functions over all the data points, so that an optimal predicated label value can be assigned to each data point. Finally, we embed it into a semi-supervised learning framework to construct the final objective function. Experiment results on MSRA-MM2.0 database demonstrate the efficiency and effectiveness of the proposed algorithm for relevance feedback-based social image retrieval. © 2012 Elsevier B.V..
{fenge}
84863832423	A comprehensive representation scheme for video semantic ontology and its applications in semantic concept detection	Recent research has discovered that leveraging ontology is an effective way to facilitate semantic video concept detection. As an explicit knowledge representation, a formal ontology definition usually consists of a lexicon, properties, and relations. In this paper, we present a comprehensive representation scheme for video semantic ontology in which all the three components are well studied. Specifically, we leverage LSCOM to construct the concept lexicon, describe concept property as the weights of different modalities which are obtained manually or by data-driven approach, and model two types of concept relations (i.e., pairwise correlation and hierarchical relation). In contrast with most existing ontologies which are only focused on one or two components for domain-specific videos, the proposed ontology is more comprehensive and general. To validate the effectiveness of this ontology, we further apply it to video concept detection. The experiments on TRECVID 2005 corpus have demonstrated a superior performance compared to existing key approaches to video concept detection. © 2012 Elsevier B.V..
{fenge}
84864124214	Event driven web video summarization by tag localization and key-shot identification	With the explosive growth of web videos on the Internet, it becomes challenging to efficiently browse hundreds or even thousands of videos. When searching an event query, users are often bewildered by the vast quantity of web videos returned by search engines. Exploring such results will be time consuming and it will also degrade user experience. In this paper, we present an approach for event driven web video summarization by tag localization and key-shot mining. We first localize the tags that are associated with each video into its shots. Then, we estimate the relevance of the shots with respect to the event query by matching the shot-level tags with the query. After that, we identify a set of key-shots from the shots that have high relevance scores by exploring the repeated occurrence characteristic of key sub-events. Following the scheme in and , we provide two types of summaries, i.e., threaded video skimming and visual-textual storyboard. Experiments are conducted on a corpus that contains 60 queries and more than 10000 web videos. The evaluation demonstrates the effectiveness of the proposed approach. © 2012 IEEE.
{fenge}
84870534552	Difficulty guided image retrieval using linear multiple feature embedding	Existing image retrieval systems suffer from a performance variance for different queries. Severe performance variance may greatly degrade the effectiveness of the subsequent query-dependent ranking optimization algorithms, especially those that utilize the information mined from the initial search results. In this paper, we tackle this problem by proposing a query difficulty guided image retrieval system, which can predict the queries' ranking performance in terms of their difficulties and adaptively apply ranking optimization approaches. We estimate the query difficulty by comprehensively exploring the information residing in the query image, the retrieval results, and the target database. To handle the high-dimensional and multi-model image features in the large-scale image retrieval setting, we propose a linear multiple feature embedding algorithm which learns a linear transformation from a small set of data by integrating a joint subspace in which the neighborhood information is preserved. The transformation can be effectively and efficiently used to infer the subspace features of the newly observed data in the online setting. We prove the significance of query difficulty to image retrieval by applying it to guide the conduction of three retrieval refinement applications, i.e., reranking, federated search, and query suggestion. Thorough empirical studies on three datasets suggest the effectiveness and scalability of the proposed image query difficulty estimation algorithm, as well as the promising of the image difficulty guided retrieval system. © 1999-2012 IEEE.
{fenge}
84871412515	Combining SIFT and global features for web image classification	Nowadays, web images are rapidly increasing with the development of internet technology. This situation leads to the difficulties on effective and efficient image retrieval from mass data under web environment. In this paper, we propose a web images classification method by integrating SIFT features of the images with global features. First, Locality Sensitive Hashing (LSH) is adopted for local feature extraction by embedding the SIFT feature vector. Then, other global features, such as color, texture or shape feature, are extracted. Support Vector Machine (SVM) is employed for image classification by using these two types of features respectively. The two classification results are integrated by decision-level fusion to get the final classification result. Experimental results on a web image dataset show that the proposed method is able to improve the performance of web images classification. © 2012 Springer-Verlag.
{fenge}
84871461922	Topology adaptation based on mobile agent in unstructured P2P networks	Peer-to-Peer (P2P) network has shown encouraging capacity in facilitating multimedia communication over Internet. This paper focuses on the topology adaptation in P2P networks. A new topology adaptation approach based on mobile agent is proposed to resolve the congestion caused by resource location in unstructured P2P networks. In our approach, peers direct mobile agent to migrate to the potential congestion peers based on their neighbors' characteristics including processing capacity and connectedness. Thus congestion can be found effectively and the topology optimization mechanism is then used to resolve the congestions. Simulation results show that the proposed method can improve networks topology, avoid congestions effectively and increase the efficiency of resource location. © 2012 Springer-Verlag.
{fenge}
84871078762	Automatic labeling hierarchical topics	Recently, statistical topic modeling has been widely applied in text mining and knowledge management due to its powerful ability. A topic, as a probability distribution over words, is usually difficult to be understood. A common, major challenge in applying such topic models to other knowledge management problem is to accurately interpret the meaning of each topic. Topic labeling, as a major interpreting method, has attracted significant attention recently. However, previous works simply treat topics individually without considering the hierarchical relation among topics, and less attention has been paid to creating a good hierarchical topic descriptors for a hierarchy of topics. In this paper, we propose two effective algorithms that automatically assign concise labels to each topic in a hierarchy by exploiting sibling and parent-child relations among topics. The experimental results show that the inter-topic relation is effective in boosting topic labeling accuracy and the proposed algorithms can generate meaningful topic labels that are useful for interpreting the hierarchical topics. © 2012 ACM.
{fenge}
84871666061	Visual-textual joint relevance learning for tag-based social image search	Due to the popularity of social media websites, extensive research efforts have been dedicated to tag-based social image search. Both visual information and tags have been investigated in the research field. However, most existing methods use tags and visual characteristics either separately or sequentially in order to estimate the relevance of images. In this paper, we propose an approach that simultaneously utilizes both visual and textual information to estimate the relevance of user tagged images. The relevance estimation is determined with a hypergraph learning approach. In this method, a social image hypergraph is constructed, where vertices represent images and hyperedges represent visual or textual terms. Learning is achieved with use of a set of pseudo-positive images, where the weights of hyperedges are updated throughout the learning process. In this way, the impact of different tags and visual words can be automatically modulated. Comparative results of the experiments conducted on a dataset including 370+images are presented, which demonstrate the effectiveness of the proposed approach. © 1992-2012 IEEE.
{fenge}
84872710355	Beyond text QA: Multimedia answer generation by harvesting web information	Community question answering (cQA) services have gained popularity over the past years. It not only allows community members to post and answer questions but also enables general users to seek information from a comprehensive set of well-answered questions. However, existing cQA forums usually provide only textual answers, which are not informative enough for many questions. In this paper, we propose a scheme that is able to enrich textual answers in cQA with appropriate media data. Our scheme consists of three components: answer medium selection, query generation for multimedia search, and multimedia data selection and presentation. This approach automatically determines which type of media information should be added for a textual answer. It then automatically collects data from the web to enrich the answer. By processing a large set of QA pairs and adding them to a pool, our approach can enable a novel multimedia question answering (MMQA) approach as users can find multimedia answers by matching their questions with those in the pool. Different from a lot of MMQA research efforts that attempt to directly answer questions with image and video data, our approach is built based on community-contributed textual answers and thus it is able to deal with more complex questions. We have conducted extensive experiments on a multi-source QA dataset. The results demonstrate the effectiveness of our approach. © 1999-2012 IEEE.
{fenge}
84874917844	GPSView: A scenic driving route planner	GPS devices have been widely used in automobiles to compute navigation routes to destinations. The generated driving route targets the minimal traveling distance, but neglects the sightseeing experience of the route. In this study, we propose an augmented GPS navigation system, GPSView, to incorporate a scenic factor into the routing. The goal of GPSView is to plan a driving route with scenery and sightseeing qualities, and therefore allow travelers to enjoy sightseeing on the drive. To do so, we first build a database of scenic roadways with vistas of landscapes and sights along the roadside. Specifically, we adapt an attention-based approach to exploit community-contributed GPS-tagged photos on the Internet to discover scenic roadways. The premise is: a multitude of photos taken along a roadway imply that this roadway is probably appealing and catches the public's attention. By analyzing the geospatial distribution of photos, the proposed approach discovers the roadside sight spots, or Points-Of-Interest (POIs), which have good scenic qualities and visibility to travelers on the roadway. Finally, we formulate scenic driving route planning as an optimization task towards the best trade-off between sightseeing experience and traveling distance. Testing in the northern California area shows that the proposed system can deliver promising results. © 2013 ACM.
{fenge}
84875402659	Interactive social group recommendation for Flickr photos	Social groups on photo sharing Websites, such as Flickr, are self-organized communities to share photos and conversations with common interest and have gained massive popularity. Currently, users have to manually assign each photo to the appropriated group. Manual assignment requires users to be familiar with existing photos in each group. It is intractable and tedious, and thus prohibits users from exploiting the relevant groups. For solution to the problem, group recommendation has attracted increasing attention recently, which aims to suggest groups to user for a particular photo. Existing works pose group recommendation as an automatic group prediction problem with a purpose of predicting the groups of each photo automatically. Despite of dramatic progress in automatic group prediction, the prediction results are still not accurate enough. In this paper, we propose an interactive group recommendation framework with Human-in-the-Loop. Given a user's photo collection, we employ the pre-built group classifiers to predict the group of each photo. These predictions are used as the initial group recommendations. We then select a small number of representative photos from the collection and ask user to assign the groups of them. Once obtaining user's feedbacks on the representative photos, we infer the groups of remaining photos through group propagation over multiple sparse graphs among the photos. We conduct experiment on 30 Flickr groups with 239,700 photos. The experimental results demonstrate that the proposed framework is able to provide accurate group recommendations with quite a small amount of user efforts. © 2012 Elsevier B.V.
{fenge}
84876283052	Multimedia encyclopedia construction by mining web knowledge	In recent years, we have witnessed the blooming of Web 2.0 content such as Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources available on the internet? This paper presents a novel concept called Mediapedia, a dynamic multimedia encyclopedia that takes advantage of, and in fact is built from the text and image resources on the Web. The Mediapedia distinguishes itself from the traditional encyclopedia in four main ways. (1) It tries to present users with multimedia contents (e.g., text, image, video) which we believed are more intuitive and informative to users. (2) It is fully automated because it downloads the media contents as well as the corresponding textual descriptions from the Web and assembles them for presentation. (3) It is dynamic as it will use the latest multimedia content to compose the answer. This is not true for the traditional encyclopedia. (4) The design of Mediapedia is flexible and extensible such that we can easily incorporate new kinds of mediums such as video and languages into the framework. The effectiveness of Mediapedia is demonstrated and two potential applications are described in this paper. © 2012 Elsevier B.V.
{fenge}
84876288566	Marginalized multi-layer multi-instance kernel for video concept detection	Video concept detection has been extensively studied in recent years. Most of the existing video concept detection approaches have treated video as a flat data sequence. However, video is essentially a kind of media with hierarchical structure, including multiple layers (e.g., video shot, frame, and region) and multiple instance relationship embedded in each pair of contiguous layers. In this paper, we propose a novel kernel, termed marginalized multi-layer multi-instance (MarMLMI) kernel for video concept detection. Different from most existing methods, the proposed MarMLMI kernel exploits the hierarchical structure of video, i.e., both the multi-layer structure and the multi-instance relationship. Furthermore, the instance label ambiguity in multi-instance setting is addressed by using the technology of marginalized kernel. We perform video concept detection on a real-world video corpus: the TREC video retrieval evaluation (TRECVID) benchmark and compare the proposed MarMLMI kernel to representative existing approaches. The experimental results demonstrate the effectiveness of the proposed MarMLMI kernel. © 2012 Elsevier B.V.
{fenge}
84883366465	Partial-duplicate image retrieval via saliency-guided visual matching	This article proposes a novel partial-duplicate image-retrieval scheme based on saliency-guided visual matching, where the localization of duplicates is done simultaneously. The image is abstracted by visually salient and rich regions (VSRRs), which are of visual saliency and contain rich visual content. Furthermore, to refine the retrieval, a relative saliency ordering constraint is constructed that captures the robust relative saliency layout of the VSRRs. The authors propose an efficient algorithm to embed this constraint into the index system so as to speed up retrieval. Comparison experiments with state-of-the-art methods on five databases show the efficiency and effectiveness of the proposed approach. © 1994-2012 IEEE.
{fenge}
84883696311	Click-boosting random walk for image search reranking	Image reranking is an effective way for improving the retrieval performance of keyword-based image search engines. A fundamental issue underlying the success of existing image reranking approaches is the ability in identifying potentially useful recurrent patterns or relevant training examples from the initial search results. Ideally, these patterns and examples can be leveraged to upgrade the ranks of visually similar images, which are also likely to be relevant. The challenge, nevertheless, originates from the fact that keyword-based queries are used to be ambiguous, resulting in difficulty in predicting the search intention. Mining useful patterns and examples without understanding query is risky, and may lead to incorrect judgment in reranking. This paper explores the use of click-through data, which can be viewed as the footprints of user searching behavior, as an effective means of understanding query, for providing the basis on identifying the recurrent patterns that are potentially helpful for reranking. A new algorithm, named click-boosting random walk, is proposed. The algorithm utilizes clicked images to locate similar images that are not clicked, and reranks them by random walk. This simple idea is shown to outperform several existing approaches on a real-world image dataset collected from a commercial search engine with click-through data. © 2013 ACM.
{fenge}
84901027043	Product aspect ranking and its applications	Numerous consumer reviews of products are now available on the Internet. Consumer reviews contain rich and valuable knowledge for both firms and users. However, the reviews are often disorganized, leading to difficulties in information navigation and knowledge acquisition. This article proposes a product aspect ranking framework, which automatically identifies the important aspects of products from online consumer reviews, aiming at improving the usability of the numerous reviews. The important product aspects are identified based on two observations: 1) the important aspects are usually commented on by a large number of consumers and 2) consumer opinions on the important aspects greatly influence their overall opinions on the product. In particular, given the consumer reviews of a product, we first identify product aspects by a shallow dependency parser and determine consumer opinions on these aspects via a sentiment classifier. We then develop a probabilistic aspect ranking algorithm to infer the importance of aspects by simultaneously considering aspect frequency and the influence of consumer opinions given to each aspect over their overall opinions. The experimental results on a review corpus of 21 popular products in eight domains demonstrate the effectiveness of the proposed approach. Moreover, we apply product aspect ranking to two real-world applications, i.e., document-level sentiment classification and extractive review summarization, and achieve significant performance improvements, which demonstrate the capacity of product aspect ranking in facilitating real-world applications. © 1989-2012 IEEE.
{fenge}
84903122155	Robust (Semi) nonnegative graph embedding	Nonnegative matrix factorization (NMF) has received considerable attention in image processing, computer vision, and patter recognition. An important variant of NMF is nonnegative graph embedding (NGE), which encodes the statistical or geometric information of data in the process of matrix factorization. The NGE offers a general framework for unsupervised/supervised settings. However, NGE-like algorithms often suffer from noisy data, unreliable graphs, and noisy labels, which are commonly encountered in real-world applications. To address these issues, in this paper, we first propose a robust nonnegative graph embedding (RNGE) framework, where the joint sparsity in both graph embedding and data reconstruction endues robustness to undesirable noises. Next, we present a robust seminonnegative graph embedding (RsNGE) framework, which only constrains the coefficient matrix to be nonnegative while places no constraint on the base matrix. This extends the applicable range of RNGE to data which are not nonnegative and endows more discriminative power of the learnt base matrix. The RNGE/RsNGE provides a general formulation such that all the algorithms unified within the graph embedding framework can be easily extended to obtain their robust nonnegative/seminonnegative solutions. Further, we develop elegant multiplicative updating solutions that can solve RNGE/RsNGE efficiently and offer a rigorous convergence analysis. We conduct extensive experiments on four real-world data sets and compare the proposed RNGE/RsNGE to other representative NMF variants and data factorization methods. The experimental results demonstrate the robustness and effectiveness of the proposed approaches. © 1992-2012 IEEE.
{fenge}
84904732573	Adaptive learning for celebrity identification with video context	In this paper, we propose a novel semi-supervised learning strategy to address the problem of celebrity identification. The video context information is explored to facilitate the learning process based on the assumption that faces in the same video track share the same identity. Once a frame within a track is recognized confidently, the label can be propagated through the whole track, referred to as the confident track. More specifically, given a few static images and vast face videos, an initial weak classifier is trained and gradually evolves by iteratively promoting the confident tracks into the 'labeled' set. The iterative selection process enriches the diversity of the 'labeled' set such that the performance of the classifier is gradually improved. This learning theme may suffer from semantic drifting caused by errors in selecting the confident tracks. To address this issue, we propose to treat the selected frames as related samples - an intermediate state between labeled and unlabeled instead of labeled as in the traditional approach. To evaluate the performance, we construct a new dataset, which includes 3000 static images and 2700 face tracks of 30 celebrities. Comprehensive evaluations on this dataset and a public video dataset indicate significant improvement of our approach over established baseline methods. © 1999-2012 IEEE.
{fenge}
84904737207	Gradient-domain-based enhancement of multi-view depth video	Multi-view depth is an emerging and attractive 3D representation in recent years, which acts a significant role in rendering numerous viewing angles from a small number of given input views. The performance of those depth-based 3D video applications is strongly dependent on the quality of multi-view depth. However, because of the limitations of depth acquisition and estimation, the quality of multi-view depth suffers from artifacts in spatial dimension and inconsistency in both the temporal and inter-view dimensions. In this paper, we propose a gradient-domain based enhancement method for multi-view depth. Being different from the traditional enhancement methods which intended to process one or two above dimensions, our proposal exploits the coherence of both temporal and inter-view dimensions in addition of the spatial one. It is very challenging to obtain the stable characteristics in multi-dimensions. To solve this problem, we propose to investigate the characteristics in the gradient domain rather than the intensity domain. The enhanced multi-view depth is obtained through the minimization of energy function under the constraint of a joint gradient field (JGF), which is estimated from multiple dimensions through motion estimation and geometric mapping. Therefore, the enhanced multi-view depth is a global optimization in multiple dimensions that ensures the consistency in temporal, inter-view and spatial domains. Furthermore, we also propose an enhancement structure to indicate the process order of depth frames. The experimental results suggest that the proposed method can enhance multi-view depth with desired sharpness and consistency. © 2014 Elsevier Inc. All rights reserved.
{fenge}
84904755064	A novel segmentation based video-denoising method with noise level estimation	Most state-of-the-art video-denoising algorithms assume an additive noise model, but such a model does not often reflect true conditions experienced in practice. In this paper, two main issues are addressed, namely, segmentation-based block matching and estimation of noise level. Unlike previously reported block-matching methods, the present method uses an efficient algorithm to perform block matching in spatially consistent segmentations of each image frame. To estimate the noise level function (NLF), which describes the noise level as a function of image brightness, a fast bilateral-median- filter-based method is proposed herein. Under the assumption of short-term coherence, this method of estimation is extended from a single frame to multiple frames. Coupling these two techniques together creates a segmentation-based, customised BM3D method that can be used to remove coloured multiplicative noise from videos. Experimental results obtained for benchmark data sets and real videos show that this method significantly outperforms other methods in removing coloured multiplicative noise. © 2014 Elsevier Inc. All rights reserved.
{fenge}
84907812437	Attribute-augmented semantic hierarchy: Towards a unified framework for content-based image retrieval	This article presents a novel attribute-augmented semantic hierarchy (A2SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in content-based image retrieval (CBIR). A2SH organizes semantic concepts into multiple semantic levels and augments each concept with a set of related attributes. The attributes are used to describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. An hierarchical semantic similarity function is learned to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedback on attributes and images. This feedback is then used to refine the search results based on A2SH. We use A2SH as a basis to develop a unified content-based image retrieval system. We conduct extensive experiments on a large-scale dataset of over one million Web images. Experimental results show that the proposed A2SH can characterize the semantic affinities among images accurately and can shape user search intent quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions.
{fenge}
84918808100	Achieving dynamic load balancing through mobile agents in small world P2P networks	Peer-to-Peer (P2P) networks are a class of distributed networking and are being deployed in a wide range of applications. Besides such an importance, P2P networks still incur complexities in the resource location policies and in the load balancing techniques of the nodes, especially in unstructured P2P networks. One potential solution to resolve such issues is to enable the P2P networks to evolve into a self-optimizing overlay network topology by identifying the overloaded peers promptly. This paper introduces a new load balancing method in unstructured P2P networks based on mobile agents and resource grouping techniques. We firstly propose a resource grouping strategy to cluster the nodes which have same set of resources, thereby balancing the load among inter-group nodes. On the other hand, load balancing among intra-group nodes is achieved by using the mobile agents monitoring technique. By using this technique, the mobile agents migrate through the nodes in the same group, for the purpose of identifying the possible network congestion. Thus, queries can reach the desired resources more quickly while congested nodes can be identified promptly. The simulation results show that our proposed network evolves into a group-based small world network significantly. The evolved network exhibits robustness and adaptability under external attacking, high query workload, and higher network churns. The simulation results also illustrate that the proposed model achieves better search performance than the DANTE system.
